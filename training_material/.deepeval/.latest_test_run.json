{"testRunData": {"testCases": [{"name": "test_case_0", "input": "What is Self RAG", "actualOutput": "# What is Self RAG?\n\n**Self RAG** (Self-Reflective Retrieval-Augmented Generation) is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by integrating retrieval and self-reflection mechanisms. Here\u2019s a breakdown of its key components:\n\n## Key Features\n\n- **Adaptive Retrieval**: \n  - Self RAG allows the model to determine when to retrieve relevant information based on the task at hand. This means it doesn't always pull in external data, only when it's deemed necessary.\n\n- **Self-Reflection**: \n  - The model generates special tokens, known as **reflection tokens**, which help it evaluate its own outputs and the relevance of retrieved information. These tokens can indicate whether to retrieve information and assess the quality of its responses.\n\n- **Improved Factuality**:\n  - By combining retrieval with self-reflection, Self RAG enhances the model's ability to produce factually accurate content, reducing errors common in standard LLMs.\n\n- **Training Efficiency**:\n  - The framework trains the model to generate text informed by relevant passages while also learning to critique its own outputs. This process reduces the need for extensive retraining or human feedback.\n\n## How It Works\n\n1. **Input Handling**: \n   - The model receives an input and generates an output based on its internal training.\n\n2. **Reflection Tokens**: \n   - It generates reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of its generated content.\n\n3. **Dynamic Decision Making**:\n   - Depending on the task, the model can adjust its behavior to prioritize factual accuracy or creativity.\n\n4. **Performance**:\n   - Experiments show that Self RAG outperforms existing models in various tasks, including open-domain question answering, reasoning, and long-form content generation, by improving accuracy and citation correctness.\n\n## Conclusion\n\nSelf RAG represents a significant advancement in the way LLMs can be trained and utilized, offering a method to enhance their performance while maintaining versatility and creativity. It effectively combines retrieval-based knowledge with self-assessment to produce higher quality outputs. \n\nFor more details on Self RAG, you can check the paper [here](https://selfrag.github.io/).", "success": true, "metricsData": [{"name": "Answer Relevancy", "threshold": 0.7, "success": true, "score": 1.0, "reason": "The score is 1.00 because the response directly addresses the question about Self RAG without any irrelevant statements.", "strictMode": false, "evaluationModel": "gpt-4o-mini", "evaluationCost": 0.00075435, "verboseLogs": "Statements:\n[\n    \"Self RAG stands for Self-Reflective Retrieval-Augmented Generation.\",\n    \"Self RAG is designed to improve the quality and factual accuracy of large language models (LLMs).\",\n    \"The framework integrates retrieval and self-reflection mechanisms.\",\n    \"Adaptive Retrieval allows the model to determine when to retrieve relevant information based on the task.\",\n    \"Self RAG does not always pull in external data, only when deemed necessary.\",\n    \"The model generates special tokens known as reflection tokens.\",\n    \"Reflection tokens help the model evaluate its own outputs and the relevance of retrieved information.\",\n    \"Reflection tokens can indicate whether to retrieve information and assess the quality of responses.\",\n    \"Self RAG enhances the model's ability to produce factually accurate content.\",\n    \"The framework reduces errors common in standard LLMs.\",\n    \"Self RAG trains the model to generate text informed by relevant passages.\",\n    \"The model learns to critique its own outputs.\",\n    \"This process reduces the need for extensive retraining or human feedback.\",\n    \"The model receives an input and generates an output based on its internal training.\",\n    \"It generates reflection tokens to decide if retrieval is necessary.\",\n    \"The model evaluates the relevance and support of its generated content.\",\n    \"Depending on the task, the model can adjust its behavior.\",\n    \"The model can prioritize factual accuracy or creativity.\",\n    \"Experiments show that Self RAG outperforms existing models in various tasks.\",\n    \"Self RAG improves accuracy and citation correctness.\",\n    \"Self RAG represents a significant advancement in the training and utilization of LLMs.\",\n    \"It combines retrieval-based knowledge with self-assessment to produce higher quality outputs.\"\n] \n \nVerdicts:\n[\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    }\n]"}], "runDuration": 15.10111660000257, "evaluationCost": 0.00075435, "order": 0}], "conversationalTestCases": [], "metricsScores": [{"metric": "Answer Relevancy", "scores": [1.0], "passes": 1, "fails": 0, "errors": 0}], "testPassed": 1, "testFailed": 0, "runDuration": 15.23290740000084, "evaluationCost": 0.00075435}}