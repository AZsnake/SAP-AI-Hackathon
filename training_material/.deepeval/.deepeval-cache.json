{"test_cases_lookup_map": {"{\"actual_output\": \"# What is Self RAG?\\n\\n**Self RAG** stands for **Self-Reflective Retrieval-Augmented Generation**. It is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection processes. Here\\u2019s a simple breakdown of its key features:\\n\\n## Key Features of Self RAG\\n\\n- **Retrieval on Demand**: Self RAG allows models to decide when to retrieve relevant information based on the input and previous outputs. This means the model can adaptively pull in information only when necessary.\\n\\n- **Self-Reflection**: The framework includes a mechanism for the model to critique its own outputs. This is done through special tokens called **reflection tokens** that provide feedback on the relevance, support, and usefulness of the generated text.\\n\\n- **Improved Performance**: Experiments show that Self RAG significantly outperforms other models, including state-of-the-art LLMs and traditional retrieval-augmented models, across various tasks like open-domain question answering and reasoning.\\n\\n- **Customizable Behavior**: The framework allows users to tailor the model\\u2019s behavior based on the specific requirements of different tasks, such as prioritizing factual accuracy or creativity.\\n\\n## How It Works\\n\\n1. **Input Processing**: Given an input prompt, the model first predicts whether retrieval is necessary.\\n2. **Retrieval**: If needed, it retrieves relevant passages from a knowledge base.\\n3. **Output Generation**: The model generates outputs based on the retrieved information and its own prior outputs.\\n4. **Self-Critique**: After generating the output, the model evaluates its performance using reflection tokens to improve future responses.\\n\\n## Performance\\n\\nSelf RAG has been shown to enhance factual accuracy and increase citation precision in generated responses when compared to other models like ChatGPT and retrieval-augmented Llama2-chat.\\n\\nFor further details, you can refer to the full documentation [here](https://selfrag.github.io/).\", \"context\": [\"Preprint.\\n0 50 100 150\\nNum of training (k)\\n35\\n40\\n45\\n50\\n55Perfomance\\n(a) PopQA\\n0 100\\nNum of training (k)\\n71\\n72\\n73\\n (b) PubHealth\\n0 100\\nNum of training (k)\\n40\\n60\\n (c) ASQA (prec)\\nPop Bio.\\nS & P 92.5 70.0\\nISREL 95.0 90.0\\nISSUP 90.0 85.0\\n(d) Human evaluation on PopQA\\nand Bio generation.\\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\\nHuman analysis on SELF -RAG outputs as well as reflection tokens.\\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models\\u2019 behaviors at\\ntest time by adjusting such parameters without requiring additional training.\\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\\noverall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\\nof threshold \\u03b4 (larger \\u03b4 results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\\nthe model\\u2019s retrieval frequencies dramatically change on both datasets. as \\u03b4 varies. On one hand,\\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\\nEffects of training data size. We conduct an analysis of how the data scale affects the model\\u2019s\\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\\n150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare\\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -\\nRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\\u2019\\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\\nnot observed such significant improvements on Llama2-FT7B when increasing the training data from\\n50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may\\nlead to further improvements, although in this work we limit our training data size to 150k.\\nHuman evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the\\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\\nresults. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\\nverify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG\\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\\nreflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported\\noutput is supported by the cited evidence). Human annotators find SELF -RAG answers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\\nconsistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\\nexamples and explanations on assessments.\\n6 C ONCLUSION\\nThis work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n10\", \"Preprint.\\nA S ELF -RAG DETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n\\u2022 Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence, which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF -RAG\\ngenerates multiple segments based on the passage.\\n\\u2022 Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\\n\\u2022 Supported ( ISSUP ): Attribution is the concept of whether the output is fully supported by\\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\\nsupported, Partially supported, and No support / Contradictory, follow-\\ning Yue et al. (2023); Nakano et al. (2021).\\n\\u2022 Useful ( ISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\\nas whether the response is a helpful and informative answer to the query, independently from\\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\\noutputs with \\u201c##\\u201d. We use the temperature 1 and set the maximum output token counts to be 200. We\\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\\nfor ISSUP , 19,317 for relevance, 3,831 for utility.\\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\\nthe same instruction, demonstrations, and test instances. We found our assessments show high\\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\\ndisagreement between 1 and 2 or 4 and 5.\\nA.2 S ELF -RAG TRAINING\\nOverview of training. Algorithm 2 provides a high-level overview of our training.\\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\\nPerformance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\\n17\", \"Preprint.\\nAPPENDIX\\nA S ELF -RAG Details 17\\nA.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB Experimental Details 19\\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC Results 20\\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nD Full List of Instructions and Demonstrations for GPT-4 21\\n16\", \"Preprint.\\nAlgorithm 2 SELF -RAG Training\\n1: Input input-output data D = {X, Y}, generator M, C \\u03b8\\n2: Initialize C with a pre-trained LM\\n3: Sample data {Xsample, Ysample} \\u223c {X, Y} \\u25b7 Training Critic LM (Section 3.2.1)\\n4: for (x, y) \\u2208 (Xsample, Ysample) do \\u25b7 Data collections for C\\n5: Prompt GPT-4 to collect a reflection token r for (x, y)\\n6: Add {(x, y, r)} to Dcritic\\n7: Update C with next token prediction loss \\u25b7 Critic learning; Eq. 1\\n8: Initialize M with a pre-trained LM \\u25b7 Training Generator LM (Section 3.2.2)\\n9: for (x, y) \\u2208 (X, Y) do \\u25b7 Data collection for M with Dcritic\\n10: Run C to predict r given (x, y)\\n11: Add (x, y, r) to Dgen\\n12: Update M on Dgen with next token prediction loss \\u25b7 Generator LM learning; Eq. 2\\nDataset name category Data source the number of instances\\nGPT-4 Alpaca Instruction-following Open-Instruct 26,168\\nStanford Alpaca Instruction-following Open-Instruct 25,153\\nFLAN-V2 Instruction-following Open-Instruct 17,817\\nShareGPT Instruction-following Open-Instruct 13,406\\nOpen Assistant 1 Instruction-following Open-Instruct 9,464\\nWizard of Wikipedia Knowledge-intensive KILT 17,367\\nNatural Questions Knowledge-intensive KILT 15,535\\nFEVER Knowledge-intensive KILT 9,966\\nOpenBoookQA Knowledge-intensive HF Dataset 4,699\\nArc-Easy Knowledge-intensive HF Dataset 2,147\\nASQA Knowledge-intensive ASQA 3,897\\nTable 3: The generator LM M training data statistics.\\nbase LM Retrieve ISSUP ISREL ISUSE\\nLlama2-7B 93.8 93.5 80.2 73.5\\nFLAN-3B 85.6 73.1 82.0 72.1\\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\\nlower performance on ISUSE , this is because both models often confuse between the two highest\\ncases (5 and 4), where human annotators can also disagree.\\nDetails of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\\nnot. For the instances where the critic predicts Retrieve =No, we only predict the ISUSE given input\\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\\nthen split output sentences using Spacy.7 For each sentence, we run C to predict whether the retrieval\\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. IfC predicts\\nRetrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\\nt-th segment. For each retrieved passage, we predict ISREL and ISSUP . If there is any passage and\\ncontinuation with ISREL =Relevant and ISSUP =Fully Supported / ISSUP =Partially\\n7https://spacy.io/\\n18\", \"Preprint.\\nPQA Med AS\\n(acc) (acc) (em)\\nSELF -RAG (50k) 45.5 73.5 32.1\\nTraining\\nNo Retriever R 43.6 67.8 31.0\\nNo Critic C 42.6 72.0 18.1\\nTest\\nNo retrieval 24.7 73.0 \\u2013\\nHard constraints 28.3 72.6 \\u2013\\nRetrieve top1 41.8 73.1 28.6\\nRemove ISSUP 44.1 73.2 30.6\\n(a) Ablation\\n1 2\\n70.0\\n70.5Precision\\n1 2\\nWeight for IsSupport\\n90\\n95Mauve\\n (b) Customization\\n0.0 0.2 0.4 0.6\\n0.98\\n0.99\\n0.99\\n1.00Accuracy\\nPubHealth\\n0.0 0.2 0.4 0.6\\nRetrieval Threshold\\n0.6\\n0.8\\n1.0Accuracy\\nPopQA\\n0.0\\n0.5\\n1.0\\nFrequency\\n0.25\\n0.50\\n0.75\\n1.00\\nFrequency\\n (c) Retrieval\\nFigure 3: Analysis on SELF -RAG: (a) Ablation studies for key components of SELF -RAG training\\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\\nprecisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\\ninstruction-output pairs as SELF -RAG without retrieval or self-reflection and is retrieval-augmented\\nat test time only, lags behind SELF -RAG. This result indicates S ELF -RAG gains are not solely from\\ntraining data and demonstrate the effectiveness of SELF -RAG framework.\\n5.2 A NALYSIS\\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\\ntop one document only, similar to standard RAG approaches;Remove ISSUP indicates the model\\nperformance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this\\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\\nand the bottom part is for inference ablations. We see that all components play important roles. We\\nalso observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\\nSELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAG\\u2019s\\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\\nEffects of inference-time customization. One key benefit of our proposed framework is that it\\nenables us to control how much each critique type affects the final generation sampling. We analyze\\nthe effects of different parameter weights on the top of our 7B model during inference time on\\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\\nthe weighting term for ISSUP , which criticizes how supported the output is by the text passage. As\\nthe figure shows, increasing the weight leads to positive effects on the models\\u2019 citation precision\\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\\n9\", \"Preprint.\\nSELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND\\nCRITIQUE THROUGH SELF -REFLECTION\\nAkari Asai\\u2020, Zeqiu Wu\\u2020, Yizhong Wang\\u2020\\u00a7, Avirup Sil\\u2021, Hannaneh Hajishirzi\\u2020\\u00a7\\n\\u2020University of Washington \\u00a7Allen Institute for AI \\u2021IBM Research AI\\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration (SELF -RAG) that enhances an LM\\u2019s quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF -\\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an\\nLLM\\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF -RAG first determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\\nretrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/.\\n1\\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023\", \"Preprint.\\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\\nthey outperforms all non-proprietary models. \\u2217 indicates concurrent or recent results reported by\\nconcurrent work. \\u2013 indicates numbers that are not reported by the original papers or are not applicable.\\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\\nShort-form Closed-set Long-form generations (with citations)\\nPopQA TQA Pub ARC Bio ASQA\\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\\nLMs with proprietary data\\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 \\u2013 \\u2013\\nRet-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\\nChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 \\u2013 \\u2013\\nRet-ChatGPT 50.8 65.7 54.7 75.3 \\u2013 40.7 39.9 79.7 65.1 76.6\\nPerplexity.ai \\u2013 \\u2013 \\u2013 \\u2013 71.2 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nBaselines without retrieval\\nLlama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 \\u2013 \\u2013\\nAlpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 \\u2013 \\u2013\\nLlama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 \\u2013 \\u2013\\nAlpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 \\u2013 \\u2013\\nCoVE65B * \\u2013 \\u2013 \\u2013 \\u2013 71.2 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nBaselines with retrieval\\nToolformer*6B \\u2013 48.8 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nLlama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\\nAlpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\\nLlama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\\nSAIL*7B \\u2013 \\u2013 69.2 48.4 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nLlama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\\nOur SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\\nOur SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\\n5 R ESULTS AND ANALYSIS\\n5.1 M AIN RESULTS\\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\\nretrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\\niteratively prompts Llama265B to refine output.\\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF -RAG also\\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\\nretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\\nin this particular task, surpassing smaller LMs. Our SELF -RAG bridges this performance gap, even\\noutperforming ChatGPT in citation precision, which measures whether the model-generated claim is\\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF -RAG\\n7B occasionally outperforms our 13B due to the tendency of smaller SELF -RAG to often generate\\n8\", \"Preprint.\\nType Input Output Definitions\\nRetrieve x / x, y {yes, no, continue} Decides when to retrieve with R\\nISREL x, d {relevant, irrelevant} d provides useful information to solve x.\\nISSUP x, d, y {fully supported, partially\\nsupported, no support}\\nAll of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.\\nTable 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF -RAG Inference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }\\n1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\\n2: M predicts Retrieve given (x, y<t)\\n3: if Retrieve == Yes then\\n4: Retrieve relevant text passages D using R given (x, yt\\u22121) \\u25b7 Retrieve\\n5: M predicts ISREL given x, dand yt given x, d, y<t for each d \\u2208 D \\u25b7 Generate\\n6: M predicts ISSUP and ISUSE given x, yt, dfor each d \\u2208 D \\u25b7 Critique\\n7: Rank yt based on ISREL , ISSUP , ISUSE \\u25b7 Detailed in Section 3.3\\n8: else if Retrieve == No then\\n9: Mgen predicts yt given x \\u25b7 Generate\\n10: Mgen predicts ISUSE given x, yt \\u25b7 Critique\\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassage\\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.4 To generate each segment, SELF -RAG processes multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\\nand d3 output is only partially supported while d1 are fully supported.\\nTraining overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\\nM to generate reflection tokens by itself without relying on the critic at inference time.\\n3.2 S ELF -RAG TRAINING\\nHere, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)\\nand the generator M (Section 3.2.2).\\n3.2.1 T RAINING THE CRITIC MODEL\\nData collection for critic model. Manual annotation of reflection tokens for each segment is\\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\\n4We follow Liu et al. (2023a) in using a \\u201cperceived\\u201d utility value that is independent of retrieved passages.\\n4\", \"Preprint.\\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\\nentities. Yet, the improved task performance of such approaches often comes at the expense of\\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\\ngeneration guided by reflections tokens to further improve generation quality and attributions.\\nConcurrent RAG work. A few concurrent works2 on RAG propose new training or prompting\\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\\ninstruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best\\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\\na summarization model to filter out or compress retrieved passages before using them to prompt the\\nLM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones\\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\\nand to generate with tree search, guided by LM-generated value scores. While their value function\\nsimply indicates an overall score of each generation, SELF -RAG trains to an arbitrary LM to learn to\\ngenerate fine-grained self-reflection and customizable inference.\\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\\non retrieval and generation, we train our target LM on task examples augmented with reflection\\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\\nreflection tokens in SELF -RAG enable controllable generation at inference, while RLHF focuses on\\nhuman preference alignment during training. Other works use general control tokens to guide LM\\ngeneration (Lu et al., 2022; Korbak et al., 2023), whileSELF -RAG uses reflection tokens to decide the\\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\\n3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\\nSELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLM\\u2019s original creativity and versatility. Our end-to-end training\\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the output\\u2019s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3\", \"Preprint.\\nseparate reward models during training, we compute critique offline and directly insert them into the\\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\\nlearns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\\n3.3 S ELF -RAG INFERENCE\\nGenerating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\\ncontrol to meet these distinct objectives during the inference process.\\nAdaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by\\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\\nCritique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp\\nt as sG\\nt , and we compute a segment score as follows:\\nf(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)\\nS( Critique ) =\\nX\\nG\\u2208G\\nwGsG\\nt for G = { ISREL , ISSUP , ISUSE }, (4)\\nwhere sG\\nt = pt(\\u02c6r)PNG\\ni=1 pt(ri)\\nstands for the generation probability of the most desirable reflection token\\n\\u02c6r (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent\\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\\ny is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while\\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\\nfilter out a segment continuation when the model generates an undesirable Critique token (e.g.,\\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\\u2019\\nbehaviors. SELF -RAG tailors an LM with no additional training.\\n4 E XPERIMENTS\\n4.1 T ASKS AND DATASETS\\nWe conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\\nour experiments\\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\\nClosed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;\\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\\n6\"], \"expected_output\": null, \"hyperparameters\": null, \"input\": \"What is Self RAG\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Hallucination", "threshold": 0.3, "success": true, "score": 0.0, "reason": "The score is 0.00 because there are no contradictions present, and all factual alignments between the actual output and the context are accurate and consistent.", "strictMode": false, "evaluationModel": "gpt-4o-mini", "evaluationCost": 0, "verboseLogs": "Verdicts:\n[\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output accurately describes Self RAG as a framework designed to improve the quality and factual accuracy of large language models through retrieval and self-reflection.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output correctly mentions the use of reflection tokens in Self RAG to critique its own outputs, aligning with the context provided.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output states that Self RAG significantly outperforms other models, which is consistent with the context that highlights its superior performance across various tasks.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's mention of customizable behavior in Self RAG aligns with the context that discusses tailoring the model's behavior for different tasks.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's explanation of the input processing and retrieval process is consistent with the context that describes how Self RAG operates.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's description of the self-critique mechanism using reflection tokens matches the context's explanation of this feature in Self RAG.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's claim about improved performance in factual accuracy and citation precision is supported by the context that discusses these enhancements.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's summary of the Self RAG framework's key features aligns well with the detailed descriptions provided in the context.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's mention of the model's ability to adaptively retrieve information is consistent with the context's emphasis on retrieval on demand.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output's reference to the framework's performance compared to models like ChatGPT and Llama2-chat is in agreement with the context's findings.\"\n    }\n]"}, "metric_configuration": {"threshold": 0.3, "evaluation_model": "gpt-4o-mini", "strict_mode": false, "include_reason": true}}]}, "{\"actual_output\": \"# What is Self RAG?\\n\\n**Self RAG** stands for **Self-Reflective Retrieval-Augmented Generation**. It is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection processes. Here\\u2019s a simple breakdown of its key features:\\n\\n## Key Features of Self RAG\\n\\n- **Retrieval on Demand**: Self RAG allows models to decide when to retrieve relevant information based on the input and previous outputs. This means the model can adaptively pull in information only when necessary.\\n\\n- **Self-Reflection**: The framework includes a mechanism for the model to critique its own outputs. This is done through special tokens called **reflection tokens** that provide feedback on the relevance, support, and usefulness of the generated text.\\n\\n- **Improved Performance**: Experiments show that Self RAG significantly outperforms other models, including state-of-the-art LLMs and traditional retrieval-augmented models, across various tasks like open-domain question answering and reasoning.\\n\\n- **Customizable Behavior**: The framework allows users to tailor the model\\u2019s behavior based on the specific requirements of different tasks, such as prioritizing factual accuracy or creativity.\\n\\n## How It Works\\n\\n1. **Input Processing**: Given an input prompt, the model first predicts whether retrieval is necessary.\\n2. **Retrieval**: If needed, it retrieves relevant passages from a knowledge base.\\n3. **Output Generation**: The model generates outputs based on the retrieved information and its own prior outputs.\\n4. **Self-Critique**: After generating the output, the model evaluates its performance using reflection tokens to improve future responses.\\n\\n## Performance\\n\\nSelf RAG has been shown to enhance factual accuracy and increase citation precision in generated responses when compared to other models like ChatGPT and retrieval-augmented Llama2-chat.\\n\\nFor further details, you can refer to the full documentation [here](https://selfrag.github.io/).\", \"context\": null, \"expected_output\": null, \"hyperparameters\": null, \"input\": \"What is Self RAG\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy", "threshold": 0.7, "success": true, "score": 1.0, "reason": "The score is 1.00 because the response directly addressed the question about Self RAG without including any irrelevant statements.", "strictMode": false, "evaluationModel": "gpt-4o-mini", "evaluationCost": 0, "verboseLogs": "Statements:\n[\n    \"Self RAG stands for Self-Reflective Retrieval-Augmented Generation.\",\n    \"Self RAG is designed to improve the quality and factual accuracy of large language models.\",\n    \"Self RAG allows models to decide when to retrieve relevant information based on input and previous outputs.\",\n    \"The model can adaptively pull in information only when necessary.\",\n    \"The framework includes a mechanism for the model to critique its own outputs.\",\n    \"Reflection tokens provide feedback on the relevance, support, and usefulness of the generated text.\",\n    \"Self RAG significantly outperforms other models across various tasks.\",\n    \"Self RAG outperforms state-of-the-art LLMs and traditional retrieval-augmented models.\",\n    \"Self RAG enhances factual accuracy and increases citation precision in generated responses.\",\n    \"The framework allows users to tailor the model\u2019s behavior based on specific requirements.\",\n    \"The model first predicts whether retrieval is necessary given an input prompt.\",\n    \"If needed, the model retrieves relevant passages from a knowledge base.\",\n    \"The model generates outputs based on the retrieved information and its own prior outputs.\",\n    \"After generating the output, the model evaluates its performance using reflection tokens.\"\n] \n \nVerdicts:\n[\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    }\n]"}, "metric_configuration": {"threshold": 0.7, "evaluation_model": "gpt-4o-mini", "strict_mode": false, "include_reason": true}}]}, "{\"actual_output\": \"# What is Self RAG?\\n\\n**Self RAG** (Self-Reflective Retrieval-Augmented Generation) is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by integrating retrieval and self-reflection mechanisms. Here\\u2019s a breakdown of its key components:\\n\\n## Key Features\\n\\n- **Adaptive Retrieval**: \\n  - Self RAG allows the model to determine when to retrieve relevant information based on the task at hand. This means it doesn't always pull in external data, only when it's deemed necessary.\\n\\n- **Self-Reflection**: \\n  - The model generates special tokens, known as **reflection tokens**, which help it evaluate its own outputs and the relevance of retrieved information. These tokens can indicate whether to retrieve information and assess the quality of its responses.\\n\\n- **Improved Factuality**:\\n  - By combining retrieval with self-reflection, Self RAG enhances the model's ability to produce factually accurate content, reducing errors common in standard LLMs.\\n\\n- **Training Efficiency**:\\n  - The framework trains the model to generate text informed by relevant passages while also learning to critique its own outputs. This process reduces the need for extensive retraining or human feedback.\\n\\n## How It Works\\n\\n1. **Input Handling**: \\n   - The model receives an input and generates an output based on its internal training.\\n\\n2. **Reflection Tokens**: \\n   - It generates reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of its generated content.\\n\\n3. **Dynamic Decision Making**:\\n   - Depending on the task, the model can adjust its behavior to prioritize factual accuracy or creativity.\\n\\n4. **Performance**:\\n   - Experiments show that Self RAG outperforms existing models in various tasks, including open-domain question answering, reasoning, and long-form content generation, by improving accuracy and citation correctness.\\n\\n## Conclusion\\n\\nSelf RAG represents a significant advancement in the way LLMs can be trained and utilized, offering a method to enhance their performance while maintaining versatility and creativity. It effectively combines retrieval-based knowledge with self-assessment to produce higher quality outputs. \\n\\nFor more details on Self RAG, you can check the paper [here](https://selfrag.github.io/).\", \"context\": [\"Preprint.\\n0 50 100 150\\nNum of training (k)\\n35\\n40\\n45\\n50\\n55Perfomance\\n(a) PopQA\\n0 100\\nNum of training (k)\\n71\\n72\\n73\\n (b) PubHealth\\n0 100\\nNum of training (k)\\n40\\n60\\n (c) ASQA (prec)\\nPop Bio.\\nS & P 92.5 70.0\\nISREL 95.0 90.0\\nISSUP 90.0 85.0\\n(d) Human evaluation on PopQA\\nand Bio generation.\\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\\nHuman analysis on SELF -RAG outputs as well as reflection tokens.\\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models\\u2019 behaviors at\\ntest time by adjusting such parameters without requiring additional training.\\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\\noverall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\\nof threshold \\u03b4 (larger \\u03b4 results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\\nthe model\\u2019s retrieval frequencies dramatically change on both datasets. as \\u03b4 varies. On one hand,\\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\\nEffects of training data size. We conduct an analysis of how the data scale affects the model\\u2019s\\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\\n150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare\\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -\\nRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\\u2019\\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\\nnot observed such significant improvements on Llama2-FT7B when increasing the training data from\\n50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may\\nlead to further improvements, although in this work we limit our training data size to 150k.\\nHuman evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the\\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\\nresults. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\\nverify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG\\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\\nreflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported\\noutput is supported by the cited evidence). Human annotators find SELF -RAG answers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\\nconsistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\\nexamples and explanations on assessments.\\n6 C ONCLUSION\\nThis work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n10\", \"Preprint.\\nA S ELF -RAG DETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n\\u2022 Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence, which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF -RAG\\ngenerates multiple segments based on the passage.\\n\\u2022 Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\\n\\u2022 Supported ( ISSUP ): Attribution is the concept of whether the output is fully supported by\\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\\nsupported, Partially supported, and No support / Contradictory, follow-\\ning Yue et al. (2023); Nakano et al. (2021).\\n\\u2022 Useful ( ISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\\nas whether the response is a helpful and informative answer to the query, independently from\\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\\noutputs with \\u201c##\\u201d. We use the temperature 1 and set the maximum output token counts to be 200. We\\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\\nfor ISSUP , 19,317 for relevance, 3,831 for utility.\\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\\nthe same instruction, demonstrations, and test instances. We found our assessments show high\\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\\ndisagreement between 1 and 2 or 4 and 5.\\nA.2 S ELF -RAG TRAINING\\nOverview of training. Algorithm 2 provides a high-level overview of our training.\\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\\nPerformance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\\n17\", \"Preprint.\\nAPPENDIX\\nA S ELF -RAG Details 17\\nA.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB Experimental Details 19\\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC Results 20\\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nD Full List of Instructions and Demonstrations for GPT-4 21\\n16\", \"Preprint.\\nAlgorithm 2 SELF -RAG Training\\n1: Input input-output data D = {X, Y}, generator M, C \\u03b8\\n2: Initialize C with a pre-trained LM\\n3: Sample data {Xsample, Ysample} \\u223c {X, Y} \\u25b7 Training Critic LM (Section 3.2.1)\\n4: for (x, y) \\u2208 (Xsample, Ysample) do \\u25b7 Data collections for C\\n5: Prompt GPT-4 to collect a reflection token r for (x, y)\\n6: Add {(x, y, r)} to Dcritic\\n7: Update C with next token prediction loss \\u25b7 Critic learning; Eq. 1\\n8: Initialize M with a pre-trained LM \\u25b7 Training Generator LM (Section 3.2.2)\\n9: for (x, y) \\u2208 (X, Y) do \\u25b7 Data collection for M with Dcritic\\n10: Run C to predict r given (x, y)\\n11: Add (x, y, r) to Dgen\\n12: Update M on Dgen with next token prediction loss \\u25b7 Generator LM learning; Eq. 2\\nDataset name category Data source the number of instances\\nGPT-4 Alpaca Instruction-following Open-Instruct 26,168\\nStanford Alpaca Instruction-following Open-Instruct 25,153\\nFLAN-V2 Instruction-following Open-Instruct 17,817\\nShareGPT Instruction-following Open-Instruct 13,406\\nOpen Assistant 1 Instruction-following Open-Instruct 9,464\\nWizard of Wikipedia Knowledge-intensive KILT 17,367\\nNatural Questions Knowledge-intensive KILT 15,535\\nFEVER Knowledge-intensive KILT 9,966\\nOpenBoookQA Knowledge-intensive HF Dataset 4,699\\nArc-Easy Knowledge-intensive HF Dataset 2,147\\nASQA Knowledge-intensive ASQA 3,897\\nTable 3: The generator LM M training data statistics.\\nbase LM Retrieve ISSUP ISREL ISUSE\\nLlama2-7B 93.8 93.5 80.2 73.5\\nFLAN-3B 85.6 73.1 82.0 72.1\\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\\nlower performance on ISUSE , this is because both models often confuse between the two highest\\ncases (5 and 4), where human annotators can also disagree.\\nDetails of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\\nnot. For the instances where the critic predicts Retrieve =No, we only predict the ISUSE given input\\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\\nthen split output sentences using Spacy.7 For each sentence, we run C to predict whether the retrieval\\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. IfC predicts\\nRetrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\\nt-th segment. For each retrieved passage, we predict ISREL and ISSUP . If there is any passage and\\ncontinuation with ISREL =Relevant and ISSUP =Fully Supported / ISSUP =Partially\\n7https://spacy.io/\\n18\", \"Preprint.\\nPQA Med AS\\n(acc) (acc) (em)\\nSELF -RAG (50k) 45.5 73.5 32.1\\nTraining\\nNo Retriever R 43.6 67.8 31.0\\nNo Critic C 42.6 72.0 18.1\\nTest\\nNo retrieval 24.7 73.0 \\u2013\\nHard constraints 28.3 72.6 \\u2013\\nRetrieve top1 41.8 73.1 28.6\\nRemove ISSUP 44.1 73.2 30.6\\n(a) Ablation\\n1 2\\n70.0\\n70.5Precision\\n1 2\\nWeight for IsSupport\\n90\\n95Mauve\\n (b) Customization\\n0.0 0.2 0.4 0.6\\n0.98\\n0.99\\n0.99\\n1.00Accuracy\\nPubHealth\\n0.0 0.2 0.4 0.6\\nRetrieval Threshold\\n0.6\\n0.8\\n1.0Accuracy\\nPopQA\\n0.0\\n0.5\\n1.0\\nFrequency\\n0.25\\n0.50\\n0.75\\n1.00\\nFrequency\\n (c) Retrieval\\nFigure 3: Analysis on SELF -RAG: (a) Ablation studies for key components of SELF -RAG training\\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\\nprecisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\\ninstruction-output pairs as SELF -RAG without retrieval or self-reflection and is retrieval-augmented\\nat test time only, lags behind SELF -RAG. This result indicates S ELF -RAG gains are not solely from\\ntraining data and demonstrate the effectiveness of SELF -RAG framework.\\n5.2 A NALYSIS\\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\\ntop one document only, similar to standard RAG approaches;Remove ISSUP indicates the model\\nperformance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this\\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\\nand the bottom part is for inference ablations. We see that all components play important roles. We\\nalso observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\\nSELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAG\\u2019s\\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\\nEffects of inference-time customization. One key benefit of our proposed framework is that it\\nenables us to control how much each critique type affects the final generation sampling. We analyze\\nthe effects of different parameter weights on the top of our 7B model during inference time on\\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\\nthe weighting term for ISSUP , which criticizes how supported the output is by the text passage. As\\nthe figure shows, increasing the weight leads to positive effects on the models\\u2019 citation precision\\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\\n9\", \"Preprint.\\nSELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND\\nCRITIQUE THROUGH SELF -REFLECTION\\nAkari Asai\\u2020, Zeqiu Wu\\u2020, Yizhong Wang\\u2020\\u00a7, Avirup Sil\\u2021, Hannaneh Hajishirzi\\u2020\\u00a7\\n\\u2020University of Washington \\u00a7Allen Institute for AI \\u2021IBM Research AI\\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration (SELF -RAG) that enhances an LM\\u2019s quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF -\\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an\\nLLM\\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF -RAG first determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\\nretrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/.\\n1\\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023\", \"Preprint.\\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\\nthey outperforms all non-proprietary models. \\u2217 indicates concurrent or recent results reported by\\nconcurrent work. \\u2013 indicates numbers that are not reported by the original papers or are not applicable.\\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\\nShort-form Closed-set Long-form generations (with citations)\\nPopQA TQA Pub ARC Bio ASQA\\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\\nLMs with proprietary data\\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 \\u2013 \\u2013\\nRet-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\\nChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 \\u2013 \\u2013\\nRet-ChatGPT 50.8 65.7 54.7 75.3 \\u2013 40.7 39.9 79.7 65.1 76.6\\nPerplexity.ai \\u2013 \\u2013 \\u2013 \\u2013 71.2 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nBaselines without retrieval\\nLlama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 \\u2013 \\u2013\\nAlpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 \\u2013 \\u2013\\nLlama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 \\u2013 \\u2013\\nAlpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 \\u2013 \\u2013\\nCoVE65B * \\u2013 \\u2013 \\u2013 \\u2013 71.2 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nBaselines with retrieval\\nToolformer*6B \\u2013 48.8 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nLlama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\\nAlpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\\nLlama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\\nSAIL*7B \\u2013 \\u2013 69.2 48.4 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013 \\u2013\\nLlama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\\nOur SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\\nOur SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\\n5 R ESULTS AND ANALYSIS\\n5.1 M AIN RESULTS\\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\\nretrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\\niteratively prompts Llama265B to refine output.\\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF -RAG also\\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\\nretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\\nin this particular task, surpassing smaller LMs. Our SELF -RAG bridges this performance gap, even\\noutperforming ChatGPT in citation precision, which measures whether the model-generated claim is\\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF -RAG\\n7B occasionally outperforms our 13B due to the tendency of smaller SELF -RAG to often generate\\n8\", \"Preprint.\\nType Input Output Definitions\\nRetrieve x / x, y {yes, no, continue} Decides when to retrieve with R\\nISREL x, d {relevant, irrelevant} d provides useful information to solve x.\\nISSUP x, d, y {fully supported, partially\\nsupported, no support}\\nAll of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.\\nTable 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF -RAG Inference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }\\n1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\\n2: M predicts Retrieve given (x, y<t)\\n3: if Retrieve == Yes then\\n4: Retrieve relevant text passages D using R given (x, yt\\u22121) \\u25b7 Retrieve\\n5: M predicts ISREL given x, dand yt given x, d, y<t for each d \\u2208 D \\u25b7 Generate\\n6: M predicts ISSUP and ISUSE given x, yt, dfor each d \\u2208 D \\u25b7 Critique\\n7: Rank yt based on ISREL , ISSUP , ISUSE \\u25b7 Detailed in Section 3.3\\n8: else if Retrieve == No then\\n9: Mgen predicts yt given x \\u25b7 Generate\\n10: Mgen predicts ISUSE given x, yt \\u25b7 Critique\\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassage\\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.4 To generate each segment, SELF -RAG processes multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\\nand d3 output is only partially supported while d1 are fully supported.\\nTraining overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\\nM to generate reflection tokens by itself without relying on the critic at inference time.\\n3.2 S ELF -RAG TRAINING\\nHere, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)\\nand the generator M (Section 3.2.2).\\n3.2.1 T RAINING THE CRITIC MODEL\\nData collection for critic model. Manual annotation of reflection tokens for each segment is\\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\\n4We follow Liu et al. (2023a) in using a \\u201cperceived\\u201d utility value that is independent of retrieved passages.\\n4\", \"Preprint.\\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\\nentities. Yet, the improved task performance of such approaches often comes at the expense of\\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\\ngeneration guided by reflections tokens to further improve generation quality and attributions.\\nConcurrent RAG work. A few concurrent works2 on RAG propose new training or prompting\\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\\ninstruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best\\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\\na summarization model to filter out or compress retrieved passages before using them to prompt the\\nLM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones\\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\\nand to generate with tree search, guided by LM-generated value scores. While their value function\\nsimply indicates an overall score of each generation, SELF -RAG trains to an arbitrary LM to learn to\\ngenerate fine-grained self-reflection and customizable inference.\\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\\non retrieval and generation, we train our target LM on task examples augmented with reflection\\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\\nreflection tokens in SELF -RAG enable controllable generation at inference, while RLHF focuses on\\nhuman preference alignment during training. Other works use general control tokens to guide LM\\ngeneration (Lu et al., 2022; Korbak et al., 2023), whileSELF -RAG uses reflection tokens to decide the\\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\\n3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\\nSELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLM\\u2019s original creativity and versatility. Our end-to-end training\\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the output\\u2019s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3\", \"Preprint.\\nseparate reward models during training, we compute critique offline and directly insert them into the\\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\\nlearns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\\n3.3 S ELF -RAG INFERENCE\\nGenerating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\\ncontrol to meet these distinct objectives during the inference process.\\nAdaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by\\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\\nCritique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp\\nt as sG\\nt , and we compute a segment score as follows:\\nf(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)\\nS( Critique ) =\\nX\\nG\\u2208G\\nwGsG\\nt for G = { ISREL , ISSUP , ISUSE }, (4)\\nwhere sG\\nt = pt(\\u02c6r)PNG\\ni=1 pt(ri)\\nstands for the generation probability of the most desirable reflection token\\n\\u02c6r (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent\\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\\ny is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while\\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\\nfilter out a segment continuation when the model generates an undesirable Critique token (e.g.,\\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\\u2019\\nbehaviors. SELF -RAG tailors an LM with no additional training.\\n4 E XPERIMENTS\\n4.1 T ASKS AND DATASETS\\nWe conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\\nour experiments\\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\\nClosed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;\\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\\n6\"], \"expected_output\": null, \"hyperparameters\": null, \"input\": \"What is Self RAG\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Hallucination", "threshold": 0.3, "success": true, "score": 0.0, "reason": "The score is 0.00 because there are no contradictions present, and all factual alignments accurately reflect the context, demonstrating complete consistency and reliability in the output.", "strictMode": false, "evaluationModel": "gpt-4o-mini", "evaluationCost": 0, "verboseLogs": "Verdicts:\n[\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output accurately describes Self RAG as a framework designed to improve the quality and factual accuracy of large language models through retrieval and self-reflection.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output correctly states that Self RAG allows the model to determine when to retrieve relevant information based on the task at hand.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output mentions the generation of reflection tokens, which help the model evaluate its own outputs and the relevance of retrieved information, aligning with the context.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output highlights that Self RAG enhances the model's ability to produce factually accurate content, which is consistent with the context provided.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output discusses the training efficiency of Self RAG, stating that it reduces the need for extensive retraining or human feedback, which is in line with the context.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output explains the input handling process of the model, which is consistent with the context that describes how Self RAG operates.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output correctly describes the use of reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of generated content.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output mentions dynamic decision-making based on the task, which aligns with the context that discusses how Self RAG tailors its behavior.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output states that Self RAG outperforms existing models in various tasks, which is supported by the context that discusses its performance improvements.\"\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": \"The actual output concludes that Self RAG represents a significant advancement in LLM training and utilization, which is consistent with the context's emphasis on its innovative approach.\"\n    }\n]"}, "metric_configuration": {"threshold": 0.3, "evaluation_model": "gpt-4o-mini", "strict_mode": false, "include_reason": true}}]}, "{\"actual_output\": \"# What is Self RAG?\\n\\n**Self RAG** (Self-Reflective Retrieval-Augmented Generation) is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by integrating retrieval and self-reflection mechanisms. Here\\u2019s a breakdown of its key components:\\n\\n## Key Features\\n\\n- **Adaptive Retrieval**: \\n  - Self RAG allows the model to determine when to retrieve relevant information based on the task at hand. This means it doesn't always pull in external data, only when it's deemed necessary.\\n\\n- **Self-Reflection**: \\n  - The model generates special tokens, known as **reflection tokens**, which help it evaluate its own outputs and the relevance of retrieved information. These tokens can indicate whether to retrieve information and assess the quality of its responses.\\n\\n- **Improved Factuality**:\\n  - By combining retrieval with self-reflection, Self RAG enhances the model's ability to produce factually accurate content, reducing errors common in standard LLMs.\\n\\n- **Training Efficiency**:\\n  - The framework trains the model to generate text informed by relevant passages while also learning to critique its own outputs. This process reduces the need for extensive retraining or human feedback.\\n\\n## How It Works\\n\\n1. **Input Handling**: \\n   - The model receives an input and generates an output based on its internal training.\\n\\n2. **Reflection Tokens**: \\n   - It generates reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of its generated content.\\n\\n3. **Dynamic Decision Making**:\\n   - Depending on the task, the model can adjust its behavior to prioritize factual accuracy or creativity.\\n\\n4. **Performance**:\\n   - Experiments show that Self RAG outperforms existing models in various tasks, including open-domain question answering, reasoning, and long-form content generation, by improving accuracy and citation correctness.\\n\\n## Conclusion\\n\\nSelf RAG represents a significant advancement in the way LLMs can be trained and utilized, offering a method to enhance their performance while maintaining versatility and creativity. It effectively combines retrieval-based knowledge with self-assessment to produce higher quality outputs. \\n\\nFor more details on Self RAG, you can check the paper [here](https://selfrag.github.io/).\", \"context\": null, \"expected_output\": null, \"hyperparameters\": null, \"input\": \"What is Self RAG\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy", "threshold": 0.7, "success": true, "score": 1.0, "reason": "The score is 1.00 because the response directly addresses the question about Self RAG without any irrelevant statements.", "strictMode": false, "evaluationModel": "gpt-4o-mini", "evaluationCost": 0, "verboseLogs": "Statements:\n[\n    \"Self RAG stands for Self-Reflective Retrieval-Augmented Generation.\",\n    \"Self RAG is designed to improve the quality and factual accuracy of large language models (LLMs).\",\n    \"The framework integrates retrieval and self-reflection mechanisms.\",\n    \"Adaptive Retrieval allows the model to determine when to retrieve relevant information based on the task.\",\n    \"Self RAG does not always pull in external data, only when deemed necessary.\",\n    \"The model generates special tokens known as reflection tokens.\",\n    \"Reflection tokens help the model evaluate its own outputs and the relevance of retrieved information.\",\n    \"Reflection tokens can indicate whether to retrieve information and assess the quality of responses.\",\n    \"Self RAG enhances the model's ability to produce factually accurate content.\",\n    \"The framework reduces errors common in standard LLMs.\",\n    \"Self RAG trains the model to generate text informed by relevant passages.\",\n    \"The model learns to critique its own outputs.\",\n    \"This process reduces the need for extensive retraining or human feedback.\",\n    \"The model receives an input and generates an output based on its internal training.\",\n    \"It generates reflection tokens to decide if retrieval is necessary.\",\n    \"The model evaluates the relevance and support of its generated content.\",\n    \"Depending on the task, the model can adjust its behavior.\",\n    \"The model can prioritize factual accuracy or creativity.\",\n    \"Experiments show that Self RAG outperforms existing models in various tasks.\",\n    \"Self RAG improves accuracy and citation correctness.\",\n    \"Self RAG represents a significant advancement in the training and utilization of LLMs.\",\n    \"It combines retrieval-based knowledge with self-assessment to produce higher quality outputs.\"\n] \n \nVerdicts:\n[\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    },\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    }\n]"}, "metric_configuration": {"threshold": 0.7, "evaluation_model": "gpt-4o-mini", "strict_mode": false, "include_reason": true}}]}}}