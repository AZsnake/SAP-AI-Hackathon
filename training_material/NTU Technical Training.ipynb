{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8b81bd-9023-47d4-be49-790222188fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "85f2fbb0-a453-40db-95b7-f9caf7d40ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepeval in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.4.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (3.12.15)\n",
      "Requirement already satisfied: anthropic in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (0.64.0)\n",
      "Requirement already satisfied: click<8.3.0,>=8.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (8.2.1)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.32.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.74.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: ollama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (0.5.3)\n",
      "Requirement already satisfied: openai in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.102.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (2.7.0)\n",
      "Collecting posthog<7.0.0,>=6.3.0 (from deepeval)\n",
      "  Downloading posthog-6.7.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: pyfiglet in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.0.4)\n",
      "Requirement already satisfied: pytest in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (8.4.1)\n",
      "Requirement already satisfied: pytest-asyncio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (1.1.0)\n",
      "Requirement already satisfied: pytest-repeat in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (0.9.4)\n",
      "Requirement already satisfied: pytest-rerunfailures<13.0,>=12.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (12.0)\n",
      "Requirement already satisfied: pytest-xdist in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (2.32.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (14.1.0)\n",
      "Requirement already satisfied: sentry-sdk in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (2.35.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (80.9.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (9.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (4.67.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (0.16.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deepeval) (0.45.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from click<8.3.0,>=8.0.0->deepeval) (0.4.6)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.11.7)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.36.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.57b0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.1)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich<15.0.0,>=13.6.0->deepeval) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich<15.0.0,>=13.6.0->deepeval) (2.19.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pytest->deepeval) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pytest->deepeval) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->deepeval) (1.20.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anthropic->deepeval) (0.10.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from portalocker->deepeval) (311)\n",
      "Requirement already satisfied: execnet>=2.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "Downloading posthog-6.7.0-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: posthog\n",
      "  Attempting uninstall: posthog\n",
      "    Found existing installation: posthog 5.4.0\n",
      "    Uninstalling posthog-5.4.0:\n",
      "      Successfully uninstalled posthog-5.4.0\n",
      "Successfully installed posthog-6.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 1.0.20 requires posthog<6.0.0,>=2.4.0, but you have posthog 6.7.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: grpcio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.74.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: grpcio-tools in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.74.0)\n",
      "Collecting protobuf<7.0.0,>=6.31.1 (from grpcio-tools)\n",
      "  Downloading protobuf-6.32.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: grpcio>=1.74.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from grpcio-tools) (1.74.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from grpcio-tools) (80.9.0)\n",
      "Downloading protobuf-6.32.0-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "Successfully installed protobuf-6.32.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\AZsnake\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogen-core 0.7.4 requires protobuf~=5.29.3, but you have protobuf 6.32.0 which is incompatible.\n",
      "mem0ai 0.1.116 requires protobuf<6.0.0,>=5.29.0, but you have protobuf 6.32.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pypdf in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"pydantic=='2.9.2'\": Expected end or semicolon (after name and no valid version specifier)\n",
      "    pydantic=='2.9.2'\n",
      "            ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langgraph in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.6.6)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph) (0.3.75)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph) (0.2.3)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core>=0.1->langgraph) (0.4.20)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autogen in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.9.9)\n",
      "Requirement already satisfied: ag2==0.9.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen) (0.9.9)\n",
      "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (4.10.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (0.0.8)\n",
      "Requirement already satisfied: diskcache in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (5.6.3)\n",
      "Requirement already satisfied: docker in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (7.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.28.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (0.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (1.1.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (3.1.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ag2==0.9.9->autogen) (0.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.9->autogen) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.9->autogen) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.28.1->ag2==0.9.9->autogen) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.28.1->ag2==0.9.9->autogen) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.9.9->autogen) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (0.4.1)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from docker->ag2==0.9.9->autogen) (311)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from docker->ag2==0.9.9->autogen) (2.32.5)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from docker->ag2==0.9.9->autogen) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.26.0->docker->ag2==0.9.9->autogen) (3.4.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tiktoken->ag2==0.9.9->autogen) (2025.7.34)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autogen-agentchat in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.7.4)\n",
      "Requirement already satisfied: autogen-core==0.7.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-agentchat) (0.7.4)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-agentchat) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-agentchat) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-agentchat) (11.3.0)\n",
      "Collecting protobuf~=5.29.3 (from autogen-core==0.7.4->autogen-agentchat)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-agentchat) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-agentchat) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-agentchat) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-agentchat) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-agentchat) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-agentchat) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-agentchat) (3.23.0)\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.32.0\n",
      "    Uninstalling protobuf-6.32.0:\n",
      "      Successfully uninstalled protobuf-6.32.0\n",
      "Successfully installed protobuf-5.29.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.74.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 5.29.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autogen-core in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.7.4)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core) (11.3.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-api>=1.34.1->autogen-core) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core) (3.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autogen-ext in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.7.4)\n",
      "Requirement already satisfied: autogen-core==0.7.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-ext) (0.7.4)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-ext) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-ext) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-ext) (11.3.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-ext) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-ext) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from autogen-core==0.7.4->autogen-ext) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-ext) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-ext) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-ext) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-ext) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-ext) (3.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: crewai in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.175.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.4.4)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.9.0)\n",
      "Requirement already satisfied: chromadb>=0.5.23 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.0.20)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (8.2.1)\n",
      "Requirement already satisfied: instructor>=1.3.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.11.2)\n",
      "Requirement already satisfied: json-repair==0.25.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (0.25.2)\n",
      "Requirement already satisfied: json5>=0.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (0.12.1)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.1.0)\n",
      "Requirement already satisfied: litellm==1.74.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.74.9)\n",
      "Requirement already satisfied: onnxruntime==1.22.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.22.0)\n",
      "Requirement already satisfied: openai>=1.13.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.102.0)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.30.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.30.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.36.0)\n",
      "Requirement already satisfied: pdfplumber>=0.11.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (0.11.7)\n",
      "Requirement already satisfied: portalocker==2.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (2.7.0)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (2.11.7)\n",
      "Requirement already satisfied: pyjwt>=2.9.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (2.10.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.1.1)\n",
      "Requirement already satisfied: pyvis>=0.3.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (0.3.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers>=0.20.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (0.21.4)\n",
      "Requirement already satisfied: tomli-w>=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (1.2.0)\n",
      "Requirement already satisfied: tomli>=2.0.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (2.2.1)\n",
      "Requirement already satisfied: uv>=0.4.25 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from crewai) (0.8.13)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm==1.74.9->crewai) (3.12.15)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm==1.74.9->crewai) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm==1.74.9->crewai) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm==1.74.9->crewai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm==1.74.9->crewai) (4.25.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm==1.74.9->crewai) (0.11.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from onnxruntime==1.22.0->crewai) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from onnxruntime==1.22.0->crewai) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from onnxruntime==1.22.0->crewai) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from onnxruntime==1.22.0->crewai) (25.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from onnxruntime==1.22.0->crewai) (5.29.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from onnxruntime==1.22.0->crewai) (1.14.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from portalocker==2.7.0->crewai) (311)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.9->crewai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (0.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.4.2->crewai) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.4.2->crewai) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.4.2->crewai) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm==1.74.9->crewai) (3.10)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (0.35.0)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=0.5.23->crewai)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (1.36.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (0.16.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (3.11.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from chromadb>=0.5.23->crewai) (14.1.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (2025.8.3)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from build>=1.0.3->chromadb>=0.5.23->crewai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from build>=1.0.3->chromadb>=0.5.23->crewai) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.23.0->litellm==1.74.9->crewai) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.23.0->litellm==1.74.9->crewai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.74.9->crewai) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from importlib-metadata>=6.8.0->litellm==1.74.9->crewai) (3.23.0)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from instructor>=1.3.3->crewai) (5.6.3)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from instructor>=1.3.3->crewai) (0.17.0)\n",
      "Requirement already satisfied: jiter<0.11,>=0.6.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from instructor>=1.3.3->crewai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai) (2.19.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer>=0.9.0->chromadb>=0.5.23->crewai) (1.5.4)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.6.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai) (0.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.23->crewai) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.23->crewai) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.23->crewai) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opentelemetry-sdk>=1.30.0->crewai) (0.57b0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pdfplumber>=0.11.4->crewai) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pdfplumber>=0.11.4->crewai) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pdfplumber>=0.11.4->crewai) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (44.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (2.22)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pyvis>=0.3.2->crewai) (9.4.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pyvis>=0.3.2->crewai) (4.1.1)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pyvis>=0.3.2->crewai) (3.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.52)\n",
      "Requirement already satisfied: stack_data in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tokenizers>=0.20.3->crewai) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (2025.7.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from coloredlogs->onnxruntime==1.22.0->crewai) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime==1.22.0->crewai) (3.5.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy->onnxruntime==1.22.0->crewai) (1.3.0)\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Installing collected packages: posthog\n",
      "  Attempting uninstall: posthog\n",
      "    Found existing installation: posthog 6.7.0\n",
      "    Uninstalling posthog-6.7.0:\n",
      "      Successfully uninstalled posthog-6.7.0\n",
      "Successfully installed posthog-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepeval 3.4.1 requires posthog<7.0.0,>=6.3.0, but you have posthog 5.4.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement crewai-tool (from versions: none)\n",
      "ERROR: No matching distribution found for crewai-tool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: presidio-analyzer in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.359)\n",
      "Requirement already satisfied: phonenumbers<10.0.0,>=8.12 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from presidio-analyzer) (9.0.12)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from presidio-analyzer) (6.0.2)\n",
      "Requirement already satisfied: regex in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from presidio-analyzer) (2025.7.34)\n",
      "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from presidio-analyzer) (3.8.7)\n",
      "Requirement already satisfied: tldextract in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from presidio-analyzer) (5.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.16.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.0.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tldextract->presidio-analyzer) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tldextract->presidio-analyzer) (3.19.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: presidio-anonymizer in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.359)\n",
      "Requirement already satisfied: cryptography<44.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from presidio-anonymizer) (44.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from cryptography<44.1->presidio-anonymizer) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from cffi>=1.12->cryptography<44.1->presidio-anonymizer) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: html2text in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2025.4.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sec-api in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.0.32)\n",
      "Requirement already satisfied: requests in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sec-api) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->sec-api) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->sec-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->sec-api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->sec-api) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: litellm in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.74.9)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (3.12.15)\n",
      "Requirement already satisfied: click in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (8.2.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.68.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (1.102.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (1.1.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (0.11.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from litellm) (0.21.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp>=3.10->litellm) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm) (3.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.23.0->litellm) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.23.0->litellm) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.68.2->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.68.2->litellm) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.68.2->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.68.2->litellm) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tiktoken>=0.7.0->litellm) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>4->openai>=1.68.2->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tokenizers->litellm) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai-agents in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.2.9)\n",
      "Requirement already satisfied: griffe<2,>=1.5.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (1.13.0)\n",
      "Requirement already satisfied: mcp<2,>=1.11.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (1.13.1)\n",
      "Requirement already satisfied: openai<2,>=1.99.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (1.102.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.10 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (2.32.5)\n",
      "Requirement already satisfied: types-requests<3,>=2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (2.32.4.20250809)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.12.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai-agents) (4.15.0)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from griffe<2,>=1.5.6->openai-agents) (0.4.6)\n",
      "Requirement already satisfied: anyio>=4.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (4.10.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.1)\n",
      "Requirement already satisfied: httpx>=0.27.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (0.47.3)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mcp<2,>=1.11.0->openai-agents) (0.35.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2,>=1.99.6->openai-agents) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2,>=1.99.6->openai-agents) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2,>=1.99.6->openai-agents) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2,>=1.99.6->openai-agents) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio>=4.5->mcp<2,>=1.11.0->openai-agents) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.10->openai-agents) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3,>=2.0->openai-agents) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents) (1.1.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_aws in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.2.31)\n",
      "Requirement already satisfied: boto3>=1.39.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_aws) (1.40.19)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.74 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_aws) (0.3.75)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_aws) (2.3.2)\n",
      "Requirement already satisfied: pydantic<3,>=2.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_aws) (2.11.7)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<0.4.0,>=0.3.74->langchain_aws) (0.4.20)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<0.4.0,>=0.3.74->langchain_aws) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<0.4.0,>=0.3.74->langchain_aws) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<0.4.0,>=0.3.74->langchain_aws) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<0.4.0,>=0.3.74->langchain_aws) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<0.4.0,>=0.3.74->langchain_aws) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.74->langchain_aws) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.10.0->langchain_aws) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.10.0->langchain_aws) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3,>=2.10.0->langchain_aws) (0.4.1)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.19 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from boto3>=1.39.7->langchain_aws) (1.40.19)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from boto3>=1.39.7->langchain_aws) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from boto3>=1.39.7->langchain_aws) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from botocore<1.41.0,>=1.40.19->boto3>=1.39.7->langchain_aws) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from botocore<1.41.0,>=1.40.19->boto3>=1.39.7->langchain_aws) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.19->boto3>=1.39.7->langchain_aws) (1.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (3.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.74->langchain_aws) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_community in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (0.3.75)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2.32.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (0.4.20)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_community) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3,>=2.32.5->langchain_community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.1.125->langchain_community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.3.32)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_openai) (0.3.75)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_openai) (1.102.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.20)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.5)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.24.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>4->openai<2.0.0,>=1.99.9->langchain_openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mem0ai in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.1.116)\n",
      "Requirement already satisfied: openai>=1.33.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (1.102.0)\n",
      "Requirement already satisfied: posthog>=3.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (5.4.0)\n",
      "Requirement already satisfied: protobuf<6.0.0,>=5.29.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (5.29.5)\n",
      "Requirement already satisfied: pydantic>=2.7.3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (2.11.7)\n",
      "Requirement already satisfied: pytz>=2024.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (2025.2)\n",
      "Requirement already satisfied: qdrant-client>=1.9.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (1.15.1)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.31 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from mem0ai) (2.0.43)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openai>=1.33.0->mem0ai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio<5,>=3.5.0->openai>=1.33.0->mem0ai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->openai>=1.33.0->mem0ai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1,>=0.23.0->openai>=1.33.0->mem0ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->mem0ai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.3->mem0ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.3->mem0ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic>=2.7.3->mem0ai) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog>=3.5.0->mem0ai) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog>=3.5.0->mem0ai) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog>=3.5.0->mem0ai) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from posthog>=3.5.0->mem0ai) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0,>=2.7->posthog>=3.5.0->mem0ai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0,>=2.7->posthog>=3.5.0->mem0ai) (2.5.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from qdrant-client>=1.9.1->mem0ai) (1.74.0)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from qdrant-client>=1.9.1->mem0ai) (2.3.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from qdrant-client>=1.9.1->mem0ai) (2.7.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client>=1.9.1->mem0ai) (311)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai) (4.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai) (4.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sqlalchemy>=2.0.31->mem0ai) (3.2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\azsnake\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>4->openai>=1.33.0->mem0ai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-cache-dir deepeval\n",
    "%pip install --no-cache-dir grpcio\n",
    "%pip install --no-cache-dir grpcio-tools\n",
    "%pip install --no-cache-dir pypdf\n",
    "%pip install --no-cache-dir pydantic=='2.9.2'\n",
    "%pip install --no-cache-dir langgraph\n",
    "%pip install --no-cache-dir autogen\n",
    "%pip install --no-cache-dir autogen-agentchat\n",
    "%pip install --no-cache-dir autogen-core\n",
    "%pip install --no-cache-dir autogen-ext\n",
    "%pip install --no-cache-dir crewai\n",
    "%pip install --no-cache-dir crewai-tool\n",
    "%pip install --no-cache-dir presidio-analyzer\n",
    "%pip install --no-cache-dir presidio-anonymizer\n",
    "%pip install --no-cache-dir html2text\n",
    "%pip install --no-cache-dir sec-api\n",
    "%pip install --no-cache-dir litellm\n",
    "%pip install --no-cache-dir openai-agents\n",
    "%pip install --no-cache-dir langchain_aws\n",
    "%pip install --no-cache-dir langchain_community\n",
    "%pip install --no-cache-dir langchain_openai\n",
    "%pip install --no-cache-dir mem0ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b362dd-7fef-4b80-a96d-381c8e2d6b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Restart Python Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad77e9f-2b25-49e9-af00-59cf96b1c433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e295090e-5ccc-44bc-a798-cdad69f2f065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c783ccc-e10a-417c-9558-a81d18029e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "config_path = \"config.json\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "  config = json.load(f)\n",
    "\n",
    "openai_api_key=config[\"OpenAIAPIKey\"]\n",
    "openai_model=config[\"OpenAIModelName\"]\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b58f3a-2e3e-4a0e-adbc-2731789984ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Initialize LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b157b0-1d55-4e60-b85b-af370fcdbf4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def init_llm_model(model: str, temperature: float=0.7):\n",
    "  return ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "def init_embedding_model(model: str):\n",
    "  return OpenAIEmbeddings(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae5dc27-1ac2-4fe8-bc8b-a3f03d5243d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Single Agent System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da508744-f34a-4389-9ea9-3ca26c867a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is Single Agent System?\n",
    "\n",
    "A single-agent system consists of an agent responsible for performing a particular task. Such systems are easier to implement and maintain and can be applied successfully in well-defined problems in a stable environment. Examples are virtual assistants or chat agents who can answer questions without extra support. However, in interactivity, adaptability, or collaboration at high levels of work, these systems do not perform well since they tend to focus too much on one thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23950b94-b06b-43ed-a1bd-8d95ac14ae80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agentic RAG with LangGraph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c762d99d-7ca3-4404-bf5c-f18279e05718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Self RAG](./Self%20RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367113dc-2658-4cab-a542-5c665926647a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Index documents, including the Self-RAG research paper and Microsoft's study on Generative AI's impact on critical thinking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e2d13715-f333-4a21-ae1a-63380cbbd8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0: Preprint.\n",
      "SELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND\n",
      "CRITIQUE THROUGH SELF -REFLECTION\n",
      "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi\n",
      "University of Washington Allen Institute for AI IBM Research AI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despite their remarkable capabilities, large language models (LLMs) often produce\n",
      "responses containing factual inaccuracies due to their sole reliance on the paramet-\n",
      "ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\n",
      "hoc approach that augments LMs with retrieval of relevant knowledge, decreases\n",
      "such issues. However, indiscriminately retrieving and incorporating a fixed number\n",
      "of retrieved passages, regardless of whether retrieval is necessary, or passages are\n",
      "relevant, diminishes LM versatility or can lead to unhelpful response generation.\n",
      "We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\n",
      "eration (SELF -RAG) that enhances an LMs quality and factuality through retrieval\n",
      "and self-reflection. Our framework trains a single arbitrary LM that adaptively\n",
      "retrieves passages on-demand, and generates and reflects on retrieved passages\n",
      "and its own generations using special tokens, called reflection tokens. Generating\n",
      "reflection tokens makes the LM controllable during the inference phase, enabling it\n",
      "to tailor its behavior to diverse task requirements. Experiments show that SELF -\n",
      "RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\n",
      "and retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG\n",
      "outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\n",
      "reasoning and fact verification tasks, and it shows significant gains in improving\n",
      "factuality and citation accuracy for long-form generations relative to these models.1\n",
      "1 I NTRODUCTION\n",
      "State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\n",
      "despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n",
      "(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\n",
      "with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n",
      "2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\n",
      "unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\n",
      "retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\n",
      "the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\n",
      "the models are not explicitly trained to leverage and follow facts from provided passages. This\n",
      "work introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an\n",
      "LLMs generation quality, including its factual accuracy without hurting its versatility, via on-demand\n",
      "retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\n",
      "its own generation process given a task input by generating both task output and intermittent special\n",
      "tokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\n",
      "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
      "given an input prompt and preceding generations, SELF -RAG first determines if augmenting the\n",
      "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\n",
      "calls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\n",
      "retrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\n",
      "2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\n",
      "of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n",
      "1Our code and trained models are available at https://selfrag.github.io/.\n",
      "1\n",
      "arXiv:2310.11511v1  [cs.CL]  17 Oct 2023\n",
      "\n",
      "Page 5: Preprint.\n",
      "separate reward models during training, we compute critique offline and directly insert them into the\n",
      "training corpus, where the generator LM is trained with a standard LM objective. This significantly\n",
      "reduces training costs compared to PPO. Our work also relates to prior work that incorporates special\n",
      "tokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\n",
      "learns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\n",
      "the use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n",
      "3.3 S ELF -RAG INFERENCE\n",
      "Generating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\n",
      "inference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\n",
      "factual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\n",
      "ensure that the output aligns closely with the available evidence. Conversely, in more open-ended\n",
      "tasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\n",
      "prioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\n",
      "control to meet these distinct objectives during the inference process.\n",
      "Adaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by\n",
      "predicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\n",
      "ability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\n",
      "designated threshold, we trigger retrieval (details in Appendix Section A.3).\n",
      "Tree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\n",
      "on hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\n",
      "parallel and outputs K different continuation candidates. We conduct a segment-level beam search\n",
      "(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\n",
      "the best sequence at the end of generation. The score of each segment yt with respect to passage d is\n",
      "updated with a critic score S that is the linear weighted sum of the normalized probability of each\n",
      "Critique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp\n",
      "t as sG\n",
      "t , and we compute a segment score as follows:\n",
      "f(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)\n",
      "S( Critique ) =\n",
      "X\n",
      "GG\n",
      "wGsG\n",
      "t for G = { ISREL , ISSUP , ISUSE }, (4)\n",
      "where sG\n",
      "t = pt(r)PNG\n",
      "i=1 pt(ri)\n",
      "stands for the generation probability of the most desirable reflection token\n",
      "r (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent\n",
      "different possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\n",
      "at inference time to enable customized behaviors at test time. For instance, to ensure that result\n",
      "y is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while\n",
      "relatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\n",
      "during decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\n",
      "filter out a segment continuation when the model generates an undesirable Critique token (e.g.,\n",
      "ISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\n",
      "in RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\n",
      "behaviors. SELF -RAG tailors an LM with no additional training.\n",
      "4 E XPERIMENTS\n",
      "4.1 T ASKS AND DATASETS\n",
      "We conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\n",
      "holistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\n",
      "fluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\n",
      "tions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\n",
      "our experiments settings, including test-time instructions, are available in the Appendix Section B.1.\n",
      "Closed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;\n",
      "Zhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\n",
      "6\n",
      "\n",
      "Page 3: Preprint.\n",
      "Type Input Output Definitions\n",
      "Retrieve x / x, y {yes, no, continue} Decides when to retrieve with R\n",
      "ISREL x, d {relevant, irrelevant} d provides useful information to solve x.\n",
      "ISSUP x, d, y {fully supported, partially\n",
      "supported, no support}\n",
      "All of the verification-worthy statement in y\n",
      "is supported by d.\n",
      "ISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.\n",
      "Table 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent\n",
      "its output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\n",
      "the most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.\n",
      "Algorithm 1 SELF -RAG Inference\n",
      "Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }\n",
      "1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\n",
      "2: M predicts Retrieve given (x, y<t)\n",
      "3: if Retrieve == Yes then\n",
      "4: Retrieve relevant text passages D using R given (x, yt1)  Retrieve\n",
      "5: M predicts ISREL given x, dand yt given x, d, y<t for each d  D  Generate\n",
      "6: M predicts ISSUP and ISUSE given x, yt, dfor each d  D  Critique\n",
      "7: Rank yt based on ISREL , ISSUP , ISUSE  Detailed in Section 3.3\n",
      "8: else if Retrieve == No then\n",
      "9: Mgen predicts yt given x  Generate\n",
      "10: Mgen predicts ISUSE given x, yt  Critique\n",
      "Inference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\n",
      "every x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\n",
      "of retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\n",
      "standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\n",
      "passages relevance, the next response segment, and a critique token to evaluate if the information in\n",
      "the response segment is supported by the passage. Finally, a new critique token evaluates the overall\n",
      "utility of the response.4 To generate each segment, SELF -RAG processes multiple passages in parallel\n",
      "and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n",
      "(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\n",
      "d1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\n",
      "and d3 output is only partially supported while d1 are fully supported.\n",
      "Training overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\n",
      "by unifying them as next token predictions from the expanded model vocabulary (i.e., the original\n",
      "vocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\n",
      "with interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\n",
      "C (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\n",
      "retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\n",
      "update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\n",
      "train the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\n",
      "M to generate reflection tokens by itself without relying on the critic at inference time.\n",
      "3.2 S ELF -RAG TRAINING\n",
      "Here, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)\n",
      "and the generator M (Section 3.2.2).\n",
      "3.2.1 T RAINING THE CRITIC MODEL\n",
      "Data collection for critic model. Manual annotation of reflection tokens for each segment is\n",
      "expensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n",
      "4We follow Liu et al. (2023a) in using a perceived utility value that is independent of retrieved passages.\n",
      "4\n",
      "\n",
      "Page 9: Preprint.\n",
      "0 50 100 150\n",
      "Num of training (k)\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55Perfomance\n",
      "(a) PopQA\n",
      "0 100\n",
      "Num of training (k)\n",
      "71\n",
      "72\n",
      "73\n",
      " (b) PubHealth\n",
      "0 100\n",
      "Num of training (k)\n",
      "40\n",
      "60\n",
      " (c) ASQA (prec)\n",
      "Pop Bio.\n",
      "S & P 92.5 70.0\n",
      "ISREL 95.0 90.0\n",
      "ISSUP 90.0 85.0\n",
      "(d) Human evaluation on PopQA\n",
      "and Bio generation.\n",
      "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\n",
      "of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n",
      "Human analysis on SELF -RAG outputs as well as reflection tokens.\n",
      "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\n",
      "fluent, there are often more claims that are not fully supported by citations, consistent with findings\n",
      "by Liu et al. (2023a). Our framework lets practitioners choose and customize models behaviors at\n",
      "test time by adjusting such parameters without requiring additional training.\n",
      "Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\n",
      "occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n",
      "overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n",
      "of threshold  (larger  results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\n",
      "the models retrieval frequencies dramatically change on both datasets. as  varies. On one hand,\n",
      "performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n",
      "Effects of training data size. We conduct an analysis of how the data scale affects the models\n",
      "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
      "150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare\n",
      "the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -\n",
      "RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\n",
      "performance trained on different amount of data. Across all datasets, increasing data size often shows\n",
      "upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\n",
      "not observed such significant improvements on Llama2-FT7B when increasing the training data from\n",
      "50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may\n",
      "lead to further improvements, although in this work we limit our training data size to 150k.\n",
      "Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the\n",
      "reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\n",
      "results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\n",
      "the model output is plausible (i.e., the output is a reasonable and on-topic response to the question\n",
      "as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\n",
      "verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG\n",
      "predicts irrelevant or no support. We then ask our annotators whether the model-predicted\n",
      "reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported\n",
      "output is supported by the cited evidence). Human annotators find SELF -RAG answers are often\n",
      "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
      "consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\n",
      "predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\n",
      "examples and explanations on assessments.\n",
      "6 C ONCLUSION\n",
      "This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\n",
      "through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables\n",
      "the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\n",
      "six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with\n",
      "more parameters or with conventional retrieval-augmented generation approaches.\n",
      "10\n",
      "\n",
      "Page 8: Preprint.\n",
      "PQA Med AS\n",
      "(acc) (acc) (em)\n",
      "SELF -RAG (50k) 45.5 73.5 32.1\n",
      "Training\n",
      "No Retriever R 43.6 67.8 31.0\n",
      "No Critic C 42.6 72.0 18.1\n",
      "Test\n",
      "No retrieval 24.7 73.0 \n",
      "Hard constraints 28.3 72.6 \n",
      "Retrieve top1 41.8 73.1 28.6\n",
      "Remove ISSUP 44.1 73.2 30.6\n",
      "(a) Ablation\n",
      "1 2\n",
      "70.0\n",
      "70.5Precision\n",
      "1 2\n",
      "Weight for IsSupport\n",
      "90\n",
      "95Mauve\n",
      " (b) Customization\n",
      "0.0 0.2 0.4 0.6\n",
      "0.98\n",
      "0.99\n",
      "0.99\n",
      "1.00Accuracy\n",
      "PubHealth\n",
      "0.0 0.2 0.4 0.6\n",
      "Retrieval Threshold\n",
      "0.6\n",
      "0.8\n",
      "1.0Accuracy\n",
      "PopQA\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "Frequency\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Frequency\n",
      " (c) Retrieval\n",
      "Figure 3: Analysis on SELF -RAG: (a) Ablation studies for key components of SELF -RAG training\n",
      "and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\n",
      "Mauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\n",
      "precisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\n",
      "instruction-output pairs as SELF -RAG without retrieval or self-reflection and is retrieval-augmented\n",
      "at test time only, lags behind SELF -RAG. This result indicates S ELF -RAG gains are not solely from\n",
      "training data and demonstrate the effectiveness of SELF -RAG framework.\n",
      "5.2 A NALYSIS\n",
      "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\n",
      "key roles. We evaluate two model variants trained differently than our model: No Retriever trains an\n",
      "LM using the standard instruction-following method given instruction-output pairs, without retrieved\n",
      "passages; No Critic trains an LM trained with input-output pairs that are always augmented with the\n",
      "top one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\n",
      "we use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\n",
      "SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\n",
      "retrieval during inference; Hard constraints indicates the model performance that retrieves when\n",
      "Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\n",
      "top one document only, similar to standard RAG approaches;Remove ISSUP indicates the model\n",
      "performance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this\n",
      "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
      "variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\n",
      "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
      "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
      "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
      "and the bottom part is for inference ablations. We see that all components play important roles. We\n",
      "also observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\n",
      "tasks, indicating that training an LM with those models largely contributes to the performance gain of\n",
      "SELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\n",
      "RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\n",
      "search results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAGs\n",
      "capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\n",
      "using all of the top passages from the retrieval model or solely depending on relevance scores.\n",
      "Effects of inference-time customization. One key benefit of our proposed framework is that it\n",
      "enables us to control how much each critique type affects the final generation sampling. We analyze\n",
      "the effects of different parameter weights on the top of our 7B model during inference time on\n",
      "ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\n",
      "the weighting term for ISSUP , which criticizes how supported the output is by the text passage. As\n",
      "the figure shows, increasing the weight leads to positive effects on the models citation precision\n",
      "since this puts more emphasis on whether model generation is supported by the evidence. On the\n",
      "9\n",
      "\n",
      "Page 15: Preprint.\n",
      "APPENDIX\n",
      "A S ELF -RAG Details 17\n",
      "A.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B Experimental Details 19\n",
      "B.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C Results 20\n",
      "C.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "C.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "D Full List of Instructions and Demonstrations for GPT-4 21\n",
      "16\n",
      "\n",
      "Page 16: Preprint.\n",
      "A S ELF -RAG DETAILS\n",
      "A.1 R EFLECTION TOKENS .\n",
      "Definitions of reflection tokens. Below, we provide a detailed definition of reflection type and\n",
      "output tokens. The first three aspects will be provided at each segment level, while the final aspect is\n",
      "only given at each output level.\n",
      " Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\n",
      "an LM determines whether the continuation requires factual grounding. No indicates retrieval\n",
      "is unnecessary as the sequence does not require factual grounding or may not be enhanced by\n",
      "knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\n",
      "to use evidence, which indicates that a model can continue to use the evidence retrieved\n",
      "previously. For instance, a passage may contain rich factual information, and thus SELF -RAG\n",
      "generates multiple segments based on the passage.\n",
      " Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\n",
      "indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n",
      " Supported ( ISSUP ): Attribution is the concept of whether the output is fully supported by\n",
      "certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\n",
      "mation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\n",
      "supported, Partially supported, and No support / Contradictory, follow-\n",
      "ing Yue et al. (2023); Nakano et al. (2021).\n",
      " Useful ( ISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\n",
      "as whether the response is a helpful and informative answer to the query, independently from\n",
      "whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\n",
      "For usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\n",
      "Details of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\n",
      "GPT-4, listed in Section D. Following an official recommendation, we separate instructions and\n",
      "outputs with ##. We use the temperature 1 and set the maximum output token counts to be 200. We\n",
      "discard instances where GPT-4 does not follow the designated output formats or output sequences\n",
      "that do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\n",
      "for ISSUP , 19,317 for relevance, 3,831 for utility.\n",
      "Manual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\n",
      "sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\n",
      "the same instruction, demonstrations, and test instances. We found our assessments show high\n",
      "agreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\n",
      "the degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\n",
      "disagreement between 1 and 2 or 4 and 5.\n",
      "A.2 S ELF -RAG TRAINING\n",
      "Overview of training. Algorithm 2 provides a high-level overview of our training.\n",
      "Full list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\n",
      "Instruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\n",
      "OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\n",
      "intensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\n",
      "2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\n",
      "makh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\n",
      "2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n",
      "Performance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\n",
      "generated feedback into training, development, and test sets. The accuracy of the reward model is\n",
      "as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\n",
      "overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\n",
      "17\n",
      "\n",
      "Page 2: Preprint.\n",
      "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\n",
      "shot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\n",
      "once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\n",
      "on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\n",
      "entities. Yet, the improved task performance of such approaches often comes at the expense of\n",
      "runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\n",
      "attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\n",
      "learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\n",
      "generation guided by reflections tokens to further improve generation quality and attributions.\n",
      "Concurrent RAG work. A few concurrent works2 on RAG propose new training or prompting\n",
      "strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\n",
      "and LM on instruction-tuning datasets in two steps. While we also train our model on diverse\n",
      "instruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best\n",
      "possible model output via fine-grained self-reflection, making it widely applicable and more robust\n",
      "and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\n",
      "a summarization model to filter out or compress retrieved passages before using them to prompt the\n",
      "LM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones\n",
      "through self-reflection, without relying on external models at inference. Moreover, our self-reflection\n",
      "mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\n",
      "et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\n",
      "and to generate with tree search, guided by LM-generated value scores. While their value function\n",
      "simply indicates an overall score of each generation, SELF -RAG trains to an arbitrary LM to learn to\n",
      "generate fine-grained self-reflection and customizable inference.\n",
      "Training and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\n",
      "Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\n",
      "effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\n",
      "fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\n",
      "on retrieval and generation, we train our target LM on task examples augmented with reflection\n",
      "tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\n",
      "reflection tokens in SELF -RAG enable controllable generation at inference, while RLHF focuses on\n",
      "human preference alignment during training. Other works use general control tokens to guide LM\n",
      "generation (Lu et al., 2022; Korbak et al., 2023), whileSELF -RAG uses reflection tokens to decide the\n",
      "need for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\n",
      "guided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n",
      "(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\n",
      "et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\n",
      "language feedback and refined task output iteratively, but at the cost of inference efficiency.\n",
      "3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\n",
      "We introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\n",
      "SELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\n",
      "self-reflection, without sacrificing LLMs original creativity and versatility. Our end-to-end training\n",
      "lets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\n",
      "learning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\n",
      "or confirm the outputs relevance, support, or completeness. In contrast, common RAG approaches\n",
      "retrieve passages indiscriminately, without ensuring complete support from cited sources.\n",
      "3.1 P ROBLEM FORMALIZATION AND OVERVIEW\n",
      "Formally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\n",
      "segments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\n",
      "tokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\n",
      "2All work is arXived within a week of this preprint.\n",
      "3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\n",
      "segment unit (i.e., sub-sentence).\n",
      "3\n",
      "\n",
      "Page 7: Preprint.\n",
      "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\n",
      "non-proprietary models, and gray-colored bold text indicates the best proprietary model when\n",
      "they outperforms all non-proprietary models.  indicates concurrent or recent results reported by\n",
      "concurrent work.  indicates numbers that are not reported by the original papers or are not applicable.\n",
      "Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\n",
      "rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n",
      "Short-form Closed-set Long-form generations (with citations)\n",
      "PopQA TQA Pub ARC Bio ASQA\n",
      "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
      "LMs with proprietary data\n",
      "Llama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6  \n",
      "Ret-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\n",
      "ChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8  \n",
      "Ret-ChatGPT 50.8 65.7 54.7 75.3  40.7 39.9 79.7 65.1 76.6\n",
      "Perplexity.ai     71.2     \n",
      "Baselines without retrieval\n",
      "Llama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0  \n",
      "Alpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7  \n",
      "Llama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0  \n",
      "Alpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6  \n",
      "CoVE65B *     71.2     \n",
      "Baselines with retrieval\n",
      "Toolformer*6B  48.8        \n",
      "Llama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\n",
      "Alpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\n",
      "Llama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\n",
      "SAIL*7B   69.2 48.4      \n",
      "Llama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\n",
      "Alpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\n",
      "Our SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\n",
      "Our SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n",
      "5 R ESULTS AND ANALYSIS\n",
      "5.1 M AIN RESULTS\n",
      "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\n",
      "retrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\n",
      "over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\n",
      "biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\n",
      "a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\n",
      "task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\n",
      "iteratively prompts Llama265B to refine output.\n",
      "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF -RAG also\n",
      "outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\n",
      "LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\n",
      "powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\n",
      "their non-retrieval baselines. However, we found that these baselines provide limited solutions for\n",
      "tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\n",
      "and ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\n",
      "retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\n",
      "accuracy. On ASQA, our model shows significantly higher citation precision and recall than all\n",
      "models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\n",
      "in this particular task, surpassing smaller LMs. Our SELF -RAG bridges this performance gap, even\n",
      "outperforming ChatGPT in citation precision, which measures whether the model-generated claim is\n",
      "fully supported by cited evidence. We also found that on the metrics for factual precision, SELF -RAG\n",
      "7B occasionally outperforms our 13B due to the tendency of smaller SELF -RAG to often generate\n",
      "8\n",
      "\n",
      "Page 17: Preprint.\n",
      "Algorithm 2 SELF -RAG Training\n",
      "1: Input input-output data D = {X, Y}, generator M, C \n",
      "2: Initialize C with a pre-trained LM\n",
      "3: Sample data {Xsample, Ysample}  {X, Y}  Training Critic LM (Section 3.2.1)\n",
      "4: for (x, y)  (Xsample, Ysample) do  Data collections for C\n",
      "5: Prompt GPT-4 to collect a reflection token r for (x, y)\n",
      "6: Add {(x, y, r)} to Dcritic\n",
      "7: Update C with next token prediction loss  Critic learning; Eq. 1\n",
      "8: Initialize M with a pre-trained LM  Training Generator LM (Section 3.2.2)\n",
      "9: for (x, y)  (X, Y) do  Data collection for M with Dcritic\n",
      "10: Run C to predict r given (x, y)\n",
      "11: Add (x, y, r) to Dgen\n",
      "12: Update M on Dgen with next token prediction loss  Generator LM learning; Eq. 2\n",
      "Dataset name category Data source the number of instances\n",
      "GPT-4 Alpaca Instruction-following Open-Instruct 26,168\n",
      "Stanford Alpaca Instruction-following Open-Instruct 25,153\n",
      "FLAN-V2 Instruction-following Open-Instruct 17,817\n",
      "ShareGPT Instruction-following Open-Instruct 13,406\n",
      "Open Assistant 1 Instruction-following Open-Instruct 9,464\n",
      "Wizard of Wikipedia Knowledge-intensive KILT 17,367\n",
      "Natural Questions Knowledge-intensive KILT 15,535\n",
      "FEVER Knowledge-intensive KILT 9,966\n",
      "OpenBoookQA Knowledge-intensive HF Dataset 4,699\n",
      "Arc-Easy Knowledge-intensive HF Dataset 2,147\n",
      "ASQA Knowledge-intensive ASQA 3,897\n",
      "Table 3: The generator LM M training data statistics.\n",
      "base LM Retrieve ISSUP ISREL ISUSE\n",
      "Llama2-7B 93.8 93.5 80.2 73.5\n",
      "FLAN-3B 85.6 73.1 82.0 72.1\n",
      "Figure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n",
      "While our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\n",
      "et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\n",
      "reward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\n",
      "the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\n",
      "lower performance on ISUSE , this is because both models often confuse between the two highest\n",
      "cases (5 and 4), where human annotators can also disagree.\n",
      "Details of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\n",
      "summarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\n",
      "first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\n",
      "not. For the instances where the critic predicts Retrieve =No, we only predict the ISUSE given input\n",
      "and output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\n",
      "the input and the entire output as queries, to find passages that are relevant to the entire output. We\n",
      "then split output sentences using Spacy.7 For each sentence, we run C to predict whether the retrieval\n",
      "is necessary or not, given the input, preceding segments, and the initial retrieved passage. IfC predicts\n",
      "Retrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\n",
      "we use the original input and the tth segment as a retrieval query to find relevant passages for the\n",
      "t-th segment. For each retrieved passage, we predict ISREL and ISSUP . If there is any passage and\n",
      "continuation with ISREL =Relevant and ISSUP =Fully Supported / ISSUP =Partially\n",
      "7https://spacy.io/\n",
      "18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "embedding_model = init_embedding_model('text-embedding-ada-002')\n",
    "\n",
    "files = ['./Self RAG.pdf', './GenAI Impact on Critical Thinking.pdf']\n",
    "pages = []\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    async for page in loader.alazy_load():\n",
    "      pages.append(page)\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(pages, embedding_model)\n",
    "documents = vector_store.similarity_search(\"What is Self RAG\", k=10) # Get the top 10 relevant results from the document\n",
    "for document in documents:\n",
    "  print(f'Page {document.metadata[\"page\"]}: {document.page_content}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e757d889-48a9-42a1-ab36-9abaf5ce7d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialising LangGraph Graph State and Self RAG Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e6cdeb76-b412-411e-9e9a-9ef9c3b2ac83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, TypedDict, List\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain.schema import Document\n",
    "\n",
    "class SelfRAGGraphState(TypedDict):\n",
    "    llm: BaseLanguageModel\n",
    "    deepeval_model: Any\n",
    "    search_query: str\n",
    "    user_query: str\n",
    "    llm_response: str\n",
    "    retrieved_documents: List[Document]\n",
    "    retrieved_documents_within_llm_context_limit: List[Document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d0c495-4758-4580-957b-f01243a433d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Setting Pydantic Data Models for LLM Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d66520cc-8d49-4fb4-b435-d38e13eb5a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SingleDocumentRelevancyModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary score for assessing relevance of retrieved documents.\n",
    "    \"\"\"\n",
    "\n",
    "    binary_score: Literal[\"YES\", \"NO\"] = Field(\n",
    "        default=\"NO\",\n",
    "        description=\"Indicates whether the retrieved document is relevant to the question. Possible values are 'YES' or 'NO'.\",\n",
    "        example=\"YES\",\n",
    "    )\n",
    "\n",
    "\n",
    "class RouteQueryModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Determines the routing of a user quer.\n",
    "\n",
    "    - \"generate_response\": Routes the query to an LLM for response generation.\n",
    "    - \"retrieve_documents\": Routes the query to a vector store for document retrieval. In this case, it is HANA Vector Database.\n",
    "    \"\"\"\n",
    "\n",
    "    action: Literal[\"generate_response\", \"retrieve_documents\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies whether to generate a response using an LLM ('generate_response') or retrieve documents from a vector store ('retrieve_documents').\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c6d393-bf9d-42fe-9a77-d937b2189d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise prompt templates to be used for LLM Generation**\n",
    "\n",
    "**It includes chat, relevancy check for retrieved documents and decision to execute retrieval prompt templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2846f452-f72b-4fc0-9fa3-ef6768ba8fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prompt engineering\n",
    "\n",
    "CONVERSATION_WITH_CONTEXT_TEMPLATE = \"\"\"You are a friendly expert to answer questions based on user provided context.\n",
    "Instructions:\n",
    "1. Use simple and straightforward language.\n",
    "2. Respond in Markdown formatting to enhance readability. Apply headings, lists, and appropriate line spacing where necessary.\n",
    "3. Include citations to the sources in the markdown for the relevant sources used from the context, use what was given in []() not the source number.\n",
    "4. ONLY specify links that exist in the provided context. Do not make up links.\n",
    "5. If you don't know the answer, do not make up a solution, say you don't know.\n",
    "----------\n",
    "Context from different sources that could be used to help answer the question\n",
    "(You DO NOT need to use all sources provided to answer the question. Some sources could be irrelevant):\n",
    "{context}\n",
    "----------\n",
    "Question:\n",
    "{question}\n",
    "----------\n",
    "Response (in correct formatting):\n",
    "\"\"\"\n",
    "\n",
    "SHOULD_EXECUTE_RETRIEVAL_TEMPLATE = \"\"\"\n",
    "Your task is to determine whether you can answer a given query using only your internal knowledge or if you need to retrieve external information. Follow these steps:\n",
    "\n",
    "1. Read and fully understand the user's query.\n",
    "\n",
    "**Note:** The external vector database contains: 1. The contents of the *Self-RAG research paper*, which focuses on retrieval-augmented generation (RAG) techniques, methodologies, and findings and 2. The contents of the *Impact of Generative AI on Critical Thinking*, which focuses on the impact of generative AI on critical thinking and reasoning.\n",
    "\n",
    "2. Perform a chain-of-thought analysis:\n",
    "   - Consider whether the query involves specialized information that may not be covered in your internal knowledge.\n",
    "   - Determine if the query can be answered with well-established, static knowledge you already have.\n",
    "\n",
    "3. Based on your analysis, output a single response:\n",
    "   - Output `retrieve_documents` if the query requires specific details from the *Self-RAG research paper* that are not covered by your internal knowledge.\n",
    "   - Output `generate_response` if your internal knowledge is sufficient to answer the query accurately.\n",
    "\n",
    "**Few-Shot Examples:**\n",
    "\n",
    "Query: 'What is the current weather in New York City?'  \n",
    "Chain-of-thought: This query requires up-to-date, real-time data, which is not covered by internal knowledge or the *Self-RAG research paper*.  \n",
    "Output: retrieve_documents  \n",
    "\n",
    "Query: 'Explain the theory of relativity.'  \n",
    "Chain-of-thought: The query involves a well-established scientific theory that is within internal knowledge.  \n",
    "Output: generate_response  \n",
    "\n",
    "Query: 'Summarize the key findings of the Self-RAG research paper.'  \n",
    "Chain-of-thought: The query specifically requests information from the *Self-RAG research paper*, which is stored in the vector database.  \n",
    "Output: retrieve_documents  \n",
    "\n",
    "After performing your chain-of-thought analysis internally, simply output either `retrieve_documents` or `generate_response` as your final answer.\n",
    "\n",
    "**User Query:**  \n",
    "{question}  \n",
    "\n",
    "**Your Answer:**\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "INDIVIDUAL_DOCUMENT_RELEVANCY_CHECK_TEMPLATE = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "Give a binary score 'YES' or 'NO' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "Retrieved document:\n",
    "{document}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "REWRITE_QUESTION_TEMPLATE = \"\"\"You are a question re-writer designed to improve the clarity, precision, and overall quality of input questions. Your task is to enhance the original question by considering its underlying meaning and intent, ensuring that the revised question is more effective and specific.\n",
    "\n",
    "Consider the following examples to guide your process:\n",
    "\n",
    "Example 1:\n",
    "Original Question: \"What is the impact of climate change?\"\n",
    "Rewritten Question: \"How does climate change affect global ecosystems and human societies?\"\n",
    "\n",
    "Example 2:\n",
    "Original Question: \"Can AI solve problems?\"\n",
    "Rewritten Question: \"What types of problems can artificial intelligence effectively solve in various industries?\"\n",
    "\n",
    "Example 3:\n",
    "Original Question: \"How do plants grow?\"\n",
    "Rewritten Question: \"What are the key factors that influence plant growth and development?\"\n",
    "\n",
    "Now, using the same reasoning, please improve the following question:\n",
    "\n",
    "Original Question:\n",
    "{question}\n",
    "\n",
    "Rewritten Question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afed2a8-50df-40cc-9933-9a62bb20fe3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Initialise LLMChains or Runnables from LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "277def3f-ffc6-4e4f-9191-d4c522e61755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def init_document_relevancy_evaluator_chain(llm: BaseLanguageModel):\n",
    "  structured_llm_grader = llm.with_structured_output(SingleDocumentRelevancyModel)\n",
    "  grade_prompt = PromptTemplate.from_template(INDIVIDUAL_DOCUMENT_RELEVANCY_CHECK_TEMPLATE)\n",
    "  return grade_prompt | structured_llm_grader\n",
    "\n",
    "def init_question_rewriter_chain(llm: BaseLanguageModel):\n",
    "  rewrite_prompt = PromptTemplate.from_template(REWRITE_QUESTION_TEMPLATE)\n",
    "  return rewrite_prompt | llm | StrOutputParser()\n",
    "  \n",
    "def init_should_execute_retrieval_chain(llm: BaseLanguageModel):\n",
    "  structured_llm = llm.with_structured_output(RouteQueryModel)\n",
    "  should_execute_retrieval_prompt = PromptTemplate.from_template(SHOULD_EXECUTE_RETRIEVAL_TEMPLATE)\n",
    "  return should_execute_retrieval_prompt | structured_llm\n",
    "\n",
    "def init_answer_generation_chain(llm: BaseLanguageModel):\n",
    "  return PromptTemplate.from_template(CONVERSATION_WITH_CONTEXT_TEMPLATE) | llm | StrOutputParser()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f89fb80-c39f-4cc1-8cf6-65a0bd3802a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Initialise retrieval pipeline and document relevancy check utility methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c92b67a9-596e-48d0-923f-50b1eddaabc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "\n",
    "def should_execute_retrieval(state: SelfRAGGraphState) -> str:\n",
    "    \"\"\"\n",
    "    Decides whether to execute document retrieval based on the user's query.\n",
    "    This function checks the user's query to determine if it is a valid question\n",
    "    that requires document retrieval.\n",
    "    Args:\n",
    "        state (GraphState): The current graph state containing the user's query.\n",
    "    Returns:\n",
    "        str: \"YES\" if retrieval is required, otherwise \"NO\".\n",
    "    \"\"\"\n",
    "\n",
    "    user_query = state[\"user_query\"]\n",
    "    llm = state[\"llm\"]\n",
    "\n",
    "    should_execute_retrieval_chain = init_should_execute_retrieval_chain(llm)\n",
    "\n",
    "    result = should_execute_retrieval_chain.invoke({\"question\": user_query}, {\"recursion_limit\": 3})\n",
    "\n",
    "    return result.action\n",
    "  \n",
    "def retrieve_documents(state: SelfRAGGraphState) -> Dict[str, Any]:\n",
    "    current_retrieved_documents = state.get(\"retrieved_documents\", [])\n",
    "    current_documents_within_llm_context_limit = state.get(\"retrieved_documents_within_llm_context_limit\", [])\n",
    "    user_query = state[\"user_query\"]\n",
    "\n",
    "    documents = vector_store.similarity_search(user_query, k=10)\n",
    "\n",
    "    all_documents_within_llm_context_limit = current_documents_within_llm_context_limit + documents\n",
    "    all_documents = current_retrieved_documents + documents\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents_within_llm_context_limit\": all_documents_within_llm_context_limit,\n",
    "        \"retrieved_documents\": all_documents\n",
    "    }\n",
    "\n",
    "def evaluate_and_filter_relevant_documents(state: SelfRAGGraphState) -> Dict[str, Any]:\n",
    "  \"\"\"\n",
    "  Evaluates the relevancy of the retrieved documents and filters them based on the evaluation.\n",
    "  Args:\n",
    "      state (SelfRAGGraphState): The current graph state containing:\n",
    "          - user_query (str): The user's most recent query.\n",
    "          - retrieved_documents_within_llm_context_limit (List[Document]): The retrieved documents within the LLM context limit.\n",
    "          - llm (BaseLanguageModel): The LLM model instance for grading the relevance.\n",
    "  Returns:\n",
    "      Dict[str, Any]: Updated Graph state with the relevant documents and a flag indicating whether web search is required.\n",
    "  \"\"\"\n",
    "\n",
    "  user_query = state[\"user_query\"]\n",
    "  retrieved_documents_within_llm_context_limit = state.get(\"retrieved_documents_within_llm_context_limit\", [])\n",
    "  llm = state[\"llm\"]\n",
    "\n",
    "  relevant_documents = _filter_relevant_documents(\n",
    "    user_query, retrieved_documents_within_llm_context_limit, llm\n",
    "  )\n",
    "\n",
    "  return {\n",
    "    \"retrieved_documents_within_llm_context_limit\": relevant_documents,\n",
    "  }\n",
    "\n",
    "def _filter_relevant_documents(\n",
    "  latest_user_query: str,\n",
    "  retrieved_documents_within_llm_context_limit: List[Document],\n",
    "  llm: BaseLanguageModel,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filters the retrieved documents to only include those that are relevant\n",
    "    to the user's query based on a relevance grading model.\n",
    "    Args:\n",
    "        latest_user_query (str): The user's most recent query.\n",
    "        retrieved_documents_within_llm_context_limit (List[Document]): The retrieved documents within the LLM context limit.\n",
    "        llm (BaseLanguageModel): The LLM instance used for evaluating document relevance.\n",
    "    Returns:\n",
    "        List[Document]: The filtered relevant documents.\n",
    "    \"\"\"\n",
    "    evaluator_chain = init_document_relevancy_evaluator_chain(llm)\n",
    "    filtered_relevant_documents = []\n",
    "\n",
    "    for document in retrieved_documents_within_llm_context_limit:\n",
    "        score = evaluator_chain.invoke(\n",
    "            {\"question\": latest_user_query, \"document\": document.page_content}, {\"recursion_limit\": 3}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade == \"YES\":\n",
    "            filtered_relevant_documents.append(document)\n",
    "\n",
    "    return filtered_relevant_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c053fd75-ce2f-4f94-9e80-9b6fc753f73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Initialise query transformation invocation chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0d271c3d-95dc-4757-a383-2576d74f248c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_query(state: SelfRAGGraphState) -> Dict[str, Any]:\n",
    "  \"\"\"\n",
    "  Transform the query to produce a better question.\n",
    "  Args:\n",
    "      state (SelfRAGGraphState): The current graph state\n",
    "  Returns:\n",
    "      state (Dict[str, Any]): Updated search_query key with the transformed query\n",
    "  \"\"\"\n",
    "\n",
    "  user_query = state[\"user_query\"]\n",
    "  llm = state[\"llm\"]\n",
    "\n",
    "  logging.info(f\"Rewriting Question: {user_query}\")\n",
    "\n",
    "  transformed_query = invoke_query_transformation(llm, user_query)\n",
    "\n",
    "  logging.info(f\"Re-written Question: {transformed_query}\")\n",
    "\n",
    "  return {\"search_query\": transformed_query}\n",
    "\n",
    "def invoke_query_transformation(llm: BaseLanguageModel, question: str) -> str:\n",
    "  \"\"\"\n",
    "  Use LLM to rewrite the user's query for better retrieval results.\n",
    "  Args:\n",
    "      llm (BaseLanguageModel): The LLM model instance for transforming the query.\n",
    "      question (str): The user's query to be transformed.\n",
    "  Returns:\n",
    "      str: The re-written question generated by the LLM.\n",
    "  \"\"\"\n",
    "  question_rewriter_chain = init_question_rewriter_chain(llm)\n",
    "  transformed_query = question_rewriter_chain.invoke({\"question\": question}, {\"recursion_limit\": 3})\n",
    "\n",
    "  return transformed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d794605e-66af-4ac4-88dd-d11cf578ecc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise routing method to rewrite user query or generate response with LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "099e0bd6-b156-4b53-b52b-3cd9077a3572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def should_generate_answer_based_on_current_retrieved_documents(state: SelfRAGGraphState) -> str:\n",
    "  \"\"\"\n",
    "  Decides whether to generate an answer or regenerate a question based on the\n",
    "  presence of relevant documents in the graph state.\n",
    "  This function checks whether any relevant documents have been retrieved.\n",
    "  If no relevant documents are found, it triggers the regeneration of a query.\n",
    "  Otherwise, it proceeds with generating an answer based on the retrieved documents.\n",
    "  Args:\n",
    "      state (SelfRAGGraphState): The current graph state, which contains retrieved documents.\n",
    "  Returns:\n",
    "      str: A binary decision to either 'generate' (to generate an answer) or\n",
    "            'transform_query' (to regenerate the query).\n",
    "  \"\"\"\n",
    "  # Retrieve the relevant documents from the graph state.\n",
    "  retrieved_documents_within_llm_context_limit = state.get(\"retrieved_documents_within_llm_context_limit\", [])\n",
    "\n",
    "  # Check if any relevant documents were found.\n",
    "  if len(retrieved_documents_within_llm_context_limit) == 0:\n",
    "      # No relevant documents found, so transform the query.\n",
    "      return \"transform_query\"\n",
    "\n",
    "  return \"generate_response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf124fe6-aa20-4ef5-90e7-519fa1d83beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialize the LLM as a Judge capability, utilizing DeepEval's Hallucination and Answer Relevancy Metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca10aa19-a639-4a9e-8d41-f2da395e42f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "def evaluate_llm_response_against_retrieved_documents_and_question(state: SelfRAGGraphState) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates whether the LLM-generated response is grounded in the retrieved documents\n",
    "    and accurately answers the user's query.\n",
    "\n",
    "    Args:\n",
    "        state (SelfRAGGraphState): The current graph state containing:\n",
    "            - latest_user_query (str): The user's most recent query.\n",
    "            - retrieved_documents_within_llm_context_limit (List[Document]): The retrieved documents within the LLM context limit.\n",
    "            - llm_response (str): The response generated by the LLM.\n",
    "            - llm (BaseLanguageModel): The LLM instance used for grading.\n",
    "\n",
    "    Returns:\n",
    "        str: One of the following labels based on evaluation results:\n",
    "            - \"USEFUL\": Response is factually supported and answers the user's query.\n",
    "            - \"NOT USEFUL\": Response is factually supported but does not answer the user's query.\n",
    "            - \"NOT SUPPORTED\": Response is not factually supported by the documents.\n",
    "    \"\"\"\n",
    "\n",
    "    user_query = state[\"user_query\"]\n",
    "    retrieved_documents_within_llm_context_limit = state.get(\"retrieved_documents_within_llm_context_limit\", [])\n",
    "    llm_response = state[\"llm_response\"]\n",
    "    model = state[\"deepeval_model\"]\n",
    "\n",
    "    is_grounded = _check_groundedness(\n",
    "      model, user_query, retrieved_documents_within_llm_context_limit, llm_response\n",
    "    )\n",
    "\n",
    "    if is_grounded == \"NO\":\n",
    "        return \"NOT SUPPORTED\"\n",
    "\n",
    "    is_answering_user_query = _check_answer_quality(model, user_query, llm_response)\n",
    "\n",
    "    return \"USEFUL\" if is_answering_user_query == \"YES\" else \"NOT USEFUL\"\n",
    "\n",
    "\n",
    "def _check_groundedness(\n",
    "    model,\n",
    "    user_query: str,\n",
    "    retrieved_documents_within_llm_context_limit: List[Document],\n",
    "    llm_response: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Performs a hallucination check on the LLM response to determine if it is factually supported\n",
    "    by the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        deepeval_evaluator (DeepEvalEvaluator): The evaluator to use for the evaluation.\n",
    "        latest_user_query (str): The user's most recent query.\n",
    "        retrieved_documents_within_llm_context_limit (List[Document]): The retrieved documents for context.\n",
    "        llm_response (str): The response generated by the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: \"YES\" if hallucination is not detected, otherwise \"NO\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Consider the LLM's response to be grounded if there are no retrieved documents to be used as context.\n",
    "    # The response could be generated from its knowledge base.\n",
    "    if len(retrieved_documents_within_llm_context_limit) == 0:\n",
    "        return \"YES\"\n",
    "    \n",
    "    context = [document.page_content for document in retrieved_documents_within_llm_context_limit]\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=user_query,\n",
    "        actual_output=llm_response,\n",
    "        context=context\n",
    "    )\n",
    "    metric = HallucinationMetric(\n",
    "        threshold=0.3,\n",
    "        model=model,\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    result = evaluate(test_cases=[test_case], metrics=[metric])\n",
    "    result = result.test_results[0]\n",
    "    result = result.metrics_data[0]\n",
    "\n",
    "    is_grounded = \"YES\" if result.score < 0.3 else \"NO\"\n",
    "\n",
    "    return is_grounded\n",
    "\n",
    "def _check_answer_quality(model, user_query: str, llm_response: str) -> str:\n",
    "  \"\"\"\n",
    "  Checks if the LLM response answers the user's query.\n",
    "\n",
    "  Args:\n",
    "      user_query (str): The user's most recent query.\n",
    "      llm_response (str): The response generated by the LLM.\n",
    "\n",
    "  Returns:\n",
    "      str: \"YES\" if the response answers the query, otherwise \"NO\".\n",
    "  \"\"\"\n",
    "\n",
    "  test_case = LLMTestCase(\n",
    "    input=user_query,\n",
    "    actual_output=llm_response\n",
    "  )\n",
    "  metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=model,\n",
    "    include_reason=True\n",
    "  )\n",
    "\n",
    "  result = evaluate(test_cases=[test_case], metrics=[metric])\n",
    "  result = result.test_results[0]\n",
    "  result = result.metrics_data[0]\n",
    "\n",
    "  is_answering_user_query = \"YES\" if result.score > 0.7 else \"NO\"\n",
    "\n",
    "  return is_answering_user_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417ebd06-72de-4a0b-ab56-1524103434ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise LLM Generation utility methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "70eaddd5-1f05-42d8-a9ab-436b8a79c7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(state: SelfRAGGraphState) -> Dict[str, Any]:\n",
    "  \"\"\"\n",
    "  Generate answer\n",
    "  Args:\n",
    "      state (GraphState): The current graph state\n",
    "  Returns:\n",
    "      state (Dict): New llm_response key added to graph state with the LLM generated response\n",
    "  \"\"\"\n",
    "  user_query = state[\"user_query\"]\n",
    "  retrieved_documents_within_llm_context_limit = state.get(\"retrieved_documents_within_llm_context_limit\", [])\n",
    "  llm = state[\"llm\"]\n",
    "\n",
    "  llm_response = invoke_llm_response_generation(\n",
    "      llm, user_query, retrieved_documents_within_llm_context_limit\n",
    "  )\n",
    "\n",
    "  return {\"llm_response\": llm_response}\n",
    "  \n",
    "\n",
    "def invoke_llm_response_generation(\n",
    "  llm: BaseLanguageModel,\n",
    "  question: str,\n",
    "  context: List[Document],\n",
    "):\n",
    "  \"\"\"\n",
    "  Generates a response using the provided LLM model.\n",
    "  Args:\n",
    "      llm (BaseLanguageModel): The LLM model instance for generating the response.\n",
    "      question (str): The user's query.\n",
    "      context (List[Document]): The retrieved documents for context.\n",
    "  Returns:\n",
    "      str: The generated response from the LLM model.\n",
    "  \"\"\"\n",
    "  answer_generation_chain = init_answer_generation_chain(llm)\n",
    "  llm_response = answer_generation_chain.invoke(\n",
    "      {\n",
    "          \"context\": context,\n",
    "          \"question\": question,\n",
    "      },\n",
    "      {\"recursion_limit\": 3},\n",
    "  )\n",
    "\n",
    "  return llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb48fb5-9d51-431c-bebf-679de726252c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise the LangGraph workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7c7d5f5c-c298-4525-b1c4-79b54ca69dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "workflow = StateGraph(SelfRAGGraphState)\n",
    "\n",
    "workflow.add_node(\"should_execute_retrieval\", should_execute_retrieval)\n",
    "workflow.add_node(\"retrieve_documents\", retrieve_documents)  # Retrieves relevant documents.\n",
    "workflow.add_node(\n",
    "  \"evaluate_and_filter_relevant_documents\", evaluate_and_filter_relevant_documents\n",
    ")  # Evaluates and extracts the relevant documents.\n",
    "workflow.add_node(\"transform_query\", transform_query)  # Refines the query if needed.\n",
    "workflow.add_node(\"generate_response\", generate_response)  # Generates a response based on documents.\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "  START,\n",
    "  should_execute_retrieval,\n",
    "  {\n",
    "    \"retrieve_documents\": \"retrieve_documents\",  # If retrieval is needed, proceed to document retrieval.\n",
    "    \"generate_response\": \"generate_response\",\n",
    "  },\n",
    ")\n",
    "workflow.add_edge(\n",
    "  \"retrieve_documents\", \"evaluate_and_filter_relevant_documents\"\n",
    ")  # Assess the quality of retrieved documents.\n",
    "\n",
    "# Based on document quality, decide whether to generate a response or refine the query.\n",
    "workflow.add_conditional_edges(\n",
    "  \"evaluate_and_filter_relevant_documents\",\n",
    "  should_generate_answer_based_on_current_retrieved_documents,\n",
    "  {\n",
    "      \"transform_query\": \"transform_query\",  # If documents are insufficient, refine the query.\n",
    "      \"generate_response\": \"generate_response\",  # If documents are good enough, proceed to response generation.\n",
    "  },\n",
    ")\n",
    "\n",
    "# If the query is refined, repeat the retrieval process\n",
    "# and append the newly retrieved documents to the existing ones.\n",
    "workflow.add_edge(\"transform_query\", \"retrieve_documents\")\n",
    "\n",
    "# After generating a response, evaluate its usefulness.\n",
    "workflow.add_conditional_edges(\n",
    "  \"generate_response\",\n",
    "  evaluate_llm_response_against_retrieved_documents_and_question,\n",
    "  {\n",
    "      \"NOT SUPPORTED\": \"generate_response\",  # If response is unsupported (which means the LLM response is not grounded in the retrieved documents), attempt generation again.\n",
    "      \"USEFUL\": END,  # If response is useful, end the process.\n",
    "      \"NOT USEFUL\": \"transform_query\",  # If response is not useful, refine query and retry retrieval.\n",
    "  },\n",
    ")\n",
    "\n",
    "self_rag_agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3829f0-d1f8-49a7-9eee-8b0554d90bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b607b36-65b1-4041-bed0-76815652597e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\AZsnake\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-package\n",
       "s\\Python313\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\AZsnake\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-package\n",
       "s\\Python313\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  -  Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.00 because there are no contradictions present, and all factual alignments accurately reflect the context, demonstrating complete consistency and reliability in the output., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Self RAG\n",
      "  - actual output: # What is Self RAG?\n",
      "\n",
      "**Self RAG** (Self-Reflective Retrieval-Augmented Generation) is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by integrating retrieval and self-reflection mechanisms. Heres a breakdown of its key components:\n",
      "\n",
      "## Key Features\n",
      "\n",
      "- **Adaptive Retrieval**: \n",
      "  - Self RAG allows the model to determine when to retrieve relevant information based on the task at hand. This means it doesn't always pull in external data, only when it's deemed necessary.\n",
      "\n",
      "- **Self-Reflection**: \n",
      "  - The model generates special tokens, known as **reflection tokens**, which help it evaluate its own outputs and the relevance of retrieved information. These tokens can indicate whether to retrieve information and assess the quality of its responses.\n",
      "\n",
      "- **Improved Factuality**:\n",
      "  - By combining retrieval with self-reflection, Self RAG enhances the model's ability to produce factually accurate content, reducing errors common in standard LLMs.\n",
      "\n",
      "- **Training Efficiency**:\n",
      "  - The framework trains the model to generate text informed by relevant passages while also learning to critique its own outputs. This process reduces the need for extensive retraining or human feedback.\n",
      "\n",
      "## How It Works\n",
      "\n",
      "1. **Input Handling**: \n",
      "   - The model receives an input and generates an output based on its internal training.\n",
      "\n",
      "2. **Reflection Tokens**: \n",
      "   - It generates reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of its generated content.\n",
      "\n",
      "3. **Dynamic Decision Making**:\n",
      "   - Depending on the task, the model can adjust its behavior to prioritize factual accuracy or creativity.\n",
      "\n",
      "4. **Performance**:\n",
      "   - Experiments show that Self RAG outperforms existing models in various tasks, including open-domain question answering, reasoning, and long-form content generation, by improving accuracy and citation correctness.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Self RAG represents a significant advancement in the way LLMs can be trained and utilized, offering a method to enhance their performance while maintaining versatility and creativity. It effectively combines retrieval-based knowledge with self-assessment to produce higher quality outputs. \n",
      "\n",
      "For more details on Self RAG, you can check the paper [here](https://selfrag.github.io/).\n",
      "  - expected output: None\n",
      "  - context: ['Preprint.\\nSELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND\\nCRITIQUE THROUGH SELF -REFLECTION\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi\\nUniversity of Washington Allen Institute for AI IBM Research AI\\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration (SELF -RAG) that enhances an LMs quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF -\\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an\\nLLMs generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF -RAG first determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\\nretrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/.\\n1\\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023', 'Preprint.\\nseparate reward models during training, we compute critique offline and directly insert them into the\\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\\nlearns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\\n3.3 S ELF -RAG INFERENCE\\nGenerating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\\ncontrol to meet these distinct objectives during the inference process.\\nAdaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by\\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\\nCritique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp\\nt as sG\\nt , and we compute a segment score as follows:\\nf(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)\\nS( Critique ) =\\nX\\nGG\\nwGsG\\nt for G = { ISREL , ISSUP , ISUSE }, (4)\\nwhere sG\\nt = pt(r)PNG\\ni=1 pt(ri)\\nstands for the generation probability of the most desirable reflection token\\nr (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent\\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\\ny is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while\\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\\nfilter out a segment continuation when the model generates an undesirable Critique token (e.g.,\\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\\nbehaviors. SELF -RAG tailors an LM with no additional training.\\n4 E XPERIMENTS\\n4.1 T ASKS AND DATASETS\\nWe conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\\nour experiments settings, including test-time instructions, are available in the Appendix Section B.1.\\nClosed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;\\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\\n6', 'Preprint.\\nType Input Output Definitions\\nRetrieve x / x, y {yes, no, continue} Decides when to retrieve with R\\nISREL x, d {relevant, irrelevant} d provides useful information to solve x.\\nISSUP x, d, y {fully supported, partially\\nsupported, no support}\\nAll of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.\\nTable 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF -RAG Inference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }\\n1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\\n2: M predicts Retrieve given (x, y<t)\\n3: if Retrieve == Yes then\\n4: Retrieve relevant text passages D using R given (x, yt1)  Retrieve\\n5: M predicts ISREL given x, dand yt given x, d, y<t for each d  D  Generate\\n6: M predicts ISSUP and ISUSE given x, yt, dfor each d  D  Critique\\n7: Rank yt based on ISREL , ISSUP , ISUSE  Detailed in Section 3.3\\n8: else if Retrieve == No then\\n9: Mgen predicts yt given x  Generate\\n10: Mgen predicts ISUSE given x, yt  Critique\\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassages relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.4 To generate each segment, SELF -RAG processes multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\\nand d3 output is only partially supported while d1 are fully supported.\\nTraining overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\\nM to generate reflection tokens by itself without relying on the critic at inference time.\\n3.2 S ELF -RAG TRAINING\\nHere, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)\\nand the generator M (Section 3.2.2).\\n3.2.1 T RAINING THE CRITIC MODEL\\nData collection for critic model. Manual annotation of reflection tokens for each segment is\\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\\n4We follow Liu et al. (2023a) in using a perceived utility value that is independent of retrieved passages.\\n4', 'Preprint.\\n0 50 100 150\\nNum of training (k)\\n35\\n40\\n45\\n50\\n55Perfomance\\n(a) PopQA\\n0 100\\nNum of training (k)\\n71\\n72\\n73\\n (b) PubHealth\\n0 100\\nNum of training (k)\\n40\\n60\\n (c) ASQA (prec)\\nPop Bio.\\nS & P 92.5 70.0\\nISREL 95.0 90.0\\nISSUP 90.0 85.0\\n(d) Human evaluation on PopQA\\nand Bio generation.\\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\\nHuman analysis on SELF -RAG outputs as well as reflection tokens.\\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models behaviors at\\ntest time by adjusting such parameters without requiring additional training.\\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\\noverall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\\nof threshold  (larger  results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\\nthe models retrieval frequencies dramatically change on both datasets. as  varies. On one hand,\\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\\nEffects of training data size. We conduct an analysis of how the data scale affects the models\\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\\n150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare\\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -\\nRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\\nnot observed such significant improvements on Llama2-FT7B when increasing the training data from\\n50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may\\nlead to further improvements, although in this work we limit our training data size to 150k.\\nHuman evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the\\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\\nresults. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\\nverify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG\\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\\nreflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported\\noutput is supported by the cited evidence). Human annotators find SELF -RAG answers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\\nconsistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\\nexamples and explanations on assessments.\\n6 C ONCLUSION\\nThis work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n10', 'Preprint.\\nPQA Med AS\\n(acc) (acc) (em)\\nSELF -RAG (50k) 45.5 73.5 32.1\\nTraining\\nNo Retriever R 43.6 67.8 31.0\\nNo Critic C 42.6 72.0 18.1\\nTest\\nNo retrieval 24.7 73.0 \\nHard constraints 28.3 72.6 \\nRetrieve top1 41.8 73.1 28.6\\nRemove ISSUP 44.1 73.2 30.6\\n(a) Ablation\\n1 2\\n70.0\\n70.5Precision\\n1 2\\nWeight for IsSupport\\n90\\n95Mauve\\n (b) Customization\\n0.0 0.2 0.4 0.6\\n0.98\\n0.99\\n0.99\\n1.00Accuracy\\nPubHealth\\n0.0 0.2 0.4 0.6\\nRetrieval Threshold\\n0.6\\n0.8\\n1.0Accuracy\\nPopQA\\n0.0\\n0.5\\n1.0\\nFrequency\\n0.25\\n0.50\\n0.75\\n1.00\\nFrequency\\n (c) Retrieval\\nFigure 3: Analysis on SELF -RAG: (a) Ablation studies for key components of SELF -RAG training\\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\\nprecisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\\ninstruction-output pairs as SELF -RAG without retrieval or self-reflection and is retrieval-augmented\\nat test time only, lags behind SELF -RAG. This result indicates S ELF -RAG gains are not solely from\\ntraining data and demonstrate the effectiveness of SELF -RAG framework.\\n5.2 A NALYSIS\\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\\ntop one document only, similar to standard RAG approaches;Remove ISSUP indicates the model\\nperformance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this\\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\\nand the bottom part is for inference ablations. We see that all components play important roles. We\\nalso observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\\nSELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAGs\\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\\nEffects of inference-time customization. One key benefit of our proposed framework is that it\\nenables us to control how much each critique type affects the final generation sampling. We analyze\\nthe effects of different parameter weights on the top of our 7B model during inference time on\\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\\nthe weighting term for ISSUP , which criticizes how supported the output is by the text passage. As\\nthe figure shows, increasing the weight leads to positive effects on the models citation precision\\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\\n9', 'Preprint.\\nAPPENDIX\\nA S ELF -RAG Details 17\\nA.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB Experimental Details 19\\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC Results 20\\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nD Full List of Instructions and Demonstrations for GPT-4 21\\n16', 'Preprint.\\nA S ELF -RAG DETAILS\\nA.1 R EFLECTION TOKENS .\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence, which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF -RAG\\ngenerates multiple segments based on the passage.\\n Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\\n Supported ( ISSUP ): Attribution is the concept of whether the output is fully supported by\\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\\nsupported, Partially supported, and No support / Contradictory, follow-\\ning Yue et al. (2023); Nakano et al. (2021).\\n Useful ( ISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\\nas whether the response is a helpful and informative answer to the query, independently from\\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\\noutputs with ##. We use the temperature 1 and set the maximum output token counts to be 200. We\\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\\nfor ISSUP , 19,317 for relevance, 3,831 for utility.\\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\\nthe same instruction, demonstrations, and test instances. We found our assessments show high\\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\\ndisagreement between 1 and 2 or 4 and 5.\\nA.2 S ELF -RAG TRAINING\\nOverview of training. Algorithm 2 provides a high-level overview of our training.\\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\\nPerformance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\\n17', 'Preprint.\\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\\nentities. Yet, the improved task performance of such approaches often comes at the expense of\\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\\ngeneration guided by reflections tokens to further improve generation quality and attributions.\\nConcurrent RAG work. A few concurrent works2 on RAG propose new training or prompting\\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\\ninstruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best\\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\\na summarization model to filter out or compress retrieved passages before using them to prompt the\\nLM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones\\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\\nand to generate with tree search, guided by LM-generated value scores. While their value function\\nsimply indicates an overall score of each generation, SELF -RAG trains to an arbitrary LM to learn to\\ngenerate fine-grained self-reflection and customizable inference.\\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\\non retrieval and generation, we train our target LM on task examples augmented with reflection\\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\\nreflection tokens in SELF -RAG enable controllable generation at inference, while RLHF focuses on\\nhuman preference alignment during training. Other works use general control tokens to guide LM\\ngeneration (Lu et al., 2022; Korbak et al., 2023), whileSELF -RAG uses reflection tokens to decide the\\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\\n3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\\nSELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLMs original creativity and versatility. Our end-to-end training\\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the outputs relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3', 'Preprint.\\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\\nthey outperforms all non-proprietary models.  indicates concurrent or recent results reported by\\nconcurrent work.  indicates numbers that are not reported by the original papers or are not applicable.\\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\\nShort-form Closed-set Long-form generations (with citations)\\nPopQA TQA Pub ARC Bio ASQA\\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\\nLMs with proprietary data\\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6  \\nRet-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\\nChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8  \\nRet-ChatGPT 50.8 65.7 54.7 75.3  40.7 39.9 79.7 65.1 76.6\\nPerplexity.ai     71.2     \\nBaselines without retrieval\\nLlama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0  \\nAlpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7  \\nLlama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0  \\nAlpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6  \\nCoVE65B *     71.2     \\nBaselines with retrieval\\nToolformer*6B  48.8        \\nLlama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\\nAlpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\\nLlama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\\nSAIL*7B   69.2 48.4      \\nLlama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\\nOur SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\\nOur SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\\n5 R ESULTS AND ANALYSIS\\n5.1 M AIN RESULTS\\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\\nretrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\\niteratively prompts Llama265B to refine output.\\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF -RAG also\\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\\nretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\\nin this particular task, surpassing smaller LMs. Our SELF -RAG bridges this performance gap, even\\noutperforming ChatGPT in citation precision, which measures whether the model-generated claim is\\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF -RAG\\n7B occasionally outperforms our 13B due to the tendency of smaller SELF -RAG to often generate\\n8', 'Preprint.\\nAlgorithm 2 SELF -RAG Training\\n1: Input input-output data D = {X, Y}, generator M, C \\n2: Initialize C with a pre-trained LM\\n3: Sample data {Xsample, Ysample}  {X, Y}  Training Critic LM (Section 3.2.1)\\n4: for (x, y)  (Xsample, Ysample) do  Data collections for C\\n5: Prompt GPT-4 to collect a reflection token r for (x, y)\\n6: Add {(x, y, r)} to Dcritic\\n7: Update C with next token prediction loss  Critic learning; Eq. 1\\n8: Initialize M with a pre-trained LM  Training Generator LM (Section 3.2.2)\\n9: for (x, y)  (X, Y) do  Data collection for M with Dcritic\\n10: Run C to predict r given (x, y)\\n11: Add (x, y, r) to Dgen\\n12: Update M on Dgen with next token prediction loss  Generator LM learning; Eq. 2\\nDataset name category Data source the number of instances\\nGPT-4 Alpaca Instruction-following Open-Instruct 26,168\\nStanford Alpaca Instruction-following Open-Instruct 25,153\\nFLAN-V2 Instruction-following Open-Instruct 17,817\\nShareGPT Instruction-following Open-Instruct 13,406\\nOpen Assistant 1 Instruction-following Open-Instruct 9,464\\nWizard of Wikipedia Knowledge-intensive KILT 17,367\\nNatural Questions Knowledge-intensive KILT 15,535\\nFEVER Knowledge-intensive KILT 9,966\\nOpenBoookQA Knowledge-intensive HF Dataset 4,699\\nArc-Easy Knowledge-intensive HF Dataset 2,147\\nASQA Knowledge-intensive ASQA 3,897\\nTable 3: The generator LM M training data statistics.\\nbase LM Retrieve ISSUP ISREL ISUSE\\nLlama2-7B 93.8 93.5 80.2 73.5\\nFLAN-3B 85.6 73.1 82.0 72.1\\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\\nlower performance on ISUSE , this is because both models often confuse between the two highest\\ncases (5 and 4), where human annotators can also disagree.\\nDetails of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\\nnot. For the instances where the critic predicts Retrieve =No, we only predict the ISUSE given input\\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\\nthen split output sentences using Spacy.7 For each sentence, we run C to predict whether the retrieval\\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. IfC predicts\\nRetrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\\nt-th segment. For each retrieved passage, we predict ISREL and ISSUP . If there is any passage and\\ncontinuation with ISREL =Relevant and ISSUP =Fully Supported / ISSUP =Partially\\n7https://spacy.io/\\n18']\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Hallucination: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\"></span> Evaluation completed ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.</span>5s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00200085</span> USD<span style=\"font-weight: bold\">)</span>\n",
       " Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "    Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       " What to share evals with your team, or a place for your test cases to live?  \n",
       "   Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m\u001b[0m Evaluation completed ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m15.\u001b[0m5s | token cost: \u001b[1;36m0.00200085\u001b[0m USD\u001b[1m)\u001b[0m\n",
       " Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "    Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       " What to share evals with your team, or a place for your test cases to live?  \n",
       "   Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  -  Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the response directly addresses the question about Self RAG without any irrelevant statements., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Self RAG\n",
      "  - actual output: # What is Self RAG?\n",
      "\n",
      "**Self RAG** (Self-Reflective Retrieval-Augmented Generation) is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by integrating retrieval and self-reflection mechanisms. Heres a breakdown of its key components:\n",
      "\n",
      "## Key Features\n",
      "\n",
      "- **Adaptive Retrieval**: \n",
      "  - Self RAG allows the model to determine when to retrieve relevant information based on the task at hand. This means it doesn't always pull in external data, only when it's deemed necessary.\n",
      "\n",
      "- **Self-Reflection**: \n",
      "  - The model generates special tokens, known as **reflection tokens**, which help it evaluate its own outputs and the relevance of retrieved information. These tokens can indicate whether to retrieve information and assess the quality of its responses.\n",
      "\n",
      "- **Improved Factuality**:\n",
      "  - By combining retrieval with self-reflection, Self RAG enhances the model's ability to produce factually accurate content, reducing errors common in standard LLMs.\n",
      "\n",
      "- **Training Efficiency**:\n",
      "  - The framework trains the model to generate text informed by relevant passages while also learning to critique its own outputs. This process reduces the need for extensive retraining or human feedback.\n",
      "\n",
      "## How It Works\n",
      "\n",
      "1. **Input Handling**: \n",
      "   - The model receives an input and generates an output based on its internal training.\n",
      "\n",
      "2. **Reflection Tokens**: \n",
      "   - It generates reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of its generated content.\n",
      "\n",
      "3. **Dynamic Decision Making**:\n",
      "   - Depending on the task, the model can adjust its behavior to prioritize factual accuracy or creativity.\n",
      "\n",
      "4. **Performance**:\n",
      "   - Experiments show that Self RAG outperforms existing models in various tasks, including open-domain question answering, reasoning, and long-form content generation, by improving accuracy and citation correctness.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Self RAG represents a significant advancement in the way LLMs can be trained and utilized, offering a method to enhance their performance while maintaining versatility and creativity. It effectively combines retrieval-based knowledge with self-assessment to produce higher quality outputs. \n",
      "\n",
      "For more details on Self RAG, you can check the paper [here](https://selfrag.github.io/).\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\"></span> Evaluation completed ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.</span>23s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00075435</span> USD<span style=\"font-weight: bold\">)</span>\n",
       " Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "    Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       " What to share evals with your team, or a place for your test cases to live?  \n",
       "   Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m\u001b[0m Evaluation completed ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m15.\u001b[0m23s | token cost: \u001b[1;36m0.00075435\u001b[0m USD\u001b[1m)\u001b[0m\n",
       " Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "    Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       " What to share evals with your team, or a place for your test cases to live?  \n",
       "   Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# What is Self RAG?\n",
       "\n",
       "**Self RAG** (Self-Reflective Retrieval-Augmented Generation) is a framework designed to improve the quality and factual accuracy of large language models (LLMs) by integrating retrieval and self-reflection mechanisms. Heres a breakdown of its key components:\n",
       "\n",
       "## Key Features\n",
       "\n",
       "- **Adaptive Retrieval**: \n",
       "  - Self RAG allows the model to determine when to retrieve relevant information based on the task at hand. This means it doesn't always pull in external data, only when it's deemed necessary.\n",
       "\n",
       "- **Self-Reflection**: \n",
       "  - The model generates special tokens, known as **reflection tokens**, which help it evaluate its own outputs and the relevance of retrieved information. These tokens can indicate whether to retrieve information and assess the quality of its responses.\n",
       "\n",
       "- **Improved Factuality**:\n",
       "  - By combining retrieval with self-reflection, Self RAG enhances the model's ability to produce factually accurate content, reducing errors common in standard LLMs.\n",
       "\n",
       "- **Training Efficiency**:\n",
       "  - The framework trains the model to generate text informed by relevant passages while also learning to critique its own outputs. This process reduces the need for extensive retraining or human feedback.\n",
       "\n",
       "## How It Works\n",
       "\n",
       "1. **Input Handling**: \n",
       "   - The model receives an input and generates an output based on its internal training.\n",
       "\n",
       "2. **Reflection Tokens**: \n",
       "   - It generates reflection tokens to decide if retrieval is necessary and to evaluate the relevance and support of its generated content.\n",
       "\n",
       "3. **Dynamic Decision Making**:\n",
       "   - Depending on the task, the model can adjust its behavior to prioritize factual accuracy or creativity.\n",
       "\n",
       "4. **Performance**:\n",
       "   - Experiments show that Self RAG outperforms existing models in various tasks, including open-domain question answering, reasoning, and long-form content generation, by improving accuracy and citation correctness.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Self RAG represents a significant advancement in the way LLMs can be trained and utilized, offering a method to enhance their performance while maintaining versatility and creativity. It effectively combines retrieval-based knowledge with self-assessment to produce higher quality outputs. \n",
       "\n",
       "For more details on Self RAG, you can check the paper [here](https://selfrag.github.io/)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "llm = init_llm_model(openai_model)\n",
    "deepeval_llm = GPTModel(model=openai_model)\n",
    "\n",
    "for output in self_rag_agent.stream({\n",
    "  \"llm\": llm,\n",
    "  \"deepeval_model\": deepeval_llm,\n",
    "  \"user_query\": \"What is Self RAG\"\n",
    "}):\n",
    "  for key, value in output.items():\n",
    "    continue\n",
    "\n",
    "llm_response = output['generate_response']['llm_response']\n",
    "\n",
    "display(Markdown(llm_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297dc433-01b4-4297-a994-c1be117c4cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recursive Problem Solver Agent with CrewAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b91294-ada0-427f-8c11-e23fd17a6d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Recursive Problem Solver Agent](./Recursive%20Problem%20Solver.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddffeaa9-8b11-4415-a3a8-067ab43c1a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Setting Pydantic Data Models for LLM Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "08ce87c4-2774-4ec9-9197-845ca4b56614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class Subtask(BaseModel):\n",
    "  id: str\n",
    "  description: str\n",
    "  rationale: str\n",
    "  dependencies: Optional[List[str]]\n",
    "\n",
    "class DecomposeResult(BaseModel):\n",
    "  result: List[Subtask] | str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666b80d1-078f-4cfe-8ced-98ebf9f12553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise the Problem Solver Agent class with CrewAI Agents and Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b1e38246-a87b-48d7-b989-515fd1ca9da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from crewai import Agent as CrewAIAgent, Task, Crew, Process\n",
    "from typing import List, Dict\n",
    "\n",
    "class RecursiveProblemSolver:\n",
    "    def __init__(self, model=\"gpt-4\", max_recursion_depth=3):\n",
    "        \"\"\"\n",
    "        Initialize the Recursive Problem Solver.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to use for the agents\n",
    "            max_recursion_depth: Maximum depth of recursion allowed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.max_recursion_depth = max_recursion_depth\n",
    "        self.solution_cache = {}  # Cache to store solved subtasks\n",
    "        \n",
    "    def solve(self, problem_statement: str) -> str:\n",
    "        \"\"\"\n",
    "        Main entry point to solve a complex problem recursively.\n",
    "        \n",
    "        Args:\n",
    "            problem_statement: A description of the complex problem to solve\n",
    "            \n",
    "        Returns:\n",
    "            A comprehensive solution to the problem\n",
    "        \"\"\"\n",
    "        # Start the recursive solving process\n",
    "        return self._recursive_solve(problem_statement, depth=0)\n",
    "    \n",
    "    def _recursive_solve(self, problem: str, depth: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        Recursively solve a problem by breaking it down and synthesizing results.\n",
    "        \n",
    "        Args:\n",
    "            problem: The problem statement or subtask\n",
    "            depth: Current recursion depth\n",
    "            \n",
    "        Returns:\n",
    "            Solution for the given problem/subtask\n",
    "        \"\"\"\n",
    "        # Check if we've hit max recursion depth\n",
    "        if depth >= self.max_recursion_depth:\n",
    "            return self._solve_directly(problem)\n",
    "        \n",
    "        # Check if we've already solved this problem\n",
    "        problem_hash = hash(problem)\n",
    "        if problem_hash in self.solution_cache:\n",
    "            return self.solution_cache[problem_hash]\n",
    "        \n",
    "        # Create the decomposer agent\n",
    "        decomposer = self._create_decomposer_agent()\n",
    "        \n",
    "        # Create the task for breaking down the problem\n",
    "        decompose_task = Task(\n",
    "            description=f\"\"\"\n",
    "            Your primary goal is to determine whether the problem can be solved directly. \n",
    "\n",
    "            PROBLEM: {problem}\n",
    "\n",
    "            **If the problem is simple enough to be solved immediately, return the string \"SIMPLE_ENOUGH\".**  \n",
    "            Do **not** break it down unless it is absolutely necessary.\n",
    "\n",
    "            Only if the problem is **too complex to solve directly**, break it into subtasks.  \n",
    "            In that case, return a JSON list of subtasks in the following format:\n",
    "\n",
    "            [\n",
    "                {{\n",
    "                    \"id\": \"subtask-1\",\n",
    "                    \"description\": \"Description of subtask 1\",\n",
    "                    \"rationale\": \"Why this subtask is necessary\",\n",
    "                    \"dependencies\": []\n",
    "                }},\n",
    "                {{\n",
    "                    \"id\": \"subtask-2\",\n",
    "                    \"description\": \"Description of subtask 2\",\n",
    "                    \"rationale\": \"Why this subtask is necessary\",\n",
    "                    \"dependencies\": [\"subtask-1\"]\n",
    "                }},\n",
    "                ...\n",
    "            ]\n",
    "\n",
    "            **REMEMBER:**  \n",
    "            - If the problem can be solved directly, return **\"SIMPLE_ENOUGH\"**.  \n",
    "            - Only decompose if **absolutely necessary**.\n",
    "            \"\"\",\n",
    "            expected_output='\"SIMPLE_ENOUGH\" if the problem is solvable directly, otherwise a JSON list of subtasks.',\n",
    "            agent=decomposer\n",
    "        )\n",
    "\n",
    "        # Run the decomposition task\n",
    "        decomposition_crew = Crew(\n",
    "            agents=[decomposer],\n",
    "            tasks=[decompose_task],\n",
    "            process=Process.sequential\n",
    "        )\n",
    "        decomposition_result = decomposition_crew.kickoff()\n",
    "        \n",
    "        # Check if the problem is simple enough to solve directly\n",
    "        if \"SIMPLE_ENOUGH\" in decomposition_result:\n",
    "            solution = self._solve_directly(problem)\n",
    "            self.solution_cache[problem_hash] = solution\n",
    "            return solution\n",
    "        \n",
    "        try:\n",
    "            raw_result = decomposition_result.raw\n",
    "            subtasks = json.loads(raw_result)\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, try to extract just the JSON part\n",
    "            json_match = re.search(r'\\[\\s*\\{.*\\}\\s*\\]', raw_result, re.DOTALL)\n",
    "            if json_match:\n",
    "                subtasks = json.loads(json_match.group(0))\n",
    "            else:\n",
    "                # If we can't parse JSON, solve directly\n",
    "                return self._solve_directly(problem)\n",
    "        \n",
    "        # Dictionary to store solutions for each subtask\n",
    "        subtask_solutions = {}\n",
    "        \n",
    "        # Solve each subtask recursively based on dependencies\n",
    "        for subtask in subtasks:\n",
    "            subtask_id = subtask[\"id\"]\n",
    "            # Check if all dependencies are solved\n",
    "            deps_solved = all(dep in subtask_solutions for dep in subtask.get(\"dependencies\", []))\n",
    "            \n",
    "            if deps_solved:\n",
    "                # Solve the subtask recursively\n",
    "                subtask_solution = self._recursive_solve(subtask[\"description\"], depth + 1)\n",
    "                subtask_solutions[subtask_id] = subtask_solution\n",
    "        \n",
    "        # Create the synthesizer agent\n",
    "        synthesizer = self._create_synthesizer_agent()\n",
    "        \n",
    "        # Create the task for synthesizing the results\n",
    "        synthesis_task = Task(\n",
    "            description=f\"\"\"\n",
    "            Synthesize the following subtask solutions into a comprehensive solution for the original problem:\n",
    "            \n",
    "            ORIGINAL PROBLEM: {problem}\n",
    "            \n",
    "            SUBTASKS AND THEIR SOLUTIONS:\n",
    "            {self._format_subtask_solutions(subtasks, subtask_solutions)}\n",
    "            \n",
    "            Provide a coherent, integrated solution that addresses all aspects of the original problem.\n",
    "            Evaluate whether the solution is complete or if further refinement is needed.\n",
    "            \"\"\",\n",
    "            expected_output=\"A comprehensive solution to the original problem\",\n",
    "            agent=synthesizer\n",
    "        )\n",
    "        \n",
    "        # Run the synthesis task\n",
    "        synthesis_crew = Crew(\n",
    "            agents=[synthesizer],\n",
    "            tasks=[synthesis_task],\n",
    "            process=Process.sequential\n",
    "        )\n",
    "        synthesis_result = synthesis_crew.kickoff()\n",
    "        \n",
    "        # Store the solution in cache\n",
    "        self.solution_cache[problem_hash] = synthesis_result\n",
    "        \n",
    "        return synthesis_result.raw\n",
    "    \n",
    "    def _solve_directly(self, problem: str) -> str:\n",
    "        \"\"\"\n",
    "        Solve a simple problem directly without further decomposition.\n",
    "        \n",
    "        Args:\n",
    "            problem: The problem to solve\n",
    "            \n",
    "        Returns:\n",
    "            Solution for the given problem\n",
    "        \"\"\"\n",
    "        solver = self._create_solver_agent()\n",
    "        \n",
    "        solve_task = Task(\n",
    "            description=f\"\"\"\n",
    "            Solve the following problem directly:\n",
    "            \n",
    "            PROBLEM: {problem}\n",
    "            \n",
    "            Provide a comprehensive solution with detailed steps and reasoning.\n",
    "            \"\"\",\n",
    "            expected_output=\"A comprehensive solution to the problem\",\n",
    "            agent=solver\n",
    "        )\n",
    "        \n",
    "        solver_crew = Crew(\n",
    "            agents=[solver],\n",
    "            tasks=[solve_task],\n",
    "            process=Process.sequential\n",
    "        )\n",
    "\n",
    "        solution = solver_crew.kickoff()\n",
    "        \n",
    "        return solution.raw\n",
    "    \n",
    "    def _format_subtask_solutions(self, subtasks: List[Dict], solutions: Dict) -> str:\n",
    "        \"\"\"Format subtask solutions for the synthesizer.\"\"\"\n",
    "        formatted = \"\"\n",
    "        for subtask in subtasks:\n",
    "            subtask_id = subtask[\"id\"]\n",
    "            formatted += f\"SUBTASK ID: {subtask_id}\\n\"\n",
    "            formatted += f\"DESCRIPTION: {subtask['description']}\\n\"\n",
    "            formatted += f\"SOLUTION: {solutions.get(subtask_id, 'Not solved')}\\n\\n\"\n",
    "        return formatted\n",
    "    \n",
    "    def _create_decomposer_agent(self) -> CrewAIAgent:\n",
    "        \"\"\"Create an agent specialized in breaking down problems.\"\"\"\n",
    "        return CrewAIAgent(\n",
    "            role=\"Problem Decomposer\",\n",
    "            goal=\"Break down complex problems into manageable subtasks\",\n",
    "            backstory=\"\"\"You are an expert in problem decomposition, with years of experience\n",
    "            breaking down complex problems into their constituent parts. Your specialty is\n",
    "            identifying the core components of any challenge and determining how they relate\n",
    "            to each other.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            llm=self.model\n",
    "        )\n",
    "    \n",
    "    def _create_solver_agent(self) -> CrewAIAgent:\n",
    "        \"\"\"Create an agent specialized in solving individual subtasks.\"\"\"\n",
    "        return CrewAIAgent(\n",
    "            role=\"Problem Solver\",\n",
    "            goal=\"Solve individual problems with precision and thoroughness\",\n",
    "            backstory=\"\"\"You are a meticulous problem solver with deep expertise across\n",
    "            multiple domains. You excel at focusing on specific challenges and providing\n",
    "            detailed, actionable solutions backed by sound reasoning.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            llm=self.model\n",
    "        )\n",
    "    \n",
    "    def _create_synthesizer_agent(self) -> CrewAIAgent:\n",
    "        \"\"\"Create an agent specialized in synthesizing solutions.\"\"\"\n",
    "        return CrewAIAgent(\n",
    "            role=\"Solution Synthesizer\",\n",
    "            goal=\"Integrate partial solutions into comprehensive wholes\",\n",
    "            backstory=\"\"\"You are a master of synthesis, capable of weaving together\n",
    "            disparate threads of information into coherent, integrated solutions.\n",
    "            Your strength lies in seeing the big picture while ensuring all details\n",
    "            are properly addressed.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            llm=self.model\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d83ed8-0e8f-4611-a316-adae33e3307b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d1e9c2-cf91-48cc-8ebe-a2798cb52003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  Agent Started </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Problem Decomposer</span>                                                                                      <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Task: </span>                                                                                                         <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Your primary goal is to determine whether the problem can be solved directly. </span>                     <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            PROBLEM: Develop a high-level strategy overview for my GTM market strategy for my new startup</span>      <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            **If the problem is simple enough to be solved immediately, return the string \"SIMPLE_ENOUGH\".** </span>  <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Do **not** break it down unless it is absolutely necessary.</span>                                        <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Only if the problem is **too complex to solve directly**, break it into subtasks.  </span>                <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            In that case, return a JSON list of subtasks in the following format:</span>                              <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            [</span>                                                                                                  <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                {</span>                                                                                              <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"id\": \"subtask-1\",</span>                                                                         <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"description\": \"Description of subtask 1\",</span>                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"rationale\": \"Why this subtask is necessary\",</span>                                              <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"dependencies\": []</span>                                                                         <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                },</span>                                                                                             <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                {</span>                                                                                              <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"id\": \"subtask-2\",</span>                                                                         <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"description\": \"Description of subtask 2\",</span>                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"rationale\": \"Why this subtask is necessary\",</span>                                              <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                    \"dependencies\": [\"subtask-1\"]</span>                                                              <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                },</span>                                                                                             <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">                ...</span>                                                                                            <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            ]</span>                                                                                                  <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            **REMEMBER:**  </span>                                                                                    <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            - If the problem can be solved directly, return **\"SIMPLE_ENOUGH\"**.  </span>                             <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            - Only decompose if **absolutely necessary**.</span>                                                      <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            </span>                                                                                                   <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m\u001b[0m\u001b[35m\u001b[0m\u001b[35m  Agent Started \u001b[0m\u001b[35m\u001b[0m\u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mProblem Decomposer\u001b[0m                                                                                      \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[37mTask: \u001b[0m                                                                                                         \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Your primary goal is to determine whether the problem can be solved directly. \u001b[0m                     \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            PROBLEM: Develop a high-level strategy overview for my GTM market strategy for my new startup\u001b[0m      \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            **If the problem is simple enough to be solved immediately, return the string \"SIMPLE_ENOUGH\".** \u001b[0m  \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Do **not** break it down unless it is absolutely necessary.\u001b[0m                                        \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Only if the problem is **too complex to solve directly**, break it into subtasks.  \u001b[0m                \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            In that case, return a JSON list of subtasks in the following format:\u001b[0m                              \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            [\u001b[0m                                                                                                  \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                {\u001b[0m                                                                                              \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"id\": \"subtask-1\",\u001b[0m                                                                         \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"description\": \"Description of subtask 1\",\u001b[0m                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"rationale\": \"Why this subtask is necessary\",\u001b[0m                                              \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"dependencies\": []\u001b[0m                                                                         \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                },\u001b[0m                                                                                             \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                {\u001b[0m                                                                                              \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"id\": \"subtask-2\",\u001b[0m                                                                         \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"description\": \"Description of subtask 2\",\u001b[0m                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"rationale\": \"Why this subtask is necessary\",\u001b[0m                                              \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                    \"dependencies\": [\"subtask-1\"]\u001b[0m                                                              \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                },\u001b[0m                                                                                             \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m                ...\u001b[0m                                                                                            \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            ]\u001b[0m                                                                                                  \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            **REMEMBER:**  \u001b[0m                                                                                    \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            - If the problem can be solved directly, return **\"SIMPLE_ENOUGH\"**.  \u001b[0m                             \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            - Only decompose if **absolutely necessary**.\u001b[0m                                                      \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            \u001b[0m                                                                                                   \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">  Agent Final Answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Problem Decomposer</span>                                                                                      <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Final Answer:</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"SIMPLE_ENOUGH\"</span>                                                                                                <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m  Agent Final Answer \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mProblem Decomposer\u001b[0m                                                                                      \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mFinal Answer:\u001b[0m                                                                                                  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m\"SIMPLE_ENOUGH\"\u001b[0m                                                                                                \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m complex_problem = \u001b[33m\"\u001b[39m\u001b[33mDevelop a high-level strategy overview for my GTM market strategy for my new startup\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Solve the problem\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m solution = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_problem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL SOLUTION:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mRecursiveProblemSolver.solve\u001b[39m\u001b[34m(self, problem_statement)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03mMain entry point to solve a complex problem recursively.\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \u001b[33;03m    A comprehensive solution to the problem\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Start the recursive solving process\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mRecursiveProblemSolver._recursive_solve\u001b[39m\u001b[34m(self, problem, depth)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Solve each subtask recursively based on dependencies\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subtask \u001b[38;5;129;01min\u001b[39;00m subtasks:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     subtask_id = \u001b[43msubtask\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Check if all dependencies are solved\u001b[39;00m\n\u001b[32m    127\u001b[39m     deps_solved = \u001b[38;5;28mall\u001b[39m(dep \u001b[38;5;129;01min\u001b[39;00m subtask_solutions \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m subtask.get(\u001b[33m\"\u001b[39m\u001b[33mdependencies\u001b[39m\u001b[33m\"\u001b[39m, []))\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from crewai import LLM as CrewLLM\n",
    "\n",
    "crewai_llm = CrewLLM(model=openai_model)\n",
    "\n",
    "solver = RecursiveProblemSolver(model=crewai_llm, max_recursion_depth=1)\n",
    "\n",
    "complex_problem = \"Develop a high-level strategy overview for my GTM market strategy for my new startup\"\n",
    "\n",
    "# Solve the problem\n",
    "solution = solver.solve(complex_problem)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"FINAL SOLUTION:\")\n",
    "print(\"\\n\")\n",
    "display(Markdown(solution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b15d470-66fa-43aa-b24b-cd1dce5c0006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Personal Assistant Agent with AutoGen and mem0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a95ac8-f46d-479d-86ab-50c9e2551519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Personal Assistant Agent](./PA%20Agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f88a08e6-7e8d-4dec-9215-94c9c296b342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise memory config utility method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "76b9b98b-bcd1-41d5-b794-92cb7154c9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "\n",
    "BASE_MEMORY_PATH = \"personal_assistant_memories\"\n",
    "\n",
    "def create_memory_config(memory_type):\n",
    "    \"\"\"\n",
    "    Create a dynamic memory configuration with a unique base path.\n",
    "    \n",
    "    :param memory_type: Subdirectory name for the specific memory type\n",
    "    :return: Configuration dictionary for the memory\n",
    "    \"\"\"\n",
    "    # Generate a random suffix to ensure unique paths\n",
    "    random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "    \n",
    "    # Method A: Random suffix approach\n",
    "    full_path = os.path.abspath(os.path.join(BASE_MEMORY_PATH, memory_type + '_' + random_suffix))\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    \n",
    "    config = {\n",
    "        \"vector_store\": {\n",
    "            \"provider\": \"qdrant\",\n",
    "            \"config\": {\n",
    "                \"path\": full_path\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bbc8232-603f-4052-b5e2-8df8c0b2cf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise the Personal Assistant Agent with usage of memory module, mem0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "99958c05-5bab-4819-b183-daea0eaffdc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from autogen import ConversableAgent, UserProxyAgent, config_list_from_json\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core.models import (\n",
    "    AssistantMessage,\n",
    "    SystemMessage,\n",
    "    UserMessage\n",
    ")\n",
    "from mem0 import Memory\n",
    "\n",
    "import json\n",
    "\n",
    "class PersonalAssistant:\n",
    "    def __init__(self, llm_config, user_id=\"default_user\"):\n",
    "        self.user_id = user_id\n",
    "        self.assistant_name = \"Your_Personal_Digital_Assistant\"\n",
    "        self.config=llm_config\n",
    "            \n",
    "        llm = init_llm_model('gpt-4o')\n",
    "        embedding_model = init_embedding_model('text-embedding-ada-002')\n",
    "\n",
    "        custom_preferences_memory = Memory.from_config(create_memory_config(\"preferences\"))\n",
    "        # custom_preferences_memory.llm = AzureOpenAILLM(llm)\n",
    "        # custom_preferences_memory.llm = llm\n",
    "        # custom_preferences_memory.embedding_model = embedding_model\n",
    "\n",
    "        # Initialize memory system\n",
    "        self.all_memories = {\n",
    "            \"preferences\": custom_preferences_memory,\n",
    "        }\n",
    "\n",
    "        # System message for the assistant\n",
    "        self.system_message = \"\"\"\n",
    "        You are a helpful digital personal assistant with memory. Your goal is to have a natural conversation with \n",
    "        the user while providing useful information and assistance. Follow these guidelines:\n",
    "        \n",
    "        1. Maintain conversation context and recall prior information shared by the user\n",
    "        2. Remember facts, preferences, and important details about the user\n",
    "        3. Ask clarifying questions when needed\n",
    "        4. Provide concise but complete answers\n",
    "        5. After providing help, ask for feedback to improve\n",
    "        6. Be friendly and conversational in tone\n",
    "        \n",
    "        When you learn something new about the user, remember it for future conversations.\n",
    "        \"\"\"\n",
    "            \n",
    "        self.tools = [self.extract_user_info]\n",
    "        \n",
    "        # Create the assistant agent\n",
    "        self.assistant_agent = ConversableAgent(\n",
    "          name=self.assistant_name,\n",
    "          system_message=self.system_message,\n",
    "          llm_config=self.config,\n",
    "          function_map={\n",
    "            \"extract_user_info\": self.extract_user_info,\n",
    "          },\n",
    "          human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "    async def extract_user_info(self, message: str) -> Dict:  \n",
    "        \"\"\"Extract user information from a message\"\"\"\n",
    "        # Use the LLM to extract facts and preferences\n",
    "        extraction_prompt = f\"\"\"\n",
    "        From the user message, extract any facts or preferences about the user.\n",
    "        Return a JSON object with \"facts\" and \"preferences\" as keys.\n",
    "        \n",
    "        Example output format:\n",
    "        {{\n",
    "            \"facts\": {{\n",
    "                \"name\": \"John\",\n",
    "                \"occupation\": \"software engineer\"\n",
    "            }},\n",
    "            \"preferences\": {{\n",
    "                \"food\": [\"pizza\", \"sushi\"],\n",
    "                \"hobbies\": [\"hiking\"]\n",
    "            }}\n",
    "        }}\n",
    "        \n",
    "        If no facts or preferences can be extracted, return empty objects.\n",
    "        \"\"\"\n",
    "        \n",
    "        extraction_agent = ConversableAgent(\n",
    "            name=\"extraction_agent\",\n",
    "            system_message=extraction_prompt,\n",
    "            llm_config=self.config,\n",
    "        )\n",
    "        response = extraction_agent.generate_reply(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": 'user',\n",
    "                    \"content\": message\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        try:\n",
    "            extracted_data = json.loads(response)\n",
    "            return extracted_data\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"facts\": {}, \"preferences\": {}}\n",
    "            \n",
    "    async def extract_and_store_user_info(self, message: str):\n",
    "        \"\"\"Extract user information and store in memory\"\"\"\n",
    "        # Use the extract_user_info tool\n",
    "        extracted_info = await self.assistant_agent.function_map[\"extract_user_info\"](message)\n",
    "                \n",
    "        if \"preferences\" in extracted_info and extracted_info[\"preferences\"]:\n",
    "            for category, prefs in extracted_info[\"preferences\"].items():\n",
    "                for pref in prefs:\n",
    "                    pref_str = f\"User prefers {pref} for {category}\"\n",
    "                    await self.add_to_memory(pref_str, \"user\", \"preferences\")\n",
    "    \n",
    "    async def _check_for_clarification_needed(self, message: str) -> tuple[bool, str]:\n",
    "        \"\"\"Check if clarification is needed\"\"\"\n",
    "        clarification_prompt = f\"\"\"\n",
    "        Based on the user message, decide:\n",
    "        \n",
    "        Do I need to ask a clarifying question before I can provide a helpful response?\n",
    "        If yes, provide just the clarifying question I should ask.\n",
    "        If no, respond with just \"NO_CLARIFICATION_NEEDED\".\n",
    "        \"\"\"\n",
    "        \n",
    "        clarification_agent = ConversableAgent(\n",
    "            name=\"clarification_agent\",\n",
    "            system_message=clarification_prompt,\n",
    "            llm_config=self.config,\n",
    "        )\n",
    "        response = clarification_agent.generate_reply(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": 'user',\n",
    "                    \"content\": message\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        if \"NO_CLARIFICATION_NEEDED\" in response:\n",
    "            return False, \"\"\n",
    "        else:\n",
    "            return True, response\n",
    "\n",
    "    async def add_to_memory(self, content: Union[str, List[Dict[str, str]]], user_type: str = \"user\", memory_type: str = \"preferences\"):\n",
    "        if (isinstance(content, str)):\n",
    "            content = [{\"role\": user_type, \"content\": content}]\n",
    "\n",
    "        memory_instance = self.all_memories[\"preferences\"]\n",
    "        result = memory_instance.add(messages=content, user_id=self.user_id)\n",
    "\n",
    "    async def retrieve_relevant_memories(self, message: str):\n",
    "        # Initialize an empty list to collect memories from all memory types\n",
    "        all_relevant_memories = []\n",
    "\n",
    "        # Iterate through all memory types\n",
    "        for memory_type, memory_instance in self.all_memories.items():\n",
    "            try:\n",
    "                # Search for relevant memories in each memory type\n",
    "                relevant_memories = memory_instance.search(message, user_id=self.user_id)\n",
    "                results = relevant_memories.get('results', [])\n",
    "                \n",
    "                # Add memory type prefix to provide context\n",
    "                type_prefixed_memories = [\n",
    "                    f\"\\n[{memory_type.upper()} Memory] {memory['memory']}\" \n",
    "                    for memory in results\n",
    "                ]\n",
    "                \n",
    "                # Extend the list of all relevant memories\n",
    "                all_relevant_memories.extend(type_prefixed_memories)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Log any errors during memory retrieval\n",
    "                print(f\"Error retrieving {memory_type} memories: {e}\")\n",
    "        \n",
    "        # Flatten memories into a single string\n",
    "        flatten_relevant_memories = \"\\n\".join(all_relevant_memories)\n",
    "\n",
    "        # Create the prompt with memories and original message\n",
    "        prompt = f\"\"\"Answer the user question considering the memories. Keep answers clear and concise.\n",
    "###### Memories:\n",
    "{flatten_relevant_memories}\n",
    "\n",
    "###### Question:\n",
    "{message}\n",
    "\"\"\"\n",
    "\n",
    "        return prompt\n",
    "            \n",
    "    async def start_conversation(self):\n",
    "        \"\"\"Start the conversation loop\"\"\"\n",
    "        display(Markdown(f\"\\n{self.assistant_name}: Hello! I'm your digital personal assistant. How can I help you today?\"))\n",
    "\n",
    "        while True:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nYou: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "                display(Markdown(f\"\\n{self.assistant_name}: Goodbye! Have a great day!\"))\n",
    "                break\n",
    "\n",
    "            # Extract and store user information\n",
    "            await self.extract_and_store_user_info(user_input)\n",
    "            \n",
    "            # Check if clarification is needed\n",
    "            needs_clarification, clarification_question = await self._check_for_clarification_needed(user_input)\n",
    "            \n",
    "            if needs_clarification:\n",
    "                display(Markdown(f\"\\n{self.assistant_name}: {clarification_question}\"))\n",
    "                \n",
    "                # Get clarification from user\n",
    "                clarification_response = input(\"\\nYou: \")\n",
    "                \n",
    "                # Combine original and clarification\n",
    "                user_input = f\"{user_input} {clarification_response}\"\n",
    "            \n",
    "\n",
    "            prompt_with_relevant_memories = await self.retrieve_relevant_memories(user_input)\n",
    "\n",
    "            display(Markdown(f\"\\nPrompt:' {prompt_with_relevant_memories}\"))\n",
    "\n",
    "            response = self.assistant_agent.generate_reply(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": 'user',\n",
    "                        \"content\": prompt_with_relevant_memories\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            display(Markdown(f\"\\n{self.assistant_name}: {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ef507e-5d23-4786-88f4-1a5e50e12f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i757420/Documents/repo/ntu-technical-training/.venv/lib/python3.13/site-packages/autogen/oai/client.py:824: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  config_list = [config.copy() for config in config_list]  # make a copy before modifying\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Your_Personal_Digital_Assistant: Hello! I'm your digital personal assistant. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Your_Personal_Digital_Assistant: Goodbye! Have a great day!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant = PersonalAssistant(llm_config={\n",
    "  \"model\": openai_model,\n",
    "})\n",
    "\n",
    "# Add initial preferences to memory.\n",
    "await assistant.add_to_memory(\"User is an AI Engineer who develops application using FastAPI and Python\", \"user\", \"preferences\")\n",
    "await assistant.add_to_memory(\"User prefers concise answers\", \"user\", \"preferences\")\n",
    "\n",
    "await assistant.start_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61514cce-1c9e-48fb-8c4d-7f05f5e99e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Semantic Router Agent with OpenAI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd68a04-93b5-4192-a121-46dd28ce4d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Semantic Router Agent](./Semantic%20Router%20Agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdff3f68-62d8-4609-b07d-dc5459e37a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise the prompt template and execute the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594cd524-19d4-49a9-bce7-97f98f27baaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RESEARCH"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from agents import Agent, Runner\n",
    "\n",
    "model_name = 'gpt-4o-mini'\n",
    "agent_name = \"Router Agent\"\n",
    "router_instructions = \"You are a classification agent. Categorize the input as either 'RESEARCH' or 'CUSTOMER_SUPPORT'. Rules: - If the input asks for data, analysis, trends, or general knowledge, return 'RESEARCH'. - If the input is about product issues, troubleshooting, or account help, return 'CUSTOMER_SUPPORT'. - Respond with only 'RESEARCH' or 'CUSTOMER_SUPPORT'. Examples: Input: 'Can you summarize the latest AI research trends?' Output: 'RESEARCH' Input: 'How do I reset my password?' Output: 'CUSTOMER_SUPPORT' Input: 'Find the most recent studies on climate change.' Output: 'RESEARCH' Input: 'My internet is not working. Can you help?' Output: 'CUSTOMER_SUPPORT' Input: 'What are the key findings from the latest medical trials on cancer treatment?' Output: 'RESEARCH' Input: 'I was charged twice for my subscription. How do I get a refund?' Output: 'CUSTOMER_SUPPORT'\"\n",
    "\n",
    "router_agent = Agent(name=agent_name, instructions=router_instructions)\n",
    "                                 \n",
    "response = Runner.run_sync(router_agent,\"Can you summarize the latest AI research trends?\")\n",
    "\n",
    "display(Markdown(response.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c3021d-ae73-41e7-a873-e724655cd936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multi Agent System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097be2d9-2cb4-4603-9806-2a0f64f1c885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is Multi Agent System \n",
    "\n",
    "Multi-agent system features several independent or autonomous agents acting in their specialized roles that can collaborate to attain a collective objective. In cases where the complexity or scale of the task mandates specialization, inter-agent coordination provides better advantages than others by MAS. Communication among any agent within an MAS and coordination of each to exchange perceptions allows the MAS, consequently, to learn dynamically in a changing environment and perform complex tasks beyond the reach of single agents. These features make MAS a pretty powerful system for complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1334d620-4658-4a40-9e15-0c420fa9ad9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Support Agent with CrewAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5dd912e-451d-40d3-a784-281125a26678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Support Agent](./CrewAI%20Support%20Agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce91c968-be46-4eeb-aed6-387440635084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise the Support Agent class with CrewAI Agents and Tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5aeccba-c28e-4f61-832d-b7841f105c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from crewai import Task, Crew, Process\n",
    "from crewai import Agent as CrewAIAgent\n",
    "from typing import List, Dict\n",
    "\n",
    "class CustomerSupportAgent:\n",
    "    \"\"\"\n",
    "    A class-based implementation of a multi-agent customer support system using CrewAI.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='gpt-4o-mini'):\n",
    "        \"\"\"\n",
    "        Initialize the customer support system with agents and tasks.\n",
    "        \"\"\"\n",
    "        # Set up the LLM\n",
    "        self.llm = CrewLLM(model_name)\n",
    "        \n",
    "        # Initialize agents and tasks\n",
    "        self._initialize_agents()\n",
    "        self._initialize_tasks()\n",
    "    \n",
    "    def _initialize_agents(self) -> None:\n",
    "        \"\"\"Initialize all the specialist agents for the support system.\"\"\"\n",
    "        self.faq_agent = CrewAIAgent(\n",
    "            role=\"FAQ Specialist\",\n",
    "            goal=\"Provide accurate and helpful answers to frequently asked questions\",\n",
    "            backstory=\"\"\"You are an expert in company products, services, and policies.\n",
    "            You have extensive knowledge of common customer questions and can provide\n",
    "            clear, concise answers that resolve customer queries efficiently.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,  # Prevents recursion\n",
    "            llm=self.llm,\n",
    "            max_iter=1,  # Limit iterations\n",
    "            max_execution_time=60,  # Add timeout\n",
    "        )\n",
    "\n",
    "        self.troubleshooting_agent = CrewAIAgent(\n",
    "            role=\"Technical Troubleshooter\",\n",
    "            goal=\"Solve technical problems with step-by-step guidance\",\n",
    "            backstory=\"\"\"You are a technical expert who can diagnose and resolve\n",
    "            common technical issues. You ask clarifying questions when needed and\n",
    "            provide clear, actionable steps for customers to follow.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,  # Prevents recursion\n",
    "            llm=self.llm,\n",
    "            max_iter=1,  # Limit iterations\n",
    "            max_execution_time=60,  # Add timeout\n",
    "        )\n",
    "\n",
    "        self.policy_agent = CrewAIAgent(\n",
    "            role=\"Policy Advisor\",\n",
    "            goal=\"Explain policies, terms, and guidelines clearly and accurately\",\n",
    "            backstory=\"\"\"You specialize in explaining complex policies and terms\n",
    "            in simple language. You help customers understand their rights,\n",
    "            responsibilities, and the company's standard practices.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,  # Prevents recursion\n",
    "            llm=self.llm,\n",
    "            max_iter=1, # Limit iterations\n",
    "            max_execution_time=60, # Add timeout\n",
    "        )\n",
    "\n",
    "        self.howto_agent = CrewAIAgent(\n",
    "            role=\"How-To Guide\",\n",
    "            goal=\"Provide step-by-step instructions for common tasks\",\n",
    "            backstory=\"\"\"You excel at breaking down processes into clear, sequential\n",
    "            steps. You help customers complete tasks successfully by providing\n",
    "            detailed yet easy-to-follow instructions.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            llm=self.llm,\n",
    "            max_iter=1, # Limit iterations\n",
    "            max_execution_time=60, # Add timeout\n",
    "        )\n",
    "\n",
    "        # CRITICAL: Router agent should NEVER delegate or recurse\n",
    "        self.routing_agent = CrewAIAgent(\n",
    "            role=\"Query Router\",\n",
    "            goal=\"Analyze customer queries and classify them into one of four categories: FAQ, Troubleshooting, Policy, or How-To\",\n",
    "            backstory=\"\"\"You are a classification expert who analyzes customer queries\n",
    "            and determines which category they belong to. You make quick, decisive \n",
    "            classifications without delegating to other agents.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            llm=self.llm,\n",
    "            max_iter=1,  # Only one iteration\n",
    "            max_execution_time=30,  # Short timeout\n",
    "        )\n",
    "        \n",
    "        # Store all agents in a dictionary for easy access\n",
    "        self.agents = {\n",
    "            \"faq\": self.faq_agent,\n",
    "            \"troubleshooting\": self.troubleshooting_agent,\n",
    "            \"policy\": self.policy_agent,\n",
    "            \"howto\": self.howto_agent,\n",
    "            \"router\": self.routing_agent\n",
    "        }\n",
    "    \n",
    "    def _initialize_tasks(self) -> None:\n",
    "        \"\"\"Initialize tasks for each specialist agent with expected output formats.\"\"\"\n",
    "        self.routing_task = Task(\n",
    "            description=\"\"\"Analyze the customer query and classify it into exactly ONE category.\n",
    "            \n",
    "            Categories:\n",
    "            - FAQ: General questions about products, services, pricing, or company information\n",
    "            - Troubleshooting: Technical problems, errors, bugs, or issues needing diagnosis\n",
    "            - Policy: Questions about terms, conditions, policies, refunds, or legal matters\n",
    "            - How-To: Requests for step-by-step instructions or tutorials\n",
    "            \n",
    "            Customer query: {query}\n",
    "            \n",
    "            Respond with ONLY the category name: FAQ, Troubleshooting, Policy, or How-To\"\"\",\n",
    "            expected_output=\"One word: FAQ, Troubleshooting, Policy, or How-To\",\n",
    "            agent=self.routing_agent,\n",
    "        )\n",
    "\n",
    "        self.faq_task = Task(\n",
    "            description=\"\"\"Provide a clear, helpful answer to the customer's FAQ question.\n",
    "            Include relevant information that resolves their query.\n",
    "            \n",
    "            Customer query: {query}\"\"\",\n",
    "            expected_output=\"\"\"Answer: [Clear, concise answer to the FAQ]\n",
    "            \n",
    "            Additional Information: [Any helpful context or related information]\n",
    "            \n",
    "            Next Steps: [If applicable, what the customer might want to do next]\"\"\",\n",
    "            agent=self.faq_agent,\n",
    "        )\n",
    "\n",
    "        self.troubleshooting_task = Task(\n",
    "            description=\"\"\"Diagnose the technical issue and provide step-by-step\n",
    "            instructions to resolve it.\n",
    "            \n",
    "            Customer query: {query}\"\"\",\n",
    "            expected_output=\"\"\"Diagnosis: [Brief assessment of the likely problem]\n",
    "            \n",
    "            Solution:\n",
    "            1. [First step with clear instructions]\n",
    "            2. [Second step with clear instructions]\n",
    "            3. [Additional steps as needed]\n",
    "            \n",
    "            If the issue persists: [Alternative suggestions]\"\"\",\n",
    "            agent=self.troubleshooting_agent,\n",
    "        )\n",
    "\n",
    "        self.policy_task = Task(\n",
    "            description=\"\"\"Explain the relevant policy, terms, or guidelines in\n",
    "            simple language.\n",
    "            \n",
    "            Customer query: {query}\"\"\",\n",
    "            expected_output=\"\"\"Policy Summary: [Brief explanation in simple terms]\n",
    "            \n",
    "            Key Points:\n",
    "            - [Important aspect of the policy]\n",
    "            - [Another important aspect]\n",
    "            \n",
    "            Customer Rights: [What the customer is entitled to]\n",
    "            \n",
    "            Customer Responsibilities: [What is expected of the customer]\"\"\",\n",
    "            agent=self.policy_agent,\n",
    "        )\n",
    "\n",
    "        self.howto_task = Task(\n",
    "            description=\"\"\"Provide clear, step-by-step instructions for completing\n",
    "            the requested task.\n",
    "            \n",
    "            Customer query: {query}\"\"\",\n",
    "            expected_output=\"\"\"Instructions:\n",
    "            1. [First step with clear guidance]\n",
    "            2. [Second step with clear guidance]\n",
    "            3. [Additional steps as needed]\n",
    "            \n",
    "            Tips:\n",
    "            - [Helpful tip related to the task]\n",
    "            \n",
    "            Common Issues:\n",
    "            - [Potential pitfall and how to avoid it]\"\"\",\n",
    "            agent=self.howto_agent,\n",
    "        )\n",
    "        \n",
    "        # Store all tasks in a dictionary for easy access\n",
    "        self.tasks = {\n",
    "            \"faq\": self.faq_task,\n",
    "            \"troubleshooting\": self.troubleshooting_task,\n",
    "            \"policy\": self.policy_task,\n",
    "            \"howto\": self.howto_task,\n",
    "            \"router\": self.routing_task\n",
    "        }\n",
    "\n",
    "    def _extract_category_from_response(self, routing_result: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the category from the routing agent's response using multiple methods.\n",
    "        \n",
    "        Args:\n",
    "            routing_result: The raw response from the routing agent\n",
    "            \n",
    "        Returns:\n",
    "            The identified category or 'faq' as default\n",
    "        \"\"\"\n",
    "        # Convert to uppercase for easier matching\n",
    "        result_upper = routing_result.upper()\n",
    "        \n",
    "        # Direct word matching (most reliable)\n",
    "        if \"TROUBLESHOOTING\" in result_upper or \"TROUBLESHOOT\" in result_upper:\n",
    "            return \"troubleshooting\"\n",
    "        elif \"POLICY\" in result_upper:\n",
    "            return \"policy\"\n",
    "        elif \"HOW-TO\" in result_upper or \"HOWTO\" in result_upper or (\n",
    "            \"HOW\" in result_upper and \"TO\" in result_upper\n",
    "        ):\n",
    "            return \"howto\"\n",
    "        elif \"FAQ\" in result_upper:\n",
    "            return \"faq\"\n",
    "        \n",
    "        # Keyword-based fallback classification\n",
    "        query_keywords = {\n",
    "            \"troubleshooting\": [\"error\", \"bug\", \"problem\", \"issue\", \"broken\", \"not working\", \"fix\"],\n",
    "            \"policy\": [\"refund\", \"return\", \"terms\", \"policy\", \"legal\", \"rights\", \"cancel\"],\n",
    "            \"howto\": [\"how to\", \"how do i\", \"steps\", \"tutorial\", \"guide\", \"instructions\"],\n",
    "        }\n",
    "        \n",
    "        for category, keywords in query_keywords.items():\n",
    "            if any(keyword in result_upper for keyword in keywords):\n",
    "                return category\n",
    "        \n",
    "        # Default to FAQ if no clear match\n",
    "        return \"faq\"\n",
    "\n",
    "    def execute_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Process a customer query through the multi-agent system.\n",
    "        \n",
    "        Args:\n",
    "            query: The customer's question or request\n",
    "            \n",
    "        Returns:\n",
    "            The response from the appropriate specialist agent\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Route the query (with strict limits to prevent recursion)\n",
    "            print(f\"Routing query: {query}\")\n",
    "            \n",
    "            routing_crew = Crew(\n",
    "                agents=[self.routing_agent],\n",
    "                tasks=[self.routing_task],\n",
    "                verbose=False,  # Reduce verbosity to prevent confusion\n",
    "                process=Process.sequential,\n",
    "                max_execution_time=30,  # Global timeout for routing\n",
    "            )\n",
    "            \n",
    "            routing_result = routing_crew.kickoff(inputs={\"query\": query})\n",
    "            print(f\"Raw routing result: {routing_result}\")\n",
    "            \n",
    "            # Step 2: Extract category using robust parsing\n",
    "            if hasattr(routing_result, 'raw'):\n",
    "                routing_text = routing_result.raw\n",
    "            else:\n",
    "                routing_text = str(routing_result)\n",
    "                \n",
    "            agent_type = self._extract_category_from_response(routing_text)\n",
    "            print(f\"Selected agent type: {agent_type}\")\n",
    "            \n",
    "            # Step 3: Execute with the selected specialist (no delegation allowed)\n",
    "            specialist_crew = Crew(\n",
    "                agents=[self.agents[agent_type]],\n",
    "                tasks=[self.tasks[agent_type]],\n",
    "                verbose=False,  # Reduce verbosity\n",
    "                process=Process.sequential,\n",
    "                max_execution_time=60,  # Global timeout for specialist\n",
    "            )\n",
    "            \n",
    "            # Get the specialist's response\n",
    "            response = specialist_crew.kickoff(inputs={\"query\": query})\n",
    "            \n",
    "            if hasattr(response, 'raw'):\n",
    "                return response.raw\n",
    "            else:\n",
    "                return str(response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in execute_query: {e}\")\n",
    "            # Fallback to direct FAQ response\n",
    "            return f\"I apologize, but I encountered an error processing your query. Here's a general response: {query}\"\n",
    "    \n",
    "    def batch_process(self, queries: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Process multiple customer queries and return all responses.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of customer queries to process\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping each query to its response\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for query in queries:\n",
    "            results[query] = self.execute_query(query)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b044cf0-8104-41f5-a8ea-7e37ab6fcc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c83ca02-1ebc-425a-ab0f-2851eeda4472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing query: How do I reset my password?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  Agent Started </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Query Router</span>                                                                                            <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Task: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Analyze the customer query and classify it into exactly ONE category.</span>                                    <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Categories:</span>                                                                                        <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            - FAQ: General questions about products, services, pricing, or company information</span>                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            - Troubleshooting: Technical problems, errors, bugs, or issues needing diagnosis</span>                   <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            - Policy: Questions about terms, conditions, policies, refunds, or legal matters</span>                   <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            - How-To: Requests for step-by-step instructions or tutorials</span>                                      <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Customer query: How do I reset my password?</span>                                                        <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Respond with ONLY the category name: FAQ, Troubleshooting, Policy, or How-To</span>                       <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m\u001b[0m\u001b[35m\u001b[0m\u001b[35m  Agent Started \u001b[0m\u001b[35m\u001b[0m\u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mQuery Router\u001b[0m                                                                                            \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[37mTask: \u001b[0m\u001b[92mAnalyze the customer query and classify it into exactly ONE category.\u001b[0m                                    \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Categories:\u001b[0m                                                                                        \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            - FAQ: General questions about products, services, pricing, or company information\u001b[0m                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            - Troubleshooting: Technical problems, errors, bugs, or issues needing diagnosis\u001b[0m                   \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            - Policy: Questions about terms, conditions, policies, refunds, or legal matters\u001b[0m                   \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            - How-To: Requests for step-by-step instructions or tutorials\u001b[0m                                      \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Customer query: How do I reset my password?\u001b[0m                                                        \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Respond with ONLY the category name: FAQ, Troubleshooting, Policy, or How-To\u001b[0m                       \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">  Agent Final Answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Query Router</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Final Answer:</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">How-To</span>                                                                                                         <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m  Agent Final Answer \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mQuery Router\u001b[0m                                                                                            \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mFinal Answer:\u001b[0m                                                                                                  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mHow-To\u001b[0m                                                                                                         \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw routing result: How-To\n",
      "Selected agent type: howto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  Agent Started </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">How-To Guide</span>                                                                                            <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Task: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Provide clear, step-by-step instructions for completing</span>                                                  <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            the requested task.</span>                                                                                <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">            Customer query: How do I reset my password?</span>                                                        <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m\u001b[0m\u001b[35m\u001b[0m\u001b[35m  Agent Started \u001b[0m\u001b[35m\u001b[0m\u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mHow-To Guide\u001b[0m                                                                                            \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[37mTask: \u001b[0m\u001b[92mProvide clear, step-by-step instructions for completing\u001b[0m                                                  \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            the requested task.\u001b[0m                                                                                \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m  \u001b[92m            Customer query: How do I reset my password?\u001b[0m                                                        \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m                                                                                                                 \u001b[35m\u001b[0m\n",
       "\u001b[35m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">  Agent Final Answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">How-To Guide</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Final Answer:</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">**Instructions:**</span>                                                                                              <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">1. **Visit the Login Page:** Open your web browser and go to the login page of the service or website where </span>   <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">you want to reset your password. Look for the \"Forgot Password?\" or \"Reset Password\" link, usually located </span>    <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">below the password entry field.</span>                                                                                <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">2. **Click on the Reset Link:** Once on the password reset page, click the \"Forgot Password?\" or \"Reset </span>       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">Password\" link. This action will redirect you to a form where you may be asked to enter your email address or</span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">username associated with your account.</span>                                                                         <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">3. **Enter Your Email or Username:** Fill in your registered email address or username in the provided field,</span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">and then click on the \"Submit\" or \"Send Link\" button. Check your email inbox for a password reset email from </span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">the service.</span>                                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">4. **Check Your Email:** Open the email you received and look for a password reset link. This link may expire</span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">after a certain period, so make sure to use it promptly. Click on the link provided in the email.</span>              <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">5. **Create a New Password:** You will be redirected to a webpage where you can enter a new password. Follow </span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">any guidelines provided regarding password complexity (such as length and character requirements). Enter your</span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new password in the designated field and confirm it by typing it again.</span>                                        <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">6. **Save Your Changes:** After entering your new password, click on the \"Save,\" \"Update Password,\" or </span>        <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">similar button to finalize the reset process. You should receive a confirmation that your password has been </span>   <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">changed successfully.</span>                                                                                          <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">7. **Log In with New Password:** Return to the login page of the service and enter your username and the new </span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">password you just created. If all went well, you should now be able to access your account.</span>                    <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">**Tips:**</span>                                                                                                      <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">- Make sure to create a strong password that includes a mix of uppercase letters, lowercase letters, numbers,</span>  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">and special characters to enhance your account security.</span>                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">**Common Issues:**</span>                                                                                             <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">- **Email Not Received:** If you do not receive the password reset email, check your spam or junk folder. </span>     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">Ensure you are checking the right email account associated with your account. If problems persist, consider </span>   <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">contacting customer support for assistance.</span>                                                                    <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m  Agent Final Answer \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mHow-To Guide\u001b[0m                                                                                            \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mFinal Answer:\u001b[0m                                                                                                  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m**Instructions:**\u001b[0m                                                                                              \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m1. **Visit the Login Page:** Open your web browser and go to the login page of the service or website where \u001b[0m   \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92myou want to reset your password. Look for the \"Forgot Password?\" or \"Reset Password\" link, usually located \u001b[0m    \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mbelow the password entry field.\u001b[0m                                                                                \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m2. **Click on the Reset Link:** Once on the password reset page, click the \"Forgot Password?\" or \"Reset \u001b[0m       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mPassword\" link. This action will redirect you to a form where you may be asked to enter your email address or\u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92musername associated with your account.\u001b[0m                                                                         \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m3. **Enter Your Email or Username:** Fill in your registered email address or username in the provided field,\u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mand then click on the \"Submit\" or \"Send Link\" button. Check your email inbox for a password reset email from \u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mthe service.\u001b[0m                                                                                                   \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m4. **Check Your Email:** Open the email you received and look for a password reset link. This link may expire\u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mafter a certain period, so make sure to use it promptly. Click on the link provided in the email.\u001b[0m              \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m5. **Create a New Password:** You will be redirected to a webpage where you can enter a new password. Follow \u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92many guidelines provided regarding password complexity (such as length and character requirements). Enter your\u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mnew password in the designated field and confirm it by typing it again.\u001b[0m                                        \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m6. **Save Your Changes:** After entering your new password, click on the \"Save,\" \"Update Password,\" or \u001b[0m        \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92msimilar button to finalize the reset process. You should receive a confirmation that your password has been \u001b[0m   \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mchanged successfully.\u001b[0m                                                                                          \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m7. **Log In with New Password:** Return to the login page of the service and enter your username and the new \u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mpassword you just created. If all went well, you should now be able to access your account.\u001b[0m                    \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m**Tips:**\u001b[0m                                                                                                      \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m- Make sure to create a strong password that includes a mix of uppercase letters, lowercase letters, numbers,\u001b[0m  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mand special characters to enhance your account security.\u001b[0m                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m**Common Issues:**\u001b[0m                                                                                             \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92m- **Email Not Received:** If you do not receive the password reset email, check your spam or junk folder. \u001b[0m     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mEnsure you are checking the right email account associated with your account. If problems persist, consider \u001b[0m   \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[92mcontacting customer support for assistance.\u001b[0m                                                                    \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How do I reset my password?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Instructions:**\n",
       "\n",
       "1. **Visit the Login Page:** Open your web browser and go to the login page of the service or website where you want to reset your password. Look for the \"Forgot Password?\" or \"Reset Password\" link, usually located below the password entry field.\n",
       "\n",
       "2. **Click on the Reset Link:** Once on the password reset page, click the \"Forgot Password?\" or \"Reset Password\" link. This action will redirect you to a form where you may be asked to enter your email address or username associated with your account.\n",
       "\n",
       "3. **Enter Your Email or Username:** Fill in your registered email address or username in the provided field, and then click on the \"Submit\" or \"Send Link\" button. Check your email inbox for a password reset email from the service.\n",
       "\n",
       "4. **Check Your Email:** Open the email you received and look for a password reset link. This link may expire after a certain period, so make sure to use it promptly. Click on the link provided in the email.\n",
       "\n",
       "5. **Create a New Password:** You will be redirected to a webpage where you can enter a new password. Follow any guidelines provided regarding password complexity (such as length and character requirements). Enter your new password in the designated field and confirm it by typing it again.\n",
       "\n",
       "6. **Save Your Changes:** After entering your new password, click on the \"Save,\" \"Update Password,\" or similar button to finalize the reset process. You should receive a confirmation that your password has been changed successfully.\n",
       "\n",
       "7. **Log In with New Password:** Return to the login page of the service and enter your username and the new password you just created. If all went well, you should now be able to access your account.\n",
       "\n",
       "**Tips:**\n",
       "- Make sure to create a strong password that includes a mix of uppercase letters, lowercase letters, numbers, and special characters to enhance your account security.\n",
       "\n",
       "**Common Issues:**\n",
       "- **Email Not Received:** If you do not receive the password reset email, check your spam or junk folder. Ensure you are checking the right email account associated with your account. If problems persist, consider contacting customer support for assistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "support_agent = CustomerSupportAgent()\n",
    "\n",
    "query = \"How do I reset my password?\"\n",
    "response = support_agent.execute_query(query)\n",
    "print(f\"\\nQuery: {query}\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212649f9-f3d6-4e06-bb0d-84d022b4176e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Research Agent with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7fcc51-75a9-49ac-bbc8-94088f774387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Research Agent](./Research%20Agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1d0529-40ed-4a85-ae42-b8941f12ce0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Setting Pydantic Data Models for LLM Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "98947055-798a-4a96-949c-ef5bec77ab5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class OutlineSubSection(BaseModel):\n",
    "    title: str = Field(..., title=\"Title of the subsection\")\n",
    "    content: str = Field(..., title=\"Description of relevant content for the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.title}\\n\\n{self.content}\".strip()\n",
    "      \n",
    "class OutlineSection(BaseModel):\n",
    "    title: str = Field(..., title=\"Title of the section\")\n",
    "    content: str = Field(..., title=\"Description of relevant content for the section\")\n",
    "    subsections: Optional[List[OutlineSubSection]] = Field(default=None, title=\"Titles and contents for each subsection in the section for the research article.\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {subsection.title}\\n\\n{subsection.content}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    title: str = Field(..., title=\"Title for the research article.\")\n",
    "    sections: List[OutlineSection] = Field(default_factory=list, title=\"Titles and contents for each section in the research article.\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.title}\\n\\n{sections}\".strip()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd3335d-9a55-47c8-ba72-c0af21322f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialising Prompt Templates to be used for LLM Generation**\n",
    "\n",
    "**It includes the Research Agent's Article Outline generation prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "066e387f-4c36-4892-b1b0-c1b13f3261e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "OUTLINE_GEN_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert research writer tasked with creating a comprehensive and structured outline for a research article on the given topic.\n",
    "\n",
    "            Guidelines for outline creation:\n",
    "            - Develop a clear, logical structure that covers the topic comprehensively\n",
    "            - Include 3-5 main sections that provide a holistic view of the subject\n",
    "            - Each main section should have 2-4 relevant subsections\n",
    "            - Ensure the outline reflects academic research writing standards\n",
    "            - Focus on creating a balanced and informative framework\n",
    "            - Return the outline in a clean, structured JSON format\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Generate a research article outline for the topic: {topic}\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05c97f7-5104-4114-94ae-ae185f17480e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialising LangGraph Graph State and several utility methods for the Research Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b20f53e-8d60-44ab-afee-7b2ccbacfbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, BaseMessage, ToolMessage\n",
    "from typing import TypedDict, List\n",
    "\n",
    "class ResearcherState(TypedDict):\n",
    "  outline: Outline\n",
    "  topic: str\n",
    "\n",
    "def set_up_outline_node(llm):\n",
    "  \"\"\"\n",
    "  Set up an outline node for generating a document outline.\n",
    "  Args:\n",
    "      llm: The language model to be used for generating the outline.\n",
    "  Returns:\n",
    "      Callable: A function to be used as outline node that generates an outline and updates the state.\n",
    "  \"\"\"\n",
    "  def outline_node(state: ResearcherState):\n",
    "      topic = state[\"topic\"]\n",
    "      outline_chain = OUTLINE_GEN_PROMPT | llm.with_structured_output(Outline)\n",
    "      outline = outline_chain.invoke(\n",
    "          {\n",
    "              \"topic\": topic,\n",
    "          }\n",
    "      )\n",
    "      if not isinstance(outline, Outline):\n",
    "          try:\n",
    "              outline = Outline(**outline)\n",
    "          except Exception as e:\n",
    "              raise ValueError(f\"Failed to parse outline: {outline}\") from e\n",
    "      \n",
    "      return {\n",
    "            **state,\n",
    "            \"outline\": outline,\n",
    "      }\n",
    "  return outline_node\n",
    "\n",
    "\n",
    "def build_researcher_agent(llm):\n",
    "  \"\"\"\n",
    "  Build a researcher agent using a state graph.\n",
    "  Args:\n",
    "      fast_llm: The language model to be used in the research process.\n",
    "      reranker (Reranker): The reranker tool to be used for refining results.\n",
    "      output_graph (bool): Whether to output the graph as an image. Defaults as False\n",
    "  Returns:\n",
    "      CompiledStateGraph: The compiled state graph representing the research process.\n",
    "  \"\"\"\n",
    "\n",
    "  outline_node = set_up_outline_node(llm)\n",
    "\n",
    "  builder = StateGraph(ResearcherState)\n",
    "\n",
    "  builder.add_node(\"outliner\", outline_node)\n",
    "\n",
    "  builder.add_edge(START, \"outliner\")\n",
    "  builder.add_edge(\"outliner\", END)\n",
    "\n",
    "  graph = builder.compile()\n",
    "  \n",
    "  return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d2e808-1e07-449d-a090-3e8968b126fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Initialising Prompt Templates to be used for LLM Generation**\n",
    "\n",
    "**It includes the Writer Agent's Full Article and Section prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "30b4a800-c4ea-4cd5-a4e1-d441d9c2e935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "FULL_ARTICLE_WRITER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "  messages=[\n",
    "    (\n",
    "      \"system\",\n",
    "      \"\"\"\n",
    "      You are an expert proofreader and writer. Write the complete research article on {topic} using the following draft:\\n\\n\n",
    "      {draft}\\n\\nStrictly enforce professional language and tone. Ensure that the article is well-structured and coherent.\n",
    "      \"\"\"\n",
    "    ),\n",
    "    (\n",
    "      \"user\", \n",
    "      \"\"\"Write the complete research article using Markdown format. \n",
    "      Ensure the article is organized and structured clearly with the following guidelines:\n",
    "      - Use # for the main title and ## for section titles.\n",
    "      - Use #### for subsections.\n",
    "      - Use bullet points for lists, and numbered lists where appropriate.\n",
    "      - Use **bold** for important terms and *italic* for emphasis.\n",
    "      - Include references in a properly formatted list at the end.\n",
    "      - Maintain a logical flow of the content and ensure sections are properly divided for easy navigation.\n",
    "      \"\"\"\n",
    "    )\n",
    "  ]\n",
    ")\n",
    "\n",
    "SECTION_WRITER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "  messages=[\n",
    "    (\n",
    "      \"system\",\n",
    "      \"\"\"You are a research writer. Complete your assigned Section from the following outline:\\n\\n\n",
    "      {outline}\".\n",
    "      \"\"\"\n",
    "    ),\n",
    "    (\"user\", \"Write the full Section content for the {section} section.\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c38e76-4ccf-453e-9a98-0254985ec2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialising LangGraph Graph State and several utility methods for the Writer Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4c06c3-ae14-41b0-80d1-303a4b6786b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, List, TypedDict\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "from langgraph.types import RetryPolicy\n",
    "\n",
    "class WriterStateGraph(TypedDict):\n",
    "  information: Optional[List[str]]\n",
    "  topic: str\n",
    "  outline: Outline\n",
    "  sections: List[OutlineSection]\n",
    "  article: str\n",
    "\n",
    "def set_up_write_section(long_context_llm):\n",
    "    \"\"\"\n",
    "    Set up a function for writing the full article.\n",
    "    Args:\n",
    "        long_context_llm: Language model for generating the full article.\n",
    "    Returns:\n",
    "        Callable: A function to be used as the write_section node that writes the full article and updates the state.\n",
    "    \"\"\"\n",
    "    def write_section(state: WriterStateGraph):\n",
    "        topic = state[\"topic\"]\n",
    "        outline = state[\"outline\"]\n",
    "        section_writer_chain = SECTION_WRITER_PROMPT | long_context_llm.with_structured_output(OutlineSubSection)\n",
    "        \n",
    "        futures = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for index, section in enumerate(outline.sections):\n",
    "                future = executor.submit(\n",
    "                    section_writer_chain.invoke,\n",
    "                    {\n",
    "                        \"topic\": topic,\n",
    "                        \"outline\": outline.as_str,\n",
    "                        \"section\": section.title\n",
    "                    }\n",
    "                )\n",
    "                futures.append((index, future))\n",
    "            \n",
    "            # Collect results and maintain order\n",
    "            sections = [None] * len(outline.sections)\n",
    "            for index, future in futures:\n",
    "                result = future.result()\n",
    "                sections[index] = result\n",
    "\n",
    "        validated_sections = []\n",
    "        try:\n",
    "            for section in sections:\n",
    "                if not isinstance(section, OutlineSubSection):\n",
    "                        section = OutlineSubSection(**section)\n",
    "                        validated_sections.append(section)\n",
    "                else:\n",
    "                    validated_sections.append(section)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse sections: {sections}\") from e\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"sections\": validated_sections\n",
    "        }\n",
    "    return write_section\n",
    "\n",
    "def set_up_write_article(long_context_llm):\n",
    "    \"\"\"\n",
    "    Set up a function for writing the full article.\n",
    "    Args:\n",
    "        long_context_llm: Language model for generating the full article.\n",
    "    Returns:\n",
    "        Callable: A function to be used as write_article node that writes the full article and updates the state.\n",
    "    \"\"\"\n",
    "    def write_article(state: WriterStateGraph):\n",
    "        topic = state[\"topic\"]\n",
    "        draft = \"\\n\\n\".join([section.as_str for section in state[\"sections\"]])\n",
    "        full_article_writer_chain = FULL_ARTICLE_WRITER_PROMPT | long_context_llm | StrOutputParser()\n",
    "        full_article = full_article_writer_chain.invoke(\n",
    "            {\n",
    "                \"topic\": topic,\n",
    "                \"draft\": draft\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"article\": full_article\n",
    "        }\n",
    "    return write_article\n",
    "\n",
    "def build_writer_agent(long_context_llm):\n",
    "    \"\"\"\n",
    "    Build a writer agent using a state graph.\n",
    "    Args:\n",
    "      embeddings: Embedding model for vectorizing document content.\n",
    "      long_context_llm: Language model for generating article content.\n",
    "    Returns:\n",
    "        CompiledStateGraph: The compiled state graph representing the writing process.\n",
    "    \"\"\"\n",
    "    write_section = set_up_write_section(long_context_llm)\n",
    "    write_article = set_up_write_article(long_context_llm)\n",
    "\n",
    "    builder_of_writer = StateGraph(WriterStateGraph)\n",
    "    nodes = [\n",
    "        (\"write_section\", write_section),\n",
    "        (\"write_article\", write_article)\n",
    "    ]\n",
    "\n",
    "    for i in range(len(nodes)):\n",
    "        name, node = nodes[i]\n",
    "        builder_of_writer.add_node(name, node, retry=RetryPolicy(max_attempts=3))\n",
    "        if i > 0:\n",
    "            builder_of_writer.add_edge(nodes[i-1][0], name)\n",
    "\n",
    "    builder_of_writer.add_edge(START, nodes[0][0])\n",
    "    builder_of_writer.add_edge(nodes[-1][0], END)\n",
    "    writer_agent = builder_of_writer.compile()\n",
    "\n",
    "    return writer_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10fda67-468d-4989-b189-f0df75f4b0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8769765e-d780-44fb-9fba-9c3754273bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i757420/Documents/repo/ntu-technical-training/.venv/lib/python3.13/site-packages/langgraph/graph/state.py:416: LangGraphDeprecatedSinceV05: `retry` is deprecated and will be removed. Please use `retry_policy` instead. Deprecated in LangGraph V0.5 to be removed in V2.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Generative AI: A Comprehensive Overview\n",
       "\n",
       "## Introduction to Generative AI\n",
       "\n",
       "Generative AI refers to a category of artificial intelligence technologies that are capable of generating new content, from text and images to music and video, based on the data they have been trained on. Unlike traditional AI systems that are primarily designed to recognize patterns and make predictions, Generative AI goes a step further by creating entirely new outputs that mimic or extend the characteristics of the existing data. This ability to generate novel content is achieved through complex algorithms and models that leverage large datasets to learn and replicate the underlying structures of the input data.\n",
       "\n",
       "## Definition of Generative AI\n",
       "\n",
       "Generative AI encompasses a variety of techniques and models, including:\n",
       "\n",
       "- **Generative Adversarial Networks (GANs)**\n",
       "- **Variational Autoencoders (VAEs)**\n",
       "- **Transformer-based models**\n",
       "\n",
       "These technologies utilize *deep learning* principles to understand and produce content across various domains. For instance, GANs involve two neural networksthe **generator** and the **discriminator**that work in tandem; the generator creates new data instances, while the discriminator evaluates them against real data, leading to continuous improvements in the generated content. This interplay allows Generative AI to produce highly realistic outputs, which have applications ranging from art creation to realistic image synthesis.\n",
       "\n",
       "## Historical Context and Evolution\n",
       "\n",
       "The roots of Generative AI can be traced back to early developments in artificial intelligence and machine learning during the mid-20th century. However, it wasn't until the advent of deep learning in the 2010s that Generative AI truly began to flourish. Key milestones include:\n",
       "\n",
       "1. The introduction of GANs by Ian Goodfellow in 2014, marking a significant breakthrough in generative modeling.\n",
       "2. The development of VAEs and Transformer architectures, which further expanded the capabilities of these systems.\n",
       "\n",
       "Over the years, advancements in computational power and the availability of vast datasets have propelled the evolution of Generative AI, leading to its current prominence in both academia and industry.\n",
       "\n",
       "## Importance and Relevance\n",
       "\n",
       "Today, Generative AI is highly relevant across various industries due to its ability to enhance creativity, efficiency, and personalization. Its applications include:\n",
       "\n",
       "- In the **creative arts**, enabling artists and musicians to explore new styles and compositions.\n",
       "- In **healthcare**, assisting in drug discovery and medical imaging analysis.\n",
       "- In **business**, supporting market analysis and customer engagement through personalized content generation.\n",
       "\n",
       "Furthermore, the growing integration of Generative AI into everyday applications, such as chatbots and virtual assistants, underscores its significance in enhancing user experiences and streamlining operations. As technology continues to evolve, the potential applications of Generative AI are expected to expand, leading to transformative changes in both professional and personal contexts.\n",
       "\n",
       "## Technological Foundations of Generative AI\n",
       "\n",
       "The technological foundations of Generative AI (GenAI) are rooted in advanced computational techniques that enable machines to learn from data and generate new content that mimics human creativity. This section explores the core technologies that drive GenAI, including machine learning, deep learning, various generative models, and the data requirements for training these systems.\n",
       "\n",
       "### Machine Learning and Deep Learning\n",
       "\n",
       "At the heart of Generative AI lies **machine learning**, a subset of artificial intelligence that enables systems to learn from data without being explicitly programmed. Machine learning algorithms can identify patterns and make predictions based on input data. **Deep learning**, a branch of machine learning, utilizes neural networks with multiple layers (hence 'deep') to process data in a hierarchical manner. This allows for the extraction of complex features from raw data, making it particularly effective for tasks such as image and speech recognition, as well as natural language processing.\n",
       "\n",
       "Deep learning has gained significant attention in recent years due to its ability to handle vast amounts of unstructured data, such as images, audio, and text. By leveraging large datasets and powerful computational resources (such as GPUs), deep learning models can learn intricate representations of data, which are critical for generating high-quality outputs in GenAI applications.\n",
       "\n",
       "### Generative Models and Architectures\n",
       "\n",
       "Generative AI encompasses a variety of models designed to create new data instances similar to the training data. Some of the most prominent generative models include:\n",
       "\n",
       "1. **Generative Adversarial Networks (GANs)**: GANs consist of two neural networksa generator and a discriminatorthat compete against each other. The generator creates synthetic data, while the discriminator evaluates its authenticity. Through this adversarial process, GANs can produce remarkably realistic images, videos, and audio.\n",
       "   \n",
       "2. **Variational Autoencoders (VAEs)**: VAEs are designed for unsupervised learning and are particularly effective in generating new data points. They encode input data into a latent space, from which new samples can be drawn, allowing VAEs to create diverse outputs while maintaining the statistical properties of the training data.\n",
       "   \n",
       "3. **Transformers**: Initially developed for natural language processing, transformers have revolutionized GenAI by enabling the generation of coherent and contextually relevant text. Their architecture, characterized by self-attention mechanisms, allows models to focus on different parts of the input data, making them adept at understanding and producing complex sequences.\n",
       "\n",
       "These generative models each have distinct advantages and applications, contributing to the versatility of GenAI across various domains.\n",
       "\n",
       "### Data Requirements and Training Processes\n",
       "\n",
       "The effectiveness of Generative AI heavily depends on the quality and quantity of data used for training. Large datasets that are representative of the target domain are crucial for training robust models. For instance, when training a GAN to generate images, a diverse dataset containing thousands or millions of images is often required to ensure that the model learns a wide range of features and styles.\n",
       "\n",
       "The training process for GenAI models typically involves several key steps:\n",
       "\n",
       "- **Data Collection and Preprocessing**: Gathering relevant data and preparing it for training, which may include normalization, augmentation, and cleaning.\n",
       "- **Model Training**: Employing optimization techniques to minimize the loss function, which measures the difference between the generated outputs and the real data. This often requires substantial computational resources and time, especially for deep learning models.\n",
       "- **Evaluation and Fine-tuning**: Assessing the performance of the model using various metrics (e.g., Inception Score for images) and making adjustments to improve quality and diversity of the generated content.\n",
       "\n",
       "In conclusion, the technological foundations of Generative AI are built upon sophisticated machine learning and deep learning techniques, diverse generative models, and comprehensive data training methodologies. These elements collectively enable the creation of innovative and high-quality content across multiple industries.\n",
       "\n",
       "## Applications of Generative AI\n",
       "\n",
       "Generative AI has emerged as a transformative force across diverse fields, revolutionizing traditional practices and creating new opportunities for innovation. This section explores the various applications of Generative AI, highlighting its impact on content creation, healthcare, business, and marketing strategies.\n",
       "\n",
       "### Content Creation and Media\n",
       "\n",
       "Generative AI is reshaping the landscape of content creation, offering tools that enhance artistic expression and streamline production processes. In the realm of art, AI algorithms can generate unique pieces that reflect various styles, enabling artists to explore new creative avenues. For example, systems like **DeepArt** and **DALL-E** allow users to create imagery based on textual descriptions, pushing the boundaries of visual art.\n",
       "\n",
       "In music, Generative AI applications such as **OpenAI's MuseNet** and **Jukedeck** can compose original scores in various genres, providing musicians with inspiration or even a complete track ready for production. These tools not only assist creators but also democratize music composition, allowing non-musicians to engage in music creation.\n",
       "\n",
       "Writing is another area experiencing a significant transformation. AI writing assistants like **ChatGPT** and **Jasper** can generate articles, stories, and poetry, helping authors overcome writer's block and streamline their writing processes. These applications allow for rapid content generation, which can be particularly beneficial for businesses and media outlets needing to produce large volumes of text quickly.\n",
       "\n",
       "### Healthcare Innovations\n",
       "\n",
       "In the healthcare sector, Generative AI is making strides in areas such as drug discovery, medical imaging, and personalized medicine. By analyzing vast datasets, AI models can identify potential drug candidates by predicting molecular interactions and optimizing chemical compounds. This accelerates the drug discovery process, reducing the time and costs associated with bringing new therapies to market.\n",
       "\n",
       "Generative AI also plays a crucial role in medical imaging, where it can enhance image quality and assist in diagnosing conditions. Algorithms can generate high-fidelity images from low-quality scans, improving the accuracy of diagnostic procedures. Moreover, in personalized medicine, Generative AI can analyze patient data to recommend individualized treatment plans, taking into account genetic information and lifestyle factors.\n",
       "\n",
       "### Business and Marketing Strategies\n",
       "\n",
       "Businesses are increasingly harnessing the power of Generative AI to refine their marketing strategies and enhance customer engagement. AI-driven tools can analyze consumer data to identify trends and predict market behavior, enabling companies to tailor their products and services to meet specific customer needs. By generating insights from data, businesses can optimize their marketing campaigns, ensuring they reach the right audience at the right time.\n",
       "\n",
       "Generative AI is also being utilized in product development, allowing companies to simulate various design iterations quickly. By generating prototypes based on consumer feedback and market analysis, businesses can innovate more effectively and reduce time-to-market for new products. Additionally, chatbots powered by Generative AI are enhancing customer service by providing instant responses to queries, improving customer satisfaction, and reducing operational costs.\n",
       "\n",
       "In conclusion, the applications of Generative AI span a wide array of fields, from content creation in art and media to significant advancements in healthcare and innovative business strategies. As technology continues to evolve, the potential for Generative AI to reshape industries and enhance human creativity remains vast.\n",
       "\n",
       "## Ethical Considerations and Challenges\n",
       "\n",
       "As Generative AI (GenAI) technologies continue to advance and proliferate across various domains, a critical examination of the ethical implications associated with their use becomes increasingly essential. The deployment of these technologies raises several ethical considerations and challenges that must be addressed to ensure responsible and equitable use.\n",
       "\n",
       "### Bias and Fairness\n",
       "\n",
       "One of the foremost ethical concerns surrounding Generative AI is the issue of bias and fairness. These systems are trained on vast datasets that often reflect existing societal biases, which can lead to the perpetuation or even amplification of these biases in generated outputs. For instance, if a Generative AI model is trained on historical data that exhibits gender or racial biases, the generated content may inadvertently reinforce stereotypes or discriminatory practices. This not only raises questions about the fairness of AI-generated content but also poses risks of harm to marginalized groups.\n",
       "\n",
       "Addressing bias in GenAI requires a multifaceted approach, including:\n",
       "\n",
       "- Implementation of bias detection and mitigation techniques during the training process.\n",
       "- Ongoing monitoring of AI outputs to ensure fairness and representation.\n",
       "- Engagement in discussions with diverse communities to understand the implications of the technology.\n",
       "\n",
       "### Intellectual Property Issues\n",
       "\n",
       "The rise of Generative AI has also sparked significant debate regarding intellectual property rights and ownership. As these systems create original worksbe it art, music, or textthe question arises: who holds the rights to these creations? Current copyright laws often do not adequately address the nuances of AI-generated content, leading to potential legal ambiguities and conflicts.\n",
       "\n",
       "For instance, if a Generative AI model is trained using copyrighted materials, the outputs may raise concerns about infringement. Additionally, the lack of clear ownership could lead to disputes over the attribution of creative works, complicating the relationship between human creators and AI systems. To navigate these challenges, it is vital to develop a robust legal framework that outlines the rights and responsibilities associated with AI-generated content, promoting clarity and fairness in the creative industries.\n",
       "\n",
       "### AI Regulation and Governance\n",
       "\n",
       "As the capabilities of Generative AI expand, the need for effective regulation and governance becomes paramount. The rapid pace of AI development often outstrips existing regulatory frameworks, leading to gaps in oversight that can result in misuse or harmful applications of the technology. Policymakers, technologists, and ethicists must collaborate to establish guidelines that ensure the responsible use of GenAI while fostering innovation.\n",
       "\n",
       "Current discussions around AI regulation involve considerations such as:\n",
       "\n",
       "- **Transparency**: Ensuring users are informed when they are interacting with AI-generated content.\n",
       "- **Accountability**: Establishing mechanisms for developers and companies that deploy Generative AI to mitigate risks and encourage ethical practices.\n",
       "\n",
       "In conclusion, while Generative AI holds immense potential for innovation across various sectors, addressing the ethical considerations and challenges it presents is crucial for fostering a responsible and fair technological landscape. By prioritizing issues of bias, intellectual property, and governance, stakeholders can work towards maximizing the benefits of GenAI while minimizing its risks.\n",
       "\n",
       "## Future Directions and Research Opportunities\n",
       "\n",
       "The field of Generative AI is rapidly evolving, presenting a myriad of opportunities for further exploration and innovation. As the technology matures, researchers and practitioners are poised to uncover new applications and refine existing methodologies, ultimately enhancing the capabilities and societal implications of Generative AI. In this section, we delve into emerging trends, interdisciplinary research opportunities, and the potential long-term societal impacts of Generative AI adoption.\n",
       "\n",
       "### Emerging Trends in GenAI\n",
       "\n",
       "The landscape of Generative AI is continuously shifting, with several emerging trends that are likely to define its trajectory in the coming years. Notable trends include:\n",
       "\n",
       "- **Multimodal generative models**: These integrate various types of data inputs, such as text, images, and audio, allowing for more sophisticated content generation.\n",
       "- **Focus on interpretability and transparency**: As GenAI becomes more integrated into decision-making processes, understanding how and why models generate specific outputs becomes crucial.\n",
       "- **Advancements in reinforcement learning**: This paves the way for applications that adapt and optimize outputs based on user feedback or environmental changes.\n",
       "\n",
       "### Interdisciplinary Research Opportunities\n",
       "\n",
       "The complexity of Generative AI necessitates collaboration across diverse fields, creating ample opportunities for interdisciplinary research. Potential areas of collaboration include:\n",
       "\n",
       "- Partnerships between **computer scientists** and **ethicists** to address the ethical implications of AI-generated content.\n",
       "- Collaboration between **artists** and **technologists** to yield innovative creative tools that push the boundaries of artistic expression.\n",
       "- Joint efforts between **AI researchers** and **medical professionals** to explore innovative applications in diagnostics or treatment personalization.\n",
       "\n",
       "### Potential Societal Impacts\n",
       "\n",
       "The widespread adoption of Generative AI is likely to have profound societal impacts, both positive and negative. On the positive side, Generative AI has the potential to democratize access to information and creative tools, enabling individuals without traditional skills to produce high-quality content. This could foster a more inclusive digital landscape where diverse voices and perspectives are amplified.\n",
       "\n",
       "Conversely, the proliferation of AI-generated content raises concerns about misinformation and authenticity. As Generative AI becomes more sophisticated, distinguishing between human-created and AI-generated content may become increasingly challenging, leading to significant implications for trust in media and information sources.\n",
       "\n",
       "Finally, the ethical implications of Generative AI, including issues of bias, copyright, and job displacement, will require ongoing research and policy development. Addressing these challenges will be crucial to ensure that the benefits of Generative AI are realized while mitigating potential harms.\n",
       "\n",
       "In conclusion, the future of Generative AI is filled with exciting possibilities and challenges. By embracing interdisciplinary collaboration and remaining vigilant about ethical considerations, researchers and practitioners can harness the power of Generative AI to create a more innovative, inclusive, and responsible technological landscape.\n",
       "\n",
       "## References\n",
       "\n",
       "1. Goodfellow, I., et al. (2014). Generative Adversarial Networks. *Advances in Neural Information Processing Systems*.\n",
       "2. Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. *International Conference on Learning Representations*.\n",
       "3. Vaswani, A., et al. (2017). Attention is All You Need. *Advances in Neural Information Processing Systems*.\n",
       "4. OpenAI. (2020). DALL-E: Creating Images from Text. Retrieved from [OpenAI](https://openai.com/dall-e/).\n",
       "5. OpenAI. (2019). MuseNet: Generating Music with Transformers. Retrieved from [OpenAI](https://openai.com/blog/musenet/).\n",
       "6. ChatGPT. (2021). OpenAIs Language Model. Retrieved from [OpenAI](https://openai.com/chatgpt/).\n",
       "7. Jukedeck. (2016). AI Music Composition. Retrieved from [Jukedeck](https://www.jukedeck.com/)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "llm = init_llm_model('gpt-4o-mini')\n",
    "\n",
    "research_agent = build_researcher_agent(llm)\n",
    "writer_agent = build_writer_agent(llm)\n",
    "\n",
    "topic = \"Write me a research topic about GenAI\"\n",
    "outline = None\n",
    "\n",
    "for stage in research_agent.stream({\"topic\": topic}):\n",
    "  if \"outliner\" in stage.keys():\n",
    "    outline = stage[\"outliner\"]['outline']\n",
    "\n",
    "sections, article = None, None\n",
    "for event in writer_agent.stream({\"topic\": topic, \"outline\": outline}):\n",
    "  for value in event.values():\n",
    "    if not sections and \"sections\" in value.keys():\n",
    "      sections = True\n",
    "    elif not article and \"article\" in value.keys():\n",
    "      article = value[\"article\"]\n",
    "\n",
    "display(Markdown(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4984ee6f-93fd-425d-8de8-fd9fd0b917cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Single-Agent Systems vs Multi-Agent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c6fcab-1913-4834-9efc-8ea91f38af3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Single Agent vs Multi Agent](./SA%20vs%20MA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0fdad6-dd88-4c79-91a0-ca9c2077c3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bonus: OpenAI Agents working with CrewAI Agents working with LangGraph Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d31352-9575-4825-aa8b-972e1289ce90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialising LangGraph Graph State and several utility methods for the Master Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e23b72fe-6f56-4d66-b2e8-c19d292dd1d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Outline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, TypedDict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mMasterAgentGraphState\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mTypedDict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m  \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m  \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mMasterAgentGraphState\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m information: Optional[List[\u001b[38;5;28mstr\u001b[39m]]\n\u001b[32m     10\u001b[39m topic: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m outline: \u001b[43mOutline\u001b[49m\n\u001b[32m     12\u001b[39m sections: List[OutlineSection]\n\u001b[32m     13\u001b[39m article: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'Outline' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, TypedDict\n",
    "\n",
    "import asyncio\n",
    "\n",
    "class MasterAgentGraphState(TypedDict):\n",
    "  query_type: str\n",
    "  question: str\n",
    "  response: str\n",
    "  information: Optional[List[str]]\n",
    "  topic: str\n",
    "  outline: Outline\n",
    "  sections: List[OutlineSection]\n",
    "  article: str\n",
    "\n",
    "async def router_agent(state: MasterAgentGraphState):\n",
    "  agent_name = \"Router Agent\"\n",
    "  router_instructions = \"You are a classification agent. Categorize the input as either 'RESEARCH' or 'CUSTOMER_SUPPORT'. Rules: - If the input asks for data, analysis, trends, or general knowledge, return 'RESEARCH'. - If the input is about product issues, troubleshooting, or account help, return 'CUSTOMER_SUPPORT'. - Respond with only 'RESEARCH' or 'CUSTOMER_SUPPORT'. Examples: Input: 'Can you summarize the latest AI research trends?' Output: 'RESEARCH' Input: 'How do I reset my password?' Output: 'CUSTOMER_SUPPORT' Input: 'Find the most recent studies on climate change.' Output: 'RESEARCH' Input: 'My internet is not working. Can you help?' Output: 'CUSTOMER_SUPPORT' Input: 'What are the key findings from the latest medical trials on cancer treatment?' Output: 'RESEARCH' Input: 'I was charged twice for my subscription. How do I get a refund?' Output: 'CUSTOMER_SUPPORT'\"\n",
    "\n",
    "  # triage_agent = OpenAIAgent(model_name, CustomOpenAIModelProvider(), agent_name, router_instructions)\n",
    "  triage_agent = Agent(name=agent_name, instructions=router_instructions)\n",
    "\n",
    "  question = state['question']\n",
    "                                 \n",
    "  response = Runner.run_sync(triage_agent, question)\n",
    "\n",
    "  return {'query_type': response.final_output}\n",
    "\n",
    "def sync_router_agent(state: MasterAgentGraphState):\n",
    "  return asyncio.run(router_agent(state))\n",
    "\n",
    "def customer_support_agent(state: MasterAgentGraphState):\n",
    "  support_agent = CustomerSupportAgent()\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  response = support_agent.execute_query(question)\n",
    "\n",
    "  return {'response': response}\n",
    "\n",
    "def research_agent(state: MasterAgentGraphState):\n",
    "  llm = init_llm_model('gpt-4o-mini')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  research_agent = build_researcher_agent(llm)\n",
    "  writer_agent = build_writer_agent(llm)\n",
    "\n",
    "  outline = None\n",
    "\n",
    "  for stage in research_agent.stream({\"topic\": question}):\n",
    "    if \"outliner\" in stage.keys():\n",
    "      outline = stage[\"outliner\"]['outline']\n",
    "\n",
    "  sections, article = None, None\n",
    "  for event in writer_agent.stream({\"topic\": topic, \"outline\": outline}):\n",
    "    for value in event.values():\n",
    "      if not sections and \"sections\" in value.keys():\n",
    "        sections = True\n",
    "      elif not article and \"article\" in value.keys():\n",
    "        article = value[\"article\"]\n",
    "\n",
    "  return {'response': article}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1224189-2a8f-45f9-84ef-bf80aeb2a31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initialise the MasterAgent class with Agents from CrewAI, LangGraph and OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c8d8b3e8-6de8-4d78-a1bb-cbf8a8ce699a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "class MasterAgent():\n",
    "  def __init__(self):\n",
    "    workflow = StateGraph(MasterAgentGraphState)\n",
    "\n",
    "    workflow.add_node('router_agent', sync_router_agent)\n",
    "    workflow.add_node('research_agent', research_agent)\n",
    "    workflow.add_node('customer_support_agent', customer_support_agent)\n",
    "\n",
    "    workflow.add_edge(START, \"router_agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"router_agent\",\n",
    "        self.research_or_support_based_query,\n",
    "        {\n",
    "          \"research\": 'research_agent',\n",
    "          \"customer_support\": 'customer_support_agent'\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\n",
    "      \"research_agent\",\n",
    "      END\n",
    "    )\n",
    "    workflow.add_edge(\n",
    "      \"customer_support_agent\",\n",
    "      END\n",
    "    )\n",
    "  \n",
    "    self.app = workflow.compile()\n",
    "  \n",
    "  def research_or_support_based_query(self, state: MasterAgentGraphState) -> str:\n",
    "    if state['query_type'].upper() == 'RESEARCH':\n",
    "      return 'research'\n",
    "    \n",
    "    return 'customer_support'\n",
    "  \n",
    "  def execute(self, query):\n",
    "    for output in self.app.stream({'question': query}):\n",
    "      for key, value in output.items():\n",
    "          continue\n",
    "\n",
    "    response = value[\"response\"]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f33703-a3d6-4920-8fc8-a20e4e89196e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02dd5363-bce2-454f-9f0d-4d778c551be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i757420/Documents/repo/ntu-technical-training/.venv/lib/python3.13/site-packages/langgraph/graph/state.py:416: LangGraphDeprecatedSinceV05: `retry` is deprecated and will be removed. Please use `retry_policy` instead. Deprecated in LangGraph V0.5 to be removed in V2.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# The Transformative Trends of Artificial Intelligence in 2023\n",
       "\n",
       "## Introduction to Artificial Intelligence Trends\n",
       "\n",
       "Artificial Intelligence (AI) has emerged as a transformative force in modern technology, impacting nearly every sector of society. At its core, AI refers to the simulation of human intelligence in machines programmed to think and learn like humans. This encompasses various branches, including:\n",
       "\n",
       "- **Machine Learning**: Systems that learn from data.\n",
       "- **Deep Learning**: A subset of machine learning that utilizes neural networks.\n",
       "- **Natural Language Processing (NLP)**: Enabling machines to understand and interpret human language.\n",
       "\n",
       "The evolution of AI technologies dates back to the mid-20th century when the concept was first introduced. Over the decades, advancements in computing power, data availability, and algorithmic development have propelled AI from theoretical discussions into practical applications. The rapid growth of AI in recent years can be attributed to significant breakthroughs in machine learning and deep learning, which have unlocked unprecedented capabilities in data analysis, pattern recognition, and decision-making.\n",
       "\n",
       "Understanding current trends in AI is crucial for businesses, researchers, and policymakers as these trends dictate the trajectory of technological development and societal change. For businesses, staying abreast of AI innovations can lead to:\n",
       "\n",
       "- Enhanced operational efficiencies\n",
       "- Improved customer experiences\n",
       "- New revenue streams\n",
       "\n",
       "For researchers, identifying trends informs future studies and exploration of AI capabilities. Policymakers, on the other hand, must navigate the ethical and regulatory landscape that AI introduces, ensuring that technological advancements benefit society while mitigating risks.\n",
       "\n",
       "In summary, the exploration of AI trends is not merely an academic exercise; it is a vital endeavor that shapes the future of technology and its integration into our daily lives. As we delve into the current trends shaping AI in 2023, we uncover the innovations that are redefining industries and creating new opportunities for growth and development.\n",
       "\n",
       "## Current Trends in Artificial Intelligence\n",
       "\n",
       "As we explore the current trends in artificial intelligence (AI), it is essential to recognize how these advancements are shaping the landscape of technology and its applications across various sectors. The year 2023 has witnessed significant innovations, and understanding these trends is crucial for businesses and individuals alike. Below are some of the most significant trends in AI currently making waves in the industry.\n",
       "\n",
       "### Generative AI and Its Applications\n",
       "\n",
       "**Generative AI** refers to algorithms that can create new content, whether it be text, images, music, or even video. Prominent examples include:\n",
       "\n",
       "- **OpenAI's GPT-3**: Demonstrates remarkable abilities in generating human-like text.\n",
       "- **DALL-E**: Creates images from textual descriptions.\n",
       "\n",
       "In industries such as entertainment, marketing, and design, generative AI is streamlining processes by:\n",
       "\n",
       "- Automating content creation\n",
       "- Enabling personalized marketing strategies\n",
       "- Elevating creative endeavors\n",
       "\n",
       "For instance, marketers leverage generative AI to produce tailored advertisements that resonate with specific audiences, while artists use it to explore new artistic expressions.\n",
       "\n",
       "### AI in Automation and Robotics\n",
       "\n",
       "**Automation powered by AI** is revolutionizing various sectors, most notably manufacturing, logistics, and service industries. Key applications include:\n",
       "\n",
       "- **Manufacturing**: AI-driven robots enhance production lines, improve efficiency, and reduce errors.\n",
       "- **Logistics**: AI optimizes supply chain management by predicting demand and improving inventory control.\n",
       "- **Service Sectors**: AI is employed in chatbots that handle customer inquiries and robotic process automation (RPA) that streamlines administrative tasks.\n",
       "\n",
       "This trend increases productivity and allows human workers to focus on higher-value tasks, transforming workplace dynamics.\n",
       "\n",
       "### Advancements in Natural Language Processing\n",
       "\n",
       "**Natural Language Processing (NLP)** continues to evolve, enabling machines to understand and respond to human language more effectively. Developments in NLP enhance human-computer interaction through:\n",
       "\n",
       "- Voice-activated assistants\n",
       "- Sentiment analysis\n",
       "- Machine translation\n",
       "\n",
       "Applications in customer support are becoming increasingly sophisticated, with tools like conversational agents and virtual assistants capable of:\n",
       "\n",
       "- Conducting complex dialogues\n",
       "- Analyzing user sentiment\n",
       "- Providing personalized responses\n",
       "\n",
       "### AI Ethics and Governance\n",
       "\n",
       "With the rapid advancement of AI technologies comes the critical need to address ethical considerations and establish governance frameworks. Discussions surrounding AI ethics include concerns over:\n",
       "\n",
       "- Algorithmic bias\n",
       "- Data privacy\n",
       "- Potential misuse of AI technologies\n",
       "\n",
       "Companies and policymakers are increasingly recognizing the importance of implementing ethical guidelines and accountability measures to ensure that AI is developed and used responsibly. Initiatives aimed at creating transparent AI systems, promoting fairness, and protecting user data are gaining traction, reflecting a growing commitment to responsible AI practices.\n",
       "\n",
       "## Impact of AI Trends on Various Sectors\n",
       "\n",
       "As artificial intelligence continues to evolve, its transformative effects are being felt across multiple sectors, reshaping traditional practices and enabling new efficiencies. This section delves into the significant impacts of current AI trends on various industries, including healthcare, financial services, education, and retail.\n",
       "\n",
       "### Healthcare Innovations\n",
       "\n",
       "AI is revolutionizing the healthcare sector by enhancing:\n",
       "\n",
       "- **Diagnostics**\n",
       "- **Treatment planning**\n",
       "- **Patient management**\n",
       "\n",
       "Machine learning algorithms analyze vast datasets, allowing for the early detection of diseases through medical imaging and predictive analytics. For instance, AI tools can identify patterns in radiology images that may be missed by the human eye, leading to quicker and more accurate diagnoses of conditions such as cancer. Additionally, AI-driven systems streamline administrative tasks, reducing paperwork and allowing healthcare professionals to focus more on patient care. Personalized medicine, powered by AI, tailors treatments based on individual patient data, improving outcomes and minimizing adverse effects.\n",
       "\n",
       "### Financial Services Transformation\n",
       "\n",
       "The financial industry is undergoing a significant transformation due to AI technologies that enhance operational efficiency and customer experience. AI algorithms are employed in:\n",
       "\n",
       "- **Fraud detection systems**: Analyzing transaction patterns to identify irregularities.\n",
       "- **Risk assessment**: Evaluating creditworthiness by analyzing a broader set of data points than traditional methods.\n",
       "\n",
       "Furthermore, AI chatbots are improving customer service by providing instant responses to inquiries, enhancing overall customer experience and reducing operational costs for banks and financial institutions.\n",
       "\n",
       "### Education and AI\n",
       "\n",
       "In the education sector, AI is being integrated into learning environments to foster personalized learning experiences. **Adaptive learning platforms** utilize AI to assess students' strengths and weaknesses, providing tailored resources that cater to individual learning paces and styles. AI-driven analytics help educators identify at-risk students and intervene proactively, ultimately improving retention rates. Additionally, AI streamlines administrative processes, such as grading and enrollment management, allowing educators to focus on teaching rather than bureaucratic tasks.\n",
       "\n",
       "### Retail and Consumer Behavior\n",
       "\n",
       "The retail sector is also witnessing a profound impact from AI technologies that enhance customer engagement and operational efficiency. AI-powered **recommendation engines** analyze consumer behavior and preferences, providing personalized shopping experiences that increase customer satisfaction and boost sales. Retailers are employing AI for inventory management, utilizing predictive analytics to optimize stock levels and reduce waste. Furthermore, AI-driven chatbots enhance online shopping experiences by providing instant assistance and recommendations, bridging the gap between consumers and retailers. As AI continues to shape consumer behavior, retailers are adapting their strategies to leverage data-driven insights for competitive advantage.\n",
       "\n",
       "## Challenges and Future Directions in AI\n",
       "\n",
       "The field of artificial intelligence (AI) is rapidly evolving, providing immense opportunities across various sectors. However, alongside these advancements come significant challenges that must be addressed to ensure responsible and equitable deployment of AI technologies. This section explores the technical challenges, societal implications, and potential future trends shaping the landscape of AI.\n",
       "\n",
       "### Technical Challenges\n",
       "\n",
       "Despite the remarkable advancements in AI, developers and researchers face numerous technical obstacles that hinder the full realization of AI's potential. Key challenges include:\n",
       "\n",
       "1. **Data Quality**: AI systems depend heavily on large datasets for training and validation. If the data is biased, incomplete, or unrepresentative, it can lead to flawed algorithms and unreliable outcomes.\n",
       "2. **Algorithm Bias**: Occurs when an AI system demonstrates prejudiced behavior based on the data it was trained on. This can manifest in various ways, from facial recognition systems misidentifying individuals from certain demographic groups to hiring algorithms favoring specific profiles.\n",
       "3. **Interpretability**: Many advanced AI models, particularly deep learning systems, operate as black boxes, making it difficult for users to understand how decisions are made. This lack of transparency poses problems in high-stakes applications such as healthcare and criminal justice.\n",
       "\n",
       "### Societal Implications\n",
       "\n",
       "As AI technologies become more pervasive, their societal implications are profound and multifaceted. Major concerns include:\n",
       "\n",
       "- **Job Displacement**: Automation driven by AI is transforming industries, leading to the elimination of some jobs while creating new ones. Policymakers and businesses must strategize on workforce reskilling and upskilling initiatives.\n",
       "- **Privacy Concerns**: AI systems often require vast amounts of personal data to function effectively, leading to potential risks related to data misuse and breaches of privacy.\n",
       "- **Ethical Implications**: Questions surrounding accountability, fairness, and moral responsibility arise as AI systems increasingly make decisions that impact people's lives.\n",
       "\n",
       "### Future Trends to Watch\n",
       "\n",
       "Looking ahead, several future trends in AI are poised to shape its trajectory:\n",
       "\n",
       "- **Explainable AI (XAI)**: As the demand for transparency grows, researchers are focusing on creating models that can provide clear explanations for their decisions.\n",
       "- **Integration with Other Technologies**: The convergence of AI with other emerging technologies, such as the Internet of Things (IoT) and blockchain, can lead to innovative solutions that enhance efficiency, security, and data integrity.\n",
       "- **AI Ethics and Governance**: The growing emphasis on AI ethics will likely lead to the establishment of international standards and regulations aimed at ensuring responsible deployment.\n",
       "\n",
       "In conclusion, while the challenges facing the AI industry are significant, they also present opportunities for innovation and growth. By addressing technical obstacles, understanding societal implications, and being mindful of future trends, stakeholders can navigate the complexities of AI and harness its potential to drive positive change across society.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "As we conclude our exploration of the latest trends in artificial intelligence (AI), it is essential to recap the key points discussed throughout this article. We have examined the definition and scope of AI, delving into its branches such as machine learning, deep learning, and natural language processing. We traced the historical context of AI development to understand how we arrived at the present state of innovation and highlighted the importance of tracking AI trends for businesses, researchers, and policymakers.\n",
       "\n",
       "In the current landscape, we identified several significant trends shaping the future of AI. Generative AI is making waves with its ability to create content that mimics human creativity, finding applications in various industries, including entertainment and marketing. AI is also revolutionizing automation and robotics, streamlining processes in manufacturing and logistics, while advancements in natural language processing are enhancing human-computer interactions, making technology more accessible and user-friendly.\n",
       "\n",
       "Moreover, we delved into the rising discussions surrounding AI ethics and governance, emphasizing the need for frameworks to ensure responsible deployment of AI technologies.\n",
       "\n",
       "The impact of these trends is profound across various sectors. In healthcare, AI is improving diagnostics and personalizing treatment plans, while in financial services, it is enhancing fraud detection and customer experiences. In education, AI is transforming learning through personalized tools, and in retail, it is reshaping consumer behavior by offering tailored shopping experiences.\n",
       "\n",
       "However, the journey of AI is not without its challenges. Technical obstacles such as data quality and algorithm bias continue to hinder progress, while societal implications, including job displacement and privacy concerns, demand careful consideration and proactive measures.\n",
       "\n",
       "Looking ahead, future trends in AI promise even more transformative advancements. We can expect continued integration of AI in everyday life, leading to smarter cities, enhanced connectivity, and improved quality of life. As we embrace these changes, it is crucial for all stakeholders to stay informed and engaged with the evolving landscape of AI.\n",
       "\n",
       "In conclusion, the trajectory of artificial intelligence is one of rapid development and profound impact. As AI continues to reshape various aspects of our lives, understanding these trends will empower us to navigate the future more effectively. Staying informed about AI trends is not just beneficial; it is essential for readiness in an increasingly automated and intelligent world.\n",
       "\n",
       "## References\n",
       "\n",
       "1. Russell, S., & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach* (4th ed.). Pearson.\n",
       "2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.\n",
       "3. Chui, M., Manyika, J., & Miremadi, M. (2016). Where machines could replace humansand where they cant (yet). *McKinsey Quarterly*.\n",
       "4. Binns, R. (2018). Fairness in machine learning: Lessons from political philosophy. In *Proceedings of the 2018 Conference on Fairness, Accountability, and Transparency*.\n",
       "5. Jobin, A., Ienca, M., & Andorno, R. (2019). The global landscape of AI ethics guidelines. *Nature Machine Intelligence*, 1(9), 389-399."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "master_agent = MasterAgent()\n",
    "\n",
    "result = master_agent.execute(\"What is the latest trend in AI?\")\n",
    "\n",
    "# result = master_agent.execute(\"How to reset my password?\")\n",
    "\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 595996707283832,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "NTU Technical Training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
